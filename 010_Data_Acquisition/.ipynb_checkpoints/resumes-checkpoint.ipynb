{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb4f076",
   "metadata": {},
   "source": [
    "## Extracting Skills from a Represenatative Sample of Resumes\n",
    "We downloaded a large dataset of CVs from the IT domain which contains CVs of data scientists. This notebook extracts the present data scientist CVs, from these it extracts all skills. Each skill is written to its own row of a pandas dataframe, which is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e853cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale\n",
    "#handy pandas visualizer, when on local Jupyter Server just run dtale.show(df)\n",
    "#from Docker container visual is available at http://localhost:40000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17088e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " design sql, r & python scripts to directly query and analyze internal and external data sources to package together clear, actionable findings\n",
      "---\n",
      " aid in the development of predictive modeling and machine learning solutions that continually improve with the collection of real-time dealer and sales results from product portfolio, using popular r/python libraries, including tensorflow, h2o, scikit-learn, keras, glm, and nnet\n",
      "---\n",
      " test model fit using cross-validation and bootstrapping methods, hypothesis testing, and measuring error and confidence\n",
      "---\n",
      " develop in an scrum environment with team members, actively using sprints to deliver solutions quickly and continuously exploring ways to improve our results\n",
      "---\n",
      " model data using pandas/r dataframes, powerpivot, tera data, and power bi. \n",
      "---\n",
      " work closely with colleagues in product, operations, and sales to structure problems and understand the impact across various departments within the company \n",
      "---\n",
      " respond to project requests using jira, and follow the project through the entire sdlc cycle by creating/updating solutions/engagements\n",
      "---\n",
      " maintain codebase and documents using subversion and confluence deep learning research intern digibiosys - san francisco bay area, ca december 2018 to april 2019 \n",
      "---\n",
      " provided data visualizations for heart rate monitor readings using django, angular and d3.js\n",
      "---\n",
      " implemented rudimental nonparametric modeling to obtain predictions\n",
      "---\n",
      " tuned ann parameters to fit models to lower rss \n",
      "---\n",
      " statistical signal processing including fft and mle\n",
      "---\n",
      " configured data visualizations to read from aws elastic beanstalk for real-time data feed junior software engineer msfw - springfield, il april 2018 to december 2018 \n",
      "---\n",
      " assisting in the development, implementation and management of technology-based business solutions to improve our clients\\x92 delivery approach.\n",
      "---\n",
      " coding, testing and implementing configuration changes and assisting in the design of software applications to meet both functional and technical requirements using c# and the .net framework\n",
      "---\n",
      " full stack development using html5, css3, and multiple javascript libraries including angular, bootstrap & jquery\n",
      "---\n",
      " implementing analyses that will identify requirements related to people, processes & technology.\n",
      "---\n",
      " maintain and analyze client databases using ms/azure sql server, and repositories using git data analyst dcc marketing - decatur, il april 2017 to april 2018 \n",
      "---\n",
      " updated and redesigned user experience for dmh company website.?\n",
      "---\n",
      " worked closely with social media team on wordpress blogs and social media marketing.\n",
      "---\n",
      " used site analytics and metrics to define and monitor success, using complex sql queries and stored procedures\n",
      "---\n",
      " built user interface, data visualizations and designed overall user experience for town country bank\n",
      "---\n",
      " designed optimized and usable interfaces for financial applications. project management intern project management intern - decatur, il march 2017 to april 2018 \n",
      "---\n",
      " led 4 person team, developing cost effective supply chain for 785c model parts not routinely kept in stock.\n",
      "---\n",
      " developed a mysql database application to store tool parts and suppliers. database developer/systems administrator, intern southside country club - decatur, il - decatur, il august 2016 to october 2016 \n",
      "---\n",
      " set up a server using red hat linux.\n",
      "---\n",
      " configured microsoft sql server on the server, built and managed a database for use in the jonas club management software system that runs all the workstations. \n",
      "---\n",
      " bachelor's in electrical engineering northwestern university - chicago, il may 2019 to present master's in business administration, data management millikin university - decatur, il august 2016 to january 2018 bachelor's in computer science monmouth college - monmouth, il may 2016 skills c++ (3 years), javascript (4 years), python (5 years), data visualization (4 years), sql (5 years), data analysis (3 years), machine learning (2 years), deep learning (1 year), applied statistics (2 years), data mining (2 years), software engineering (2 years), r (2 years), big data, data science, business intelligence, sap, excel, html, c# (2 years), .net (2 years), tensorflow (1 year), neural networks (less than 1 year), keras (less than 1 year), torch (less than 1 year), pyspark (less than 1 year) links http://github.com/erikedmonds http://erikedmonds.github.io/election certifications/licenses data science immersive bootcamp july 2018 to present springboard\\x92s data science program assessments data analysis \\x97 expert august 2019 measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. full results: https://share.indeedassessments.com/share_assignment/4v2gy8hm75w1579r technical support \\x97 proficient august 2019 measures a candidate's ability to apply protocols to identify errors and solutions in order to maintain system function. full results: https://share.indeedassessments.com/share_assignment/rv-teu566d7d7nfs indeed assessments provides skills tests that are not indicative of a license or certification, or continued development in any professional field. publications hospital data mining with r 2017-05 - collected, munged and normalized hospital data from healthdata.gov to manipulate in a mysql database. built a database application with python to be analyzed with r. built a bi dashboard in r using flexdashboard and shiny. ride-sharing database application 2017-05 - built a ride-sharing web application for millikin students, using python and django, run on an apache 2.4 web server hosted at millikin. built the front-end using angularjs.\n",
      "\n",
      "---\n",
      " designed, modeled and built a star schema and cube to model internal logging data.\n",
      "---\n",
      " (currently) creating visuals of logging data using the aforementioned cube with power bi and tableau.\n",
      "---\n",
      " used sql and r to profile service related data.\n",
      "---\n",
      " wrote functions, r scripts and rmarkdown documentation for 6 shiny applications (soon to be 7) and\n",
      "---\n",
      " 5 r packages.\n",
      "---\n",
      " wrote complicated sql stored procedures and table creation scripts to regularly gather and house\n",
      "---\n",
      " customer data for analysis and modeling.\n",
      "---\n",
      " conducted exploratory analysis such as pca and feature reduction on small (1mil+) row datasets to determine appropriate model input.\n",
      "---\n",
      " troubleshot and performance tuned shiny applications and packages.\n",
      "---\n",
      " wrote hundreds of unit tests using testthat for r packages. production database administrator elead one may 2016 to july 2017 \n",
      "---\n",
      " triaged and resolved live problems on 50+ production boxes, including\n",
      "---\n",
      " - bad performance\n",
      "---\n",
      " - job failures\n",
      "---\n",
      " - data recovery\n",
      "---\n",
      " - query plans and tuning\n",
      "---\n",
      " wrote stored procedures and sql jobs to fetch, archive and remove data from production, qa and dev.\n",
      "---\n",
      " maintained and modified many other production-facing stored procs.\n",
      "---\n",
      " constantly monitored systems\n",
      "---\n",
      " included in two on-call rotations and patched servers during maintenance\n",
      "---\n",
      " windows (sunday at midnight). \n",
      "---\n",
      " bachelor of science in computer science florida state university - tallahassee, fl august 2012 to may 2016 skills sql, business intelligence, bi\n",
      "\n",
      "---\n",
      " used multiple embeddings to create a multi-layered neural network to detect insincere/toxic questions in a quora dataset.\n",
      "---\n",
      " used reddit's api to scrape posting data, and applied natural language processing to build a classification model predicting reddit post origins.\n",
      "---\n",
      " built a regularized regression model to predict house sale prices using data with a large number of features. insurance benefits specialist american income life - concord, ca september 2018 to february 2019 effectively manage sales and activation of insurance benefits, respond to policy service requests, and assist clients with application process. marketing support analyst - contract realogy - concord, ca april 2018 to september 2018 \n",
      "---\n",
      " support real estate agents in development and execution of marketing initiatives, especially with software/hardware issues. tabulate and troubleshoot technical issues related to the rollout of new agent-side software (hubspot crm, zendesk, etc).\n",
      "---\n",
      " assisted first-time users with demonstrations of new in-house marketing creation platform \"listing concierge\" (lc) and collaborated with service support teams to ensure top-quality listings. completed marketing material touch-ups as needed. energy services analyst commercial energy of california - oakland, ca march 2017 to december 2017 \n",
      "---\n",
      " analyzed client energy spend within pg&e tariff restrictions, and made recommendations based on past usage and future energy needs while ensuring the proper processing and submission of pg&e tariff rate changes.\n",
      "---\n",
      " used salesforce to track and audit customer contract information. enter audit quotes into great plains and run reports to track and verify billing.\n",
      "---\n",
      " used excel macros, pivot tables, and vlookup functions to create interactive excel sheets for sales teams to present to customers. marketing coordinator - contract friendmedia - pleasanton, ca february 2017 to march 2017 co-founder and front- end developer civilinformer - livermore, ca march 2016 to november 2016 associate technical recruiter collabera - san ramon, ca october 2015 to february 2016 skills in depth\n",
      "---\n",
      " programming with object-oriented design in python (especially jupyter notebooks)\n",
      "---\n",
      " predictive analytics, machine learning, and deep learning with: tensorflow, scikit-learn, keras, pandas, numpy, scipy, matplotlib, seaborn, pyplot, beautiful soup, nltk, pyplot, statsmodels, nlp\n",
      "---\n",
      " modeling with: times series analysis, ensemble models, model stacking, random forest, knn, logistic regression, linear regression, regularization, na\\xefve bayes, support vector machines, gradient descent, arima, k-means, dbscan, bayesian regression, maximum likelihood, ab testing, regressive neural networks, convolutional neural networks\n",
      "---\n",
      " web scraping, xml, html, json, api, aws\n",
      "---\n",
      " advanced microsoft excel skills (e.g pivot tables, etc.), sql, matlab\n",
      "---\n",
      " tableau, qlickview\n",
      "---\n",
      " microsoft office, adobe photoshop, indesign, illustrator \n",
      "---\n",
      " bachelor's in marketing university of arizona - tucson, az august 2011 to may 2015 skills excel, sql, powerpoint, microsoft office, business intelligence\n",
      "\n",
      "---\n",
      " responsible for supporting 3d printing subscription team with lifespan analysis of long-term consumables (ltc) \n",
      "---\n",
      " assisted customer success managers with inventory investigations and fraud detection\n",
      "---\n",
      " created project briefs for customer surveys, conducted site interviews to understand 3d printer use in the wild speech analytics analyst nautilus, inc april 2017 to march 2018 \n",
      "---\n",
      " responsible for natural language processing projects, presenting results to sales, marketing, operations\n",
      "---\n",
      " designed sqlite database to maintain call metadata for time series analysis, maintained .csv back-up\n",
      "---\n",
      " coordinated project progress between devops, data architecture, and ops support to pull in email, chat data\n",
      "---\n",
      " created and managed audit process to ensure accuracy of speech categorization system, metadata collection\n",
      "---\n",
      " started effort to translate callminer api cookbook into anaconda-python, jupyter notebooks business data analyst s.r.smith october 2016 to january 2017 managed etl processes, requirements gathering, dashboard creation for board of directors, australia team data architect spear \n",
      "---\n",
      " january 2015 to october 2016 \n",
      "---\n",
      " created and led cross-department initiative to improve data quality, reduce data products from seven to four\n",
      "---\n",
      " company lead for data analysis using survey design, a/b testing, anova, linear regression, focus groups\n",
      "---\n",
      " administrated netsuite database and led meetings with devops group for feature enhancements, bug fixes\n",
      "---\n",
      " designed tableau dashboard and etl process for netsuite, hubspot, mysql/amazon s3 data sets senior data analyst, interim it manager oncology convergence, inc january 2013 to november 2014 \n",
      "---\n",
      " administrated emr databases for remote clients, responsible for reporting, data governance, troubleshooting\n",
      "---\n",
      " responsible for feature enhancements for server systems, acted as point of contact with external vendors\n",
      "---\n",
      " led trainings for internal and external analysts on systems, hipaa compliance, data analysis skills business analyst/ project coordinator toyota financial systems october 2011 to august 2012 \n",
      "---\n",
      " coordinated remote team for upgrade from lotus notes to outlook, for 5k employees in over fifty locations\n",
      "---\n",
      " designed and implemented feedback system, later used for toyota motor systems migration (15k people)\n",
      "---\n",
      " developed and presented training on ms outlook, excel, word for english and spanish speaking employees \n",
      "---\n",
      " m.b.a. university of new hampshire august 2011 b.a. in psychology in psychology university of new hampshire august 2009 skills linear regression (1 year), lotus notes (less than 1 year), netsuite (1 year), python (less than 1 year), testing (1 year), excel, sql links http://www.kattkennedy.com additional information skills\n",
      "---\n",
      " programming: python (pandas, matplotlib, requests)\n",
      "---\n",
      " sql\n",
      "---\n",
      " github\n",
      "---\n",
      " r (tidyverse)\n",
      "---\n",
      " ruby (jekyll)\n",
      "---\n",
      " statistics: a/b, ab/aab testing\n",
      "---\n",
      " linear regression\n",
      "---\n",
      " knn\n",
      "---\n",
      " unsupervised/supervised learning\n",
      "---\n",
      " monte carlo\n",
      "---\n",
      " business analysis: requirements gathering\n",
      "---\n",
      " project/scope management\n",
      "---\n",
      " client relationship maintenance\n",
      "---\n",
      " devops: jira\n",
      "---\n",
      " bash/fish\n",
      "---\n",
      " linux (ubuntu)\n",
      "---\n",
      " windows server\n",
      "---\n",
      " aws\n",
      "---\n",
      " filezilla/cyberduck\n",
      "---\n",
      " slack\n",
      "---\n",
      " discord\n",
      "---\n",
      " crm/erp: netsuite\n",
      "---\n",
      " xero\n",
      "---\n",
      " epicor\n",
      "---\n",
      " hubspot\n",
      "---\n",
      " google analytics\n",
      "---\n",
      " allscripts\n",
      "---\n",
      " lotus notes\n",
      "---\n",
      " tableau\n",
      "\n",
      "---\n",
      " has experience in business analysis, big data, cyber security, mobile development and research\n",
      "---\n",
      " assistance. he holds a master's degree in computer science and is going to finish his general\n",
      "---\n",
      " management mba in december. he will be looking for an opportunity starting january 2015. work experience data scientist intern high 5 games - new york, ny august 2014 to december 2014 full stack development of a tool for the marketing department to analyze and predict the marketing campaigns of competitors\n",
      "---\n",
      " facebook data mining and text analysis\n",
      "---\n",
      " usage of google analytics api\n",
      "---\n",
      " optimization of the big data dashboard front end teaching/graduate assistant high 5 games - new paltz, ny january 2013 to december 2014 assists faculty and students in finance, marketing, c language and matlab classes it business analyst high 5 games - new york, ny may 2014 to august 2014 intern, rotational internship through various departments including big data,\n",
      "---\n",
      " industry intelligence, consumer insights, paid advertising, technology platform and\n",
      "---\n",
      " technology partner integration\n",
      "---\n",
      " analyzed and presented a plan for needed business changes due to company growth\n",
      "---\n",
      " competitor analysis\n",
      "---\n",
      " cross matched competitors' analysis with big data to predict sales trends\n",
      "---\n",
      " optimization of the big data front end\n",
      "---\n",
      " analyzed a new strategic market the company was considering\n",
      "---\n",
      " created various automation tools to collect and publish information to increase efficiency it, cyber security trainee eads cassidian - paris (75) april 2011 to september 2011 identified the specifications required for the project\n",
      "---\n",
      " ? researched existing deployment application and supervision tools\n",
      "---\n",
      " ? integrated a light deployment/supervision tool without making software vulnerable to attack\n",
      "---\n",
      " ? created an overview document on the existing deploying application and supervision tools\n",
      "---\n",
      " ? conceived, developed and integrated a solution in the software\n",
      "---\n",
      " ? developed know-how transmission of the solution it trainee suny new york - new paltz, ny may 2010 to september 2010 adapted an existing distributed computing framework written in java for the lan\n",
      "---\n",
      " environment, making it compatible with android phones \n",
      "---\n",
      " mba in general business administration and management suny new paltz - new paltz, ny december 2014 master's in network engineering polytech grenoble - grenoble, pa 2011 skills statistical data analysis (a/b testing, regression), corporate finance, management, global business and leadership, network architecture and security, algorithm performance design, database, matlab, r, mathematica, shell scripting, full stack web developer and ocaml, bilingual english & french\n",
      "\n",
      "---\n",
      " working alongside engineering team to build common factory testing platform. this platform uses clustering algorithms to determine if a part needs to be replaced or repaired.\n",
      "---\n",
      " creating 3d visualization of common factory testing platform output data with unity game engine that can be viewed on hololens to understand the data from different points of view.\n",
      "---\n",
      " utilizing nltk to analyze descriptive problem description of failure parts and then use k-means to group the problems into clusters. this reduces the amount of time it takes for an engineer to find the corrective solution to the problem.\n",
      "---\n",
      " created a graph based threat detection system with networkx library that can detect malicious port scans initiated inside the organization's network. this allows it security to detect insider threats as well as outsider attacks.\n",
      "---\n",
      " generated time series forecasting visualization of terrorism attacks that occurred within certain ranges of raytheon facilities located around the world by using sarimax and prophet packages. business development customers use these as actionable insights to plan their new facilities and employee foreign travel routes.\n",
      "---\n",
      " employing statistical methods such as hypothesis testing and confident intervals to provide actionable insights and answer business problems. it analyst intern nextmed - tucson, az october 2014 to may 2015 \n",
      "---\n",
      " supported it team on various development projects to improve the business operation processes.\n",
      "---\n",
      " assisted in integrating legacy database system to new oracle database system.\n",
      "---\n",
      " organized, collected, and generated it key performance indicators monthly for upper management. cyber security intern honeywell - tempe, az june 2014 to september 2014 \n",
      "---\n",
      " assisted with research proposals, elicited requirements and domain knowledge from customers, researched cyber security threats for cyber physical systems, and presented research results to customers and honeywell management for program status reporting.\n",
      "---\n",
      " trained as a security analyst and closed over 200 security incident tickets. \n",
      "---\n",
      " master of science in management information systems in business intelligence university of arizona - tucson, az may 2015 bachelor of science in management information systems in management information systems university of arizona - tucson, az may 2014 skills pandas (3 years), scikit learn (3 years), python (3 years), nltk (2 years), sql (3 years), hadoop (1 year), machine learning, spark, big data, nlp, excel, microsoft office, business intelligence additional information skills\n",
      "---\n",
      " classification, regression, clustering, neural nets, nlp, data mining, statistical analysis, time series forecasting, and feature engineering\n",
      "---\n",
      " python: scikit-learn, numpy, pandas, scipy, tensorflow, keras, nltk, statsmodels, seaborn, matplotlib, and plotly\n",
      "---\n",
      " hadoop: spark, pyspark, hive, zeppelin, sqoop, and ambari\n",
      "---\n",
      " sql/plsql, sqldeveloper, sap crystal reports, tableau, rapid miner, unity, and ms office.\n",
      "---\n",
      " tcp/ip, lan, wan, network design, ethical hacking, and risk assessment.\n",
      "\n",
      "---\n",
      " developing machine learning models to classify customer's intent and provide real-time recommendation to customer care agents on chat.\n",
      "---\n",
      " designed metrics and developed processes to evaluate the performance of the recommendation system and to get continuous data-driven\n",
      "---\n",
      " insights for areas of improvements.\n",
      "---\n",
      " setup annotation schemes and use active learning to prioritize data to be labeled for significant system improvement.\n",
      "---\n",
      " the improved dialogue-based recommendation system lead to 17% lift in sales for our telecommunication clients.\n",
      "---\n",
      " worked with clustering, classification (random forest, svm, rnn and more), word embedding's, topic modeling and more. software developer (python, java, aws, git) niki.ai - bengaluru, karnataka october 2015 to july 2016 \n",
      "---\n",
      " conceptualised and implemented the nlp backend for users to shop using the chatbot in simple human understandable messages.\n",
      "---\n",
      " improved understanding capability of the chatbot by around 30% by standardizing entity extraction across the application.\n",
      "---\n",
      " developed lightweight restful api's to interact with the merchants and users to resolve user queries from android application. summer intern (python) the university of sheffield - sheffield june 2015 to september 2015 \n",
      "---\n",
      " developed an algorithm to summarize comments on online newspapers. used lda to cluster user comments according to topics\n",
      "---\n",
      " ranked comments in each cluster using pagerank. top ranked comment from each cluster were combined to form a summary.\n",
      "---\n",
      " summaries formed were significantly better than baseline system at significance level of 0.05. research assistant (python, terrier search engine) da-iict - gandhinagar, gujarat january 2014 to april 2014 \n",
      "---\n",
      " developed an algorithm for a legal recommendation system to find and summarize the past legal cases similar to the searched case.\n",
      "---\n",
      " expanded queries using blind and actual relevance feedback for efficient retrieval of documents. \n",
      "---\n",
      " master's in data science illinois institute of technology - chicago, il march 2016 to december 2017 master's in advanced computer science university of sheffield - sheffield september 2014 to september 2015 bachelor's in information and communication technology dhirubhai ambani institute of information and communication technology - gandhinagar, gujarat july 2010 to may 2014 skills git, hadoop, hive, python, keras, matplotlib, numpy, pandas, tensorflow, machine learning, hadoop, nlp, mysql, postgresql, java, sql, nltk, keras, scikit-learn, gensim, aws, stanford corenlp, hypothesis testing, spark, r links https://www.linkedin.com/in/ishita-agarwal-ds https://github.com/github-ish\n",
      "\n",
      "---\n",
      " development and problem solving skills. previously, improved time optimization for web application and improved\n",
      "---\n",
      " predictive maintainance performance work experience data scientist intern live in bing - binghamton, ny september 2018 to may 2019 \n",
      "---\n",
      " model creation: created machine learning model for predictive maintenance for electricity utilization\n",
      "---\n",
      " data storage: wrote python script on raspberry pi3 to capture real-time data and storing in google bigquery\n",
      "---\n",
      " tables\n",
      "---\n",
      " data analysis: organized collected data on daily, monthly and yearly basis using scheduling query techniques\n",
      "---\n",
      " notification: worked on notifying user for any unexpected spike after observing data patterns\n",
      "---\n",
      " achievement: estimated 80% accurate reading of future electricity usage using linear regression model java developer pronto infotech - mumbai, maharashtra october 2014 to february 2016 development: involved in the requirement analysis and actively involved in developing various business layer\n",
      "---\n",
      " components of icici cms multi-tiered web based system, over spring boot and hibernate framework along with oracle 10g database at backend\n",
      "---\n",
      " programming standard: complied with and contributed to standards and procedures to ensure development\n",
      "---\n",
      " consistency\n",
      "---\n",
      " testing: supported programming changes during quality assurance, user acceptance testing, and post\n",
      "---\n",
      " implementation\n",
      "---\n",
      " interation with client: analyzed and designed program changes for any change in requirement after interacting\n",
      "---\n",
      " with client\n",
      "---\n",
      " attainment: achieved 50% time-cost reduction by eliminating java code and adopting stored procedures\n",
      "---\n",
      " data migration: scheduled job to migrate day-to-day website data into system's database \n",
      "---\n",
      " master of science in computer science in computer science binghamton university, suny, watson school of engineering - binghamton, ny august 2017 to may 2019 bachelor of engineering in information technology in information technology university of mumbai, vidyalankar institute of technology - mumbai, maharashtra september 2009 to may 2013 skills oracle, oracle 10g, pl/sql, sql, eclipse, java j2ee, java, hibernate, spring, jsp links https://www.linkedin.com/in/dhakegaurang additional information skills\n",
      "---\n",
      " good level: core java, hibernate, struts, spring boot, restful apis, maven, ant, oracle 10g, pl/sql, eclipse\n",
      "---\n",
      " basic level: python 3, html, css, javascript, jquery, xml\n",
      "\n",
      "---\n",
      " to be represented as a single menu on bootler's site. completely rewrote the data pipeline, and created the food and alcohol pair- ing recommender. designed and hacked together the system that is used to submit orders on behalf of mobile app user to other\n",
      "---\n",
      " food delivery sites. additionally wrote some web scrapers for new services to be added to the site. data scientist metis - san francisco, ca january 2016 to may 2016 12 week immersive data science fellowship focusing on supervised and unsupervised machine learning methods, data mining, web\n",
      "---\n",
      " scraping, data visualization, sql/nosql databases, statistical methods, and other various data science topics\n",
      "---\n",
      " over a course of 3 months. tools and libraries include: python, pandas, sklearn, sql, mongodb, flask, javascript, d3.js, aws, and pybrain data scientist epsilon financial llc - west lafayette, in may 2015 to january 2016 quantitative hedge fund startup formed by a group of purdue students. responsibilities included designing and implementing models to aide the algorithm's predictions, optimizing the existing models for both speed and memory, meeting on a daily basis to\n",
      "---\n",
      " discuss strategy, and a host of other various tasks that go with being in a small startup company. regularly helped pitch the busi- ness to mentors and investors alike. mostly responsible for the design of the sentiment analysis portion of the algorithm. network administrator/service technician networkinguys, inc - lakewood, oh january 2014 to january 2016 \n",
      "---\n",
      " administered linux and windows servers, vbs/batch scripting, active directory, data/backup management.\n",
      "---\n",
      " wrote a lot of the custom internal facing tools that made a large portion of our day to day work fully automated.\n",
      "---\n",
      " secured networks and servers to be pci compliant by conducting penetration testing using kali linux/openvas platform.\n",
      "---\n",
      " configured routers and switches, setup patch cabling, setup and troubleshooted small to medium sized networks \n",
      "---\n",
      " computer science and statistics purdue university - college of science - west lafayette, in august 2014 to january 2016 skills c/c++, c++, javascript, d3.js, python, numpy, pandas, dynamodb, machine learning, nosql, sql, selenium, deployment, visualization, statistics additional information skills\n",
      "---\n",
      " programming: experienced in python. familiar with c/c++, and javascript. good working knowledge of cs fundamentals.\n",
      "---\n",
      " data processing tools: strong knowledge of the pydata stack (pandas, numpy, sci-kit learn, statsmodels). good sql skills. confi-\n",
      "---\n",
      " dent with nosql technologies, and very experienced in webscraping frameworks such as scrapy, selenium, etc.\n",
      "---\n",
      " statistics and ml: strong mathematical skills, and working knowledge of statistical and machine learning methods.\n",
      "---\n",
      " cloud analytics and deployment: experienced with aws, do, and \n",
      "---\n",
      "nix systems. experience with server-less (lambda/dynamodb).\n",
      "---\n",
      " visualization/web: able to build visuals in d3.js, familiar with front-end technologies and have experience with meteor.js.\n",
      "\n",
      "---\n",
      " experienced in facilitating the entire life-cycle of a data science project: data extraction, data pre-processing, featureengineering, dimensionality reduction, algorithm implementation and validation.\n",
      "---\n",
      " proficient in data transformations using log, square-root, reciprocal, differencing and complete box-cox transformation depending upon the dataset.\n",
      "---\n",
      " knowledge of normality tests like shapiro-wilk, anderson-darling.\n",
      "---\n",
      " adept at analysis of missing data by exploring correlations and similarities, introducing dummy variables for missingness, and choosing from imputation methods such iterative imputer on python.\n",
      "---\n",
      " experienced in machine learning techniques such as regression and classification models like linear, polynomial, support vector machines, decision trees, logistic regression.\n",
      "---\n",
      " experienced in ensemble learning using bagging, boosting & random forests\n",
      "---\n",
      " clustering like k-means.\n",
      "---\n",
      " in-depth knowledge of dimensionality reduction (pca, lda), hyper-parameter tuning, model regularization (ridge, lasso, elastic net) and grid search techniques to optimize model performance.\n",
      "---\n",
      " adept with python and oop concepts such as inheritance, polymorphism, abstraction, association, etc.\n",
      "---\n",
      " hands on experience in artificial neural networks, deep learning, convolution neural networks.\n",
      "---\n",
      " expertise in creating executive tableau dashboards for data visualization and deploying it to the servers\n",
      "---\n",
      " skilled in using tidyverse framework in r and pandas in python for performing exploratory data analysis.\n",
      "---\n",
      " proficient in data visualization tools such as tableau and powerbi, big data tools such as hadoop hdfs, spark and mapreduce and microsoft excel (vlookup, pivot tables).\n",
      "---\n",
      " experience in web data mining with python's scrapy and beautifulsoup packages along with working knowledge of natural language processing (nlp) to analyze text patterns.\n",
      "---\n",
      " experience with python libraries including numpy, pandas, scipy, scikit-learn&statsmodels, matplotlib, seaborn, nltk and r libraries like ggplot2, dplyr. work experience data scientist vf corporation - greensboro, nc june 2018 to present description:\n",
      "---\n",
      " vf corporation is an american worldwide apparel and footwear company. worked on the recommender system by implementing sentiment analysis on other people reviews and extract the best product for the user to buy.\n",
      "---\n",
      " reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives.\n",
      "---\n",
      " extracted data by web-scraping through the reviews using beautiful soup.\n",
      "---\n",
      " involved in various pre-processing phases of text-data like tokenization, stemming, lemmatization and converting the raw text data to structured data.\n",
      "---\n",
      " performed data collection, data cleaning, feature scaling, feature engineering, validation, visualization, report findings, develop strategic uses of data by python libraries like numpy, pandas, scipy, matplotlib, scikit-learn.\n",
      "---\n",
      " used tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data.\n",
      "---\n",
      " implemented various statistical techniques to manipulate the data like missing data imputation, principal component analysis for dimension-reduction.\n",
      "---\n",
      " constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like bag of words model, tf-idf, word2vec.\n",
      "---\n",
      " employed statistical methodologies such as a/b test, experiment design and hypothesis testing and deployed models on docker.\n",
      "---\n",
      " performed na\\xefve bayes, knn, logistic regression, random forest, svm and kmeans to categorize customers into certain groups.\n",
      "---\n",
      " employed various metrics such as cross-validation, logloss function, confusion matrix, rocauc to evaluate the performance of each model.\n",
      "---\n",
      " using nlp developed deep learning algorithms for analyzing text, over the existing dictionary-based approaches.\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), tensorflow, nlp data scientist siriusxm - washington, dc april 2017 to may 2018 description:\n",
      "---\n",
      " siriusxm is a broadcasting company that provides satellite radio and online radio. the project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account.\n",
      "---\n",
      " performed data collection, data cleaning, data visualization using python, deep feature synthesis and extracted key statistical findings to develop business strategies.\n",
      "---\n",
      " since sound is represented in the form of audio signals, parameters like frequency, decibel, timbre, pitch were usedfor analysis.\n",
      "---\n",
      " used librosa ( python library) to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound.\n",
      "---\n",
      " cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold.\n",
      "---\n",
      " created 2-d convolution neural networks using keras on gpu's by extracting different number of mfcc features using librosa.\n",
      "---\n",
      " used global-temporal pooling layer to effectively compute statistics of learned features across time.\n",
      "---\n",
      " implemented regularization methods like dropout, lasso regression and ridge regression to prevent the model from overfitting.\n",
      "---\n",
      " final model was selected by evaluating them using various metrics like accuracy, confusionmatrix, precision, recall.\n",
      "---\n",
      " environment: python (pandas, scikit, numpy), tensorflow, keras, librosa. data scientist swiggy - in november 2014 to january 2017 description:\n",
      "---\n",
      " swiggy is online food delivery company in india. the project was predicting deals and coupons for frequent customers of the company.\n",
      "---\n",
      " participated in all phases of project life cycle including data collection, data mining, data cleaning, developing models, validation and creating reports.\n",
      "---\n",
      " performed data cleaning on a huge dataset which had missing data and extreme outliers from hadoop workbooks and explored data to draw relationships and correlations between variables.\n",
      "---\n",
      " performed data-preprocessing on messy data including imputation, normalization, scaling, and feature engineering using scikit-learn.\n",
      "---\n",
      " conducted exploratory data analysis using python matplotlib and seaborn to identify underlying patterns and correlations between features.\n",
      "---\n",
      " build classification models based on logistic regression, decision trees, support vector machine to predict the probability of a customer using the application.\n",
      "---\n",
      " employed ensemble learning techniques such as random forests and ada gradient boosting to improve the model performance by 10%.\n",
      "---\n",
      " used various metrics such as f-score, roc and auc to evaluate the performance of each model and 5-fold cross validation to test the models with different batches of data to optimize the models.\n",
      "---\n",
      " implemented and tested the model on aws ec2 and collaborated with development team to get the best algorithms and parameters.\n",
      "---\n",
      " prepared data-visualization designed dashboards with tableau, and generated complex reports including summaries and graphs to interpret the findings to the team.\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), amazon web services, jupyter notebook, tableau data analyst - python developer bigbasket - in july 2012 to october 2014 description:\n",
      "---\n",
      " bigbasket is the indian online grocery delivery service. my responsibilities included working on restful web services on python flask and working on a team building a predictive model to enhance the online shopping for the users.\n",
      "---\n",
      " worked on both legacy data and new data mostly built around the user experience and grocery inventory available.\n",
      "---\n",
      " performed data analysis on target data after transfer to data warehouse.\n",
      "---\n",
      " created etl solution using ms sql server and worked with agile and test-driven development within sdlc.\n",
      "---\n",
      " worked on restful web services on python flask and built primary functions for classification.\n",
      "---\n",
      " conducted data preparation and outlier detection using pythonand implemented logistic regression, random forest, na\\xefve bayes classifier for classification for recommendation.\n",
      "---\n",
      " employed k-fold cross-validation to test and verify the model accuracy.\n",
      "---\n",
      " worked with the team to host data and certain web interfaces on amazon web services ec2 and store data on s3 bucket.\n",
      "---\n",
      " worked with team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run.\n",
      "---\n",
      " presented executive dashboards and scorecards to visualize and present trends in the data using excel and python (matplotlib).\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), amazon web services, python flask, rest apis, linux \n",
      "---\n",
      " bachelor's skills amazon web services, hadoop, hdfs, mapreduce, python, ggplot2, matplotlib, anova, mapreduce, kafka, data visualization, hadoop, mongodb, database, microsoft sql server, sql server, mysql, oracle, postgresql, sql additional information skills\n",
      "---\n",
      " languages python, r, matlab, sql\n",
      "---\n",
      " database mysql, postgresql, oracle, mongodb, microsoft sql server\n",
      "---\n",
      " statistical tests hypothesis testing, anova tests, t-tests, chi-square fit test.\n",
      "---\n",
      " validation techniques k-fold cross validation, out of the box estimates, a/b tests.\n",
      "---\n",
      " optimization techniques\n",
      "---\n",
      " gradient descent, stochastic gradient descent, mini-batch gradient descent, gradient optimization - adam, momentum, rmsprop\n",
      "---\n",
      " data visualization tableau, microsoft powerbi, ggplot2, matplotlib, seaborn\n",
      "---\n",
      " big data apache hadoop, hdfs, kafka, mapreduce, spark\n",
      "---\n",
      " cloud technologies amazon web services\n",
      "---\n",
      " tools and software pycharm, xcode, jupyter notebook, microsoft sql, linux, unix, microsoft office\n",
      "\n",
      "---\n",
      " around 6+ years of experience in it field with 3+ years as data scientist with strong technical and business experience, and communication skills to drive high-impact business outcomes through data-driven innovations and decisions.\n",
      "---\n",
      " expertise in statistical analysis, predictive modeling, text mining, supervised learning, unsupervised learning, and reinforcement learning\n",
      "---\n",
      " strong mathematical background in linear algebra, probability, statistics, differentiation and integration\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data\n",
      "---\n",
      " proficient in data mining methods, factor analysis, anova, hypothetical testing, normal distribution and other advanced statistical modeling both linear and nonlinear (logistic, linear, na\\xefve bayes, decision trees, random forest, neural networks, svm, clustering, knn)\n",
      "---\n",
      " experience with deep learning techniques such as convolutional neural networks, recurrent neural networks by using keras and tensorflow\n",
      "---\n",
      " worked on several python packages like numpy, pandas, scikit learn, matplotlib, beautiful soup, pickle, scipy, python, pytables etc.\n",
      "---\n",
      " proficient in implementing dimensionality reduction techniques like principal component analysis, t-stochastics neighborhood embedding (t-sne), and linear discriminant analysis (lda)\n",
      "---\n",
      " expertise on distributed computing, hadoop architecture and its ecosystem components like hdfs, map reduce, hive, impala, spark (pyspark) and kafka.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0, jupyter notebook 4.x.\n",
      "---\n",
      " expertise in all aspects of software development lifecycle (sdlc) from requirement analysis, design, development coding, testing, implementation, and maintenance\n",
      "---\n",
      " hands on advanced sql experience summarizing, transforming, segmenting, joining datasets.\n",
      "---\n",
      " well experienced in normalization & de-normalization techniques for optimum performance in relational and dimensional database environments\n",
      "---\n",
      " experience in visualization tools like tableau 9.x, 10.x for creating dashboards\n",
      "---\n",
      " experience in working on both windows, linux platforms\n",
      "---\n",
      " participated in all phases of machine learning and data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " leveraged disparate data sources that provide deep customer insight including online transactional data, web data, payment, orders history and marketing campaigns exposure data.\n",
      "---\n",
      " implemented the end-to-end platform for performing user behavior analytics using unsupervised machine learning.\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " identified outliers and inconsistencies in data by conducting exploratory data analysis (eda) using python numpy and seaborn to see the insights of data and validate each feature.\n",
      "---\n",
      " validated models using cross-validation and loss function to measure model performance. created confusion matrix and roc.\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " performed price sensitivity and variation analysis across different marketing channels and conducted exploratory data analysis on variables such as lifetime value and profit score.\n",
      "---\n",
      " built data pipelines, implemented code modularization involving package creation and co-developed rest api's using flask for production deployment.\n",
      "---\n",
      " performed data discovery and build a stream that automatically retrieves data from multitude of sources (sql databases, external data such as social network data, user reviews) to generate kpi's using tableau.\n",
      "---\n",
      " analyzed the data using various machine learning algorithms whether to extend/not credit limit to an existing applicant and to approve/not new credit line to a new applicant will likely result in profit or loss based on various circumstances like credit history, utilization rate, income, age, location, hard enquiries & number of deliquesces.\n",
      "---\n",
      " extracted terabytes of structured and unstructured data by using sql queries and performed data mining tasks including handling missing data, data wrangling, feature scaling, outlier analysis in python by importing pandas.\n",
      "---\n",
      " conducted data investigation, discovery & mapping tools to scan every single data record.\n",
      "---\n",
      " performed data analysis, data validation, data cleansing, and data verification to identify data mismatch using relational data modeling (3nf) and dimensional data modeling.\n",
      "---\n",
      " performed exploratory data analysis on all the features to understand feature importance and analyzed the behavior of features by using different statistical approaches.\n",
      "---\n",
      " studied the feature distribution with the help of probability density function, cumulative distribution function, percentiles, quantiles to draw some insights.\n",
      "---\n",
      " developed automated model training, testing & deployment via machine learning continuous delivery pipelines.\n",
      "---\n",
      " built decision tree model from the set of training data using the information entropy and the attribute with the highest normalized information gain is chosen to make the decision of credit approval.\n",
      "---\n",
      " used ml algorithms logistic regression, support vector machine, k nearest neighbors, na\\xefve bayes, bagging, boosting, ensemble learning to analyze the data based on the features selected for data-driven decisions.\n",
      "---\n",
      " performed text analysis on the reviews of the products using nlp techniques like bag of words, term frequency-inverse document frequency, word2vec, average word2vec with help of nltk, beautiful soup libraries.\n",
      "---\n",
      " used machine learning algorithms to forecast the company's short-term and long-term growth in terms of revenue, number of customers, various costs, stock changes etcetera.\n",
      "---\n",
      " used classified instances, relative operating characteristic curve (roc) and confusion matrix to find the accuracy of the models built.\n",
      "---\n",
      " acquired knowledge on designing, iterating and fine-tuning neural network model's architecture for runtime efficiency to achieve optimal performance.\n",
      "---\n",
      " visualized results in python using matplotlib, seaborn libraries of scikit-learn and used tableau to create the interactive dashboards to present results for team members, management and clients\n",
      "---\n",
      " environment: anaconda, python, r studio, jupyter notebook, vs code, spyder, pycharm, ssms, unix, tableau, jira, hdfs, spark, impala, hive, hue. python developer, mavin solution hyderabad, telangana july 2012 to august 2015 india\n",
      "---\n",
      " involved in the design and development of different web-based applications based on client's requirements.\n",
      "---\n",
      " designed use case diagrams, class diagrams, sequence diagrams and state diagrams.\n",
      "---\n",
      " learned new technical skills as required for the system like django, flask frameworks and model-view-controller (mvc) design pattern.\n",
      "---\n",
      " developed applications using flask ( python frameworks).\n",
      "---\n",
      " designed email marketing campaigns and created responsive web forms that saved data into a database using python/ django framework.\n",
      "---\n",
      " developed python scripts to read from excel files, generate xml configuration files and for generating ip access frequency lists in different data logs.\n",
      "---\n",
      " deployed web applications to google app engine. learnt to deploy projects using jenkins.\n",
      "---\n",
      " utilized pandas - python library for analyzing data and data structures.\n",
      "---\n",
      " managed large datasets using pandas data frames and sqlite.\n",
      "---\n",
      " performed front-end development for web initiatives to ensure usability, using html and css and enhanced quality, feel, and usability of consumer-facing website.\n",
      "---\n",
      " tested all completed work to ensure proper and error free functionality.\n",
      "---\n",
      " collaborated with a team of instructors and programmers to develop the curriculum and guidelines for workshops to teach the logic of programming.\n",
      "---\n",
      " created and ran custom sql queries, stored procedures and created an application to store client phone calls and emails that were routed to various developers.\n",
      "---\n",
      " performed data profiling and analysis, applied various data cleansing rules, designed data standards, architecture and designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " environment: python, django, flask, sqlite, ssms, google app engine, jenkins, pandas, html, css python developer sutherland - hyderabad, telangana may 2011 to june 2012 india\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " actively involved in interacting with front end users, project lead and business analyst to gather user requirements and online system specifications.\n",
      "---\n",
      " followed agile methodologies to manage full life-cycle development of the project.\n",
      "---\n",
      " designed and developed communication between client and server using secured web services.\n",
      "---\n",
      " written backend programming in python and used the django framework to develop the application.\n",
      "---\n",
      " participated in entire lifecycle of the projects including design, development, and deployment, testing and implementation and support.\n",
      "---\n",
      " implemented user interface guidelines and standards throughout the development and maintenance of the website using the html5, css3, javascript\n",
      "---\n",
      " developed views and templates with python and django's view controller and templating language to create a user-friendly website interface.\n",
      "---\n",
      " developed restful services using django.\n",
      "---\n",
      " developed and tested many features for dashboard using python, css, javascript.\n",
      "---\n",
      " used javascript and xml to update a portion of a webpage.\n",
      "---\n",
      " successfully migrated the django database from sqlite3 to postgresql with complete data integrity.\n",
      "---\n",
      " worked on jenkins continuous integration tool for deployment of project.\n",
      "---\n",
      " created custom t-sql procedures to read data from flat files to dump to sql server database using sql server import and export data wizard.\n",
      "---\n",
      " developed user defined functions based on the requirements and used various built-in functions.\n",
      "---\n",
      " handled errors using exception handling (try, catch) extensively for the ease of debugging and displaying the error messages in the application.\n",
      "---\n",
      " developed batch scripts for scheduling data migration scripts.\n",
      "---\n",
      " created clustered, non-clustered indexes and indexed views to optimize the queries performance.\n",
      "---\n",
      " coordinated with onsite folks and mentored the offshore team\n",
      "---\n",
      " worked pl/sql in oracle database for writing queries, functions, stored procedures and triggers.\n",
      "---\n",
      " environment: python, django, javascript, html, css, xml, mysql, t-sql, ssms, ms- excel, ms-word, t-sql, windows server.\n",
      "---\n",
      " tools & technologies\n",
      "---\n",
      " languages & packages python, sql, tensorflow, pyspark, numpy, pandas, keras, nltk, caffe\n",
      "---\n",
      " languages & packages python, sql, numpy, pandas, scikit-learn, matplotlib tensorflow, keras, nltk, tableau, mysql.\n",
      "---\n",
      " databases hadoop ecosystem sql server. hdfs, map reduce, hive, impala, spark (pyspark) and kafka\n",
      "---\n",
      " mathematical matrix operations, differentiation, integration, probability, statistics, linear algebra, geometry\n",
      "---\n",
      " machine learning algorithms\n",
      "---\n",
      " logistic regression, linear regression, k means clustering algorithm, decision trees, support vector machines, na\\xefve bayes, hierarchical clustering.\n",
      "---\n",
      " deep learning techniques\n",
      "---\n",
      " artificial neural networks, convolutional neural networks, multi-layer perceptron's, recurrent neural networks, lstm, back propagation, chain rule, choosing activation functions, drop out, optimization algorithms.\n",
      "---\n",
      " user interfaces html, css, java script, xml.\n",
      "---\n",
      " version control tools git\n",
      "---\n",
      " visualization tools tableau, plotly.\n",
      "---\n",
      " operating systems windows, linux\n",
      "---\n",
      " methodologies agile, scrum \n",
      "---\n",
      " master's skills python, microsoft office, excel, sql, r programming, machine learning, deep learning, hadoop, ms office, powerpoint, access links http://www.linkedin.com/in/mounika-t-23923a18a\n",
      "\n",
      "---\n",
      " around 7+ years of experience in it field with 3+ years as data scientist with strong technical and business experience, and communication skills to drive high-impact business outcomes through data-driven innovations and decisions .\n",
      "---\n",
      " expertise in statistical analysis, predictive modeling, text mining, supervised learning, unsupervised learning, and reinforcement learning.\n",
      "---\n",
      " expert in developing nlp models for topic extraction, sentiment analysis\n",
      "---\n",
      " strong mathematical background in linear algebra, probability, statistics, differentiation and integration\n",
      "---\n",
      " having good experience in amazon web services (aws).\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data\n",
      "---\n",
      " proficient in data mining methods, factor analysis, anova, hypothetical testing, normal distribution and other advanced statistical modeling both linear and nonlinear (logistic regression, linear regression, na\\xcfve bayes, decision trees, random forest, neural networks, svm, clustering, knn)\n",
      "---\n",
      " experience with deep learning techniques such as convolutional neural networks, recurrent neural networks by using keras , tensorflow and opencv\n",
      "---\n",
      " worked on several python packages like numpy, pandas, scikit learn, matplotlib, beautiful soup, pickle, scipy, python, pytables etc.\n",
      "---\n",
      " proficient in implementing dimensionality reduction techniques like principal component analysis, t-stochastics neighborhood embedding (t-sne), and linear discriminant analysis (lda)\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0, jupyter notebook 4.x, rstudio, rshiney.\n",
      "---\n",
      " expertise in all aspects of software development lifecycle (sdlc) from requirement analysis, design, development coding, testing, implementation, and maintenance\n",
      "---\n",
      " hands on advanced sql experience summarizing, transforming, segmenting, joining datasets.\n",
      "---\n",
      " experience in visualization tools like tableau 9.x, 10.x for creating dashboards\n",
      "---\n",
      " participated in all phases of machine learning and data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " leveraged disparate data sources that provide deep customer insight including online transactional data, web data, payment, orders history and marketing campaigns exposure data.\n",
      "---\n",
      " implemented the end-to-end platform for performing user behavior analytics using unsupervised machine learning.\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " designed and implemented system architecture for amazonec2 based cloud -hosted solution for the client.\n",
      "---\n",
      " identified outliers and inconsistencies in data by conducting exploratory data analysis (eda) using python numpy and seaborn to see the insights of data and validate each feature.\n",
      "---\n",
      " prepare scripts to ensure proper data access, manipulation and reporting functions with r programming.\n",
      "---\n",
      " involved in converting hive/sql queries into spark transformations using spark rdds, and python\n",
      "---\n",
      " developed ad-hoc queries for moving data from hdfs to hive and analyzing the data using hive ql\n",
      "---\n",
      " validated models using cross-validation and loss function to measure model performance. created confusion matrix and roc.\n",
      "---\n",
      " expert in performing text mining and text classification in nlp by using tfidfvectorizer.\n",
      "---\n",
      " developed nlp models for topic extraction, sentiment analysis.\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " performed price sensitivity and variation analysis across different marketing channels and conducted exploratory data analysis on variables such as lifetime value and profit score.\n",
      "---\n",
      " built data pipelines, implemented code modularization involving package creation and co-developed rest api's using flask for production deployment.\n",
      "---\n",
      " performed data discovery and build a stream that automatically retrieves data from multitude of sources (sql databases, external data such as social network data, user reviews) to generate kpi's using tableau.\n",
      "---\n",
      " analyzed the data using various machine learning algorithms whether to extend/not credit limit to an existing applicant and to approve/not new credit line to a new applicant will likely result in profit or loss based on various circumstances like credit history, utilization rate, income, age, location, hard enquiries & number of deliquesces.\n",
      "---\n",
      " extracted terabytes of structured and unstructured data by using sql queries and performed data mining tasks including handling missing data, data wrangling, feature scaling, outlier analysis in python by importing pandas.\n",
      "---\n",
      " conducted data investigation, discovery & mapping tools to scan every single data record.\n",
      "---\n",
      " worked on predictive and what-if analysis using python from hdfs and successfully loaded files to hdfs from teradata and loaded from hdfs to hive.\n",
      "---\n",
      " performed data analysis, data validation, data cleansing, and data verification to identify data mismatch using relational data modeling (3nf) and dimensional data modeling.\n",
      "---\n",
      " performed exploratory data analysis on all the features to understand feature importance and analyzed the behavior of features by using different statistical approaches.\n",
      "---\n",
      " studied the feature distribution with the help of probability density function, cumulative distribution function, percentiles, quantiles to draw some insights.\n",
      "---\n",
      " developed automated model training, testing & deployment via machine learning continuous delivery pipelines.\n",
      "---\n",
      " built decision tree model from the set of training data using the information entropy and the attribute with the highest normalized information gain is chosen to make the decision of credit approval.\n",
      "---\n",
      " created s3 buckets and managing policies for s3 buckets and utilized s3 bucket and glacier for archival storage and backup on aws.\n",
      "---\n",
      " experienced in designing and deploying aws solutions using ec2, s3, ebs, an elastic load balancer (elb) and auto scaling groups.\n",
      "---\n",
      " able to create scripts for system administration and aws using languages such as bash and python.\n",
      "---\n",
      " used ml algorithms logistic regression, support vector machine, k nearest neighbors, na\\xefve bayes, bagging, boosting, ensemble learning to analyze the data based on the features selected for data-driven decisions.\n",
      "---\n",
      " performed text analysis on the reviews of the products using nlp techniques like bag of words, term frequency-inverse document frequency, word2vec, average word2vec with help of nltk, beautiful soup libraries.\n",
      "---\n",
      " used machine learning algorithms to forecast the company's short-term and long-term growth in terms of revenue, number of customers, various costs, stock changes etcetera.\n",
      "---\n",
      " used classified instances, relative operating characteristic curve (roc) and confusion matrix to find the accuracy of the models built.\n",
      "---\n",
      " acquired knowledge on designing, iterating and fine-tuning neural network model's architecture for runtime efficiency to achieve optimal performance.\n",
      "---\n",
      " visualized results in python using matplotlib, seaborn libraries of scikit-learn and used tableau to create the interactive dashboards to present results for team members, management and clients\n",
      "---\n",
      " environment: anaconda, python, r studio, jupyter notebook, vs code, spyder, pycharm, ssms, unix, tableau, jira, hdfs, spark, impala, hive, hue. python developer mavin solution - hyderabad, telangana july 2012 to august 2015 india\n",
      "---\n",
      " involved in the design and development of different web-based applications based on client's requirements.\n",
      "---\n",
      " designed use case diagrams, class diagrams, sequence diagrams and state diagrams.\n",
      "---\n",
      " learned new technical skills as required for the system like django, flask frameworks and model-view-controller (mvc) design pattern.\n",
      "---\n",
      " developed applications using flask ( python frameworks).\n",
      "---\n",
      " designed email marketing campaigns and created responsive web forms that saved data into a database using python/ django framework.\n",
      "---\n",
      " developed python scripts to read from excel files, generate xml configuration files and for generating ip access frequency lists in different data logs.\n",
      "---\n",
      " deployed web applications to google app engine. learnt to deploy projects using jenkins.\n",
      "---\n",
      " utilized pandas - python library for analyzing data and data structures.\n",
      "---\n",
      " managed large datasets using pandas data frames and sqlite.\n",
      "---\n",
      " performed front-end development for web initiatives to ensure usability, using html and css and enhanced quality, feel, and usability of consumer-facing website.\n",
      "---\n",
      " tested all completed work to ensure proper and error free functionality.\n",
      "---\n",
      " collaborated with a team of instructors and programmers to develop the curriculum and guidelines for workshops to teach the logic of programming.\n",
      "---\n",
      " created and ran custom sql queries, stored procedures and created an application to store client phone calls and emails that were routed to various developers.\n",
      "---\n",
      " performed data profiling and analysis, applied various data cleansing rules, designed data standards, architecture and designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " environment: python, django, flask, sqlite, ssms, google app engine, jenkins, pandas, html, css python developer sutherland - hyderabad, telangana may 2011 to june 2012 india\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " actively involved in interacting with front end users, project lead and business analyst to gather user requirements and online system specifications.\n",
      "---\n",
      " followed agile methodologies to manage full life-cycle development of the project.\n",
      "---\n",
      " designed and developed communication between client and server using secured web services.\n",
      "---\n",
      " written backend programming in python and used the django framework to develop the application.\n",
      "---\n",
      " participated in entire lifecycle of the projects including design, development, and deployment, testing and implementation and support.\n",
      "---\n",
      " implemented user interface guidelines and standards throughout the development and maintenance of the website using the html5, css3, javascript\n",
      "---\n",
      " developed views and templates with python and django's view controller and templating language to create a user-friendly website interface.\n",
      "---\n",
      " developed restful services using django.\n",
      "---\n",
      " developed and tested many features for dashboard using python, css, javascript.\n",
      "---\n",
      " used javascript and xml to update a portion of a webpage.\n",
      "---\n",
      " successfully migrated the django database from sqlite3 to postgresql with complete data integrity.\n",
      "---\n",
      " worked on jenkins continuous integration tool for deployment of project.\n",
      "---\n",
      " created custom t-sql procedures to read data from flat files to dump to sql server database using sql server import and export data wizard.\n",
      "---\n",
      " developed user defined functions based on the requirements and used various built-in functions.\n",
      "---\n",
      " handled errors using exception handling (try, catch) extensively for the ease of debugging and displaying the error messages in the application.\n",
      "---\n",
      " developed batch scripts for scheduling data migration scripts.\n",
      "---\n",
      " created clustered, non-clustered indexes and indexed views to optimize the queries performance.\n",
      "---\n",
      " coordinated with onsite folks and mentored the offshore team\n",
      "---\n",
      " worked pl/sql in oracle database for writing queries, functions, stored procedures and triggers.\n",
      "---\n",
      " environment: python, django, javascript, html, css, xml, mysql, t-sql, ssms, ms- excel, ms-word, t-sql, windows server.\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tools & technologies\n",
      "---\n",
      " languages & packages python, sql, tensorflow, pyspark, numpy, pandas, keras, nltk, caffe\n",
      "---\n",
      " languages & packages python, sql, numpy, pandas, scikit-learn, matplotlib tensorflow, keras, nltk, tableau, mysql.\n",
      "---\n",
      " databases hadoop ecosystem sql server. hdfs, map reduce, hive, impala, spark (pyspark) and kafka\n",
      "---\n",
      " mathematical matrix operations, differentiation, integration, probability, statistics, linear algebra, geometry\n",
      "---\n",
      " machine learning algorithms\n",
      "---\n",
      " logistic regression, linear regression, k means clustering algorithm, decision trees, support vector machines, na\\xefve bayes, hierarchical clustering.\n",
      "---\n",
      " deep learning techniques\n",
      "---\n",
      " artificial neural networks, convolutional neural networks, multi-layer perceptron's, recurrent neural networks, lstm, back propagation, chain rule, choosing activation functions, drop out, optimization algorithms.\n",
      "---\n",
      " user interfaces html, css, java script, xml.\n",
      "---\n",
      " version control tools git\n",
      "---\n",
      " tools spyder, jupyter notebook, vs code, r studio, sql server, oracle, anconda, r shiney, tableau, plotly.\n",
      "---\n",
      " operating systems windows, linux\n",
      "---\n",
      " methodologies agile, scrum \n",
      "---\n",
      " master's skills linux, business intelligence, sql, statistical analysis, nlp, aws, python, hive/sql\n",
      "\n",
      "---\n",
      " around 6 years of experience in data science , data analyst, python developer and project management.\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data, created er diagrams and schemas.\n",
      "---\n",
      " proficient in process research which requires analytic models, data inputs and output, analytic metrics and user interface needs.\n",
      "---\n",
      " developed predictive models using decision tree, random forest, naive bayes, logistic regression, cluster analysis, and artificial neural networks.\n",
      "---\n",
      " experienced with machine learning algorithms such as logistic regression, random forest, knn, support vector machines, neural networks, linear/non-linear regression and k-means\n",
      "---\n",
      " expertise in employing techniques for supervised and unsupervised learning(clustering, classification, pca, decision trees, knn, svm), predictive analytics\n",
      "---\n",
      " excellent understanding of agile and scrum development methodology\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, numpy and pandas for data analysis.\n",
      "---\n",
      " hands on experience of data science libraries in python such as pandas, numpy, scikit-learn, matplotlib, seaborn, keras.\n",
      "---\n",
      " equipped with experience in utilizing statistical techniques which include correlation, hypothesis modeling, inferential statistics as well as data mining and modeling techniques using linear and logistic regression, clustering, decision trees, and k-mean clustering.\n",
      "---\n",
      " mitigated risk factors through careful analysis of financial and statistical data. transformed and processed raw data for further analysis, visualization, and modeling.\n",
      "---\n",
      " experience in big data technologies like spark 1.6, spark sql, pyspark, hadoop 2.x, hive 1.x\n",
      "---\n",
      " experience in visualization tools like tableau 9.x, 10.x for creating dashboards\n",
      "---\n",
      " automated recurring reports using sql and python and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked on artificial neural networks and deep learning models using theano and keras packages using python.\n",
      "---\n",
      " worked with no sql database including cassandra and mongo db.\n",
      "---\n",
      " experienced in writing sql queries, working knowledge of rdbms like sql server 2008, no sql databases like mongo db 3.2\n",
      "---\n",
      " experience working on microsoft sql server, oracle, hadoop/hive.\n",
      "---\n",
      " analytical, performance-focused, and detail-oriented professional, offering in-depth knowledge of data analysis and statistics\n",
      "---\n",
      " utilized complex sql queries for data manipulation.\n",
      "---\n",
      " excellent team player and self-starter possess good communication skills.\n",
      "---\n",
      " experience in various phases of software development life cycle (analysis, requirements gathering, designing) with expertise in writing/documenting technical design document (tdd), functional specification document (fsd), test plans, gap analysis and source to target mapping documents.\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " c, c++, java, python, sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " responsible for predictive analysis of cro trial data to predict patient yield and recruitment timeline for clinical trials across apac & emea regions.\n",
      "---\n",
      " data was extracted extensively by using sql queries and used python packages for the data mining tasks.\n",
      "---\n",
      " performed exploratory data analysis, data wrangling and development of algorithms in python for data mining and analysis.\n",
      "---\n",
      " extensively used python's multiple data science packages like pandas, numpy, matplotlib, scipy, scikit-learn, keras and theano.\n",
      "---\n",
      " ensure complete understanding for the broader team on the datasets and how they tie to clinical trial execution, and thereby modeling & analytics.\n",
      "---\n",
      " identify and resolve contradictions in datasets, develop effective validations and heuristics.ensure data structure is appropriate for future complex analytics initiatives and not limited to a near-term object.\n",
      "---\n",
      " discuss alternative modeling approaches, considerations, limitations, feature definition, and develop strategy to ensure objectives are met in near-term while also supporting longer-term increased sophistication and performance metrics.\n",
      "---\n",
      " investigational sites chosen for clinical trials rarely meet enrollment goals leading to delays and high costs in conducting clinical trials. we need to identify sites that have the potential to complete the clinical trial on time and under budget.\n",
      "---\n",
      " site performance model outputs and resulting top-line trial performance summary, based on site selection scenario.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions from past and future behavior.\n",
      "---\n",
      " step-by-step instructions for use of new features to facilitate verification of the new content.\n",
      "---\n",
      " used python-based data manipulation and visualization tools such as pandas, matplotlib and seaborn to clean corrupted data before generating business requested reports.\n",
      "---\n",
      " developed extension models relying on but not limited to the random forest, logistic, linear regression, support vector regression and boosting techniques like gradient boosting and xgboost for site selection.\n",
      "---\n",
      " used python programming language to graphically analyze the data and perform data mining.\n",
      "---\n",
      " performed extensive data mining to find out relevant features in an anonymized dataset using python.\n",
      "---\n",
      " explored supervised machine learning algorithms (decision tree regression, random forest regression, svm) and used parameters such as root mean squared error and mean absolute error to select the winning model.\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and customized visualization tools using python, hadoop and mongodb.\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn in python for developing various machine learning algorithms.\n",
      "---\n",
      " worked on csv, json, excel different types of files for the data cleaning and data analysis\n",
      "---\n",
      " used python for statistical operations on the data and matplotlib for the visualizing the data\n",
      "---\n",
      " ensured that the model has a low false positive rate.\n",
      "---\n",
      " managed large datasets using pandas data frames and mysql.\n",
      "---\n",
      " built various graphs for business decision-making using python matplotlib library.\n",
      "---\n",
      " identified root causes of problems, and facilitated the implementation of cost-effective solutions with all levels of management.\n",
      "---\n",
      " performed data cleaning, handled missing data, outliers, feature scaling, features engineering.\n",
      "---\n",
      " application of various ml algorithms and statistical modeling like decision trees, regression models, random forest, svm, clustering to identify volume using different packages in python.\n",
      "---\n",
      " performed data visualization with tableau and generated dashboards to present the findings\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions from past and future behavior.\n",
      "---\n",
      " worked independently and collaboratively throughout the project lifecycle including data extraction/preparation, design and implementation of scalable machine learning analysis and solutions, and documentation of results.\n",
      "---\n",
      " performed classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " performed data profiling to merge the data from multiple data sources.\n",
      "---\n",
      " knowledge of other relational database platforms such as oracle, nosql\n",
      "---\n",
      " environment: python 3, mysql, matplotlib, seaborn, linear regression, logistic regression, random forest, support vector machines, knn, tableau. python developer zen3 info solutions - hyderabad, telangana august 2014 to december 2016 \n",
      "---\n",
      " involved in building database model, apis and views utilizing python, in order to build an interactive web-based solution.\n",
      "---\n",
      " used data types like dictionaries, tuples and object -concepts based inheritance features for making complex algorithms of networks.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and amazon aws architecture.\n",
      "---\n",
      " worked on python open stack api's.\n",
      "---\n",
      " carried out various mathematical operations for calculation purpose using python libraries.\n",
      "---\n",
      " managed large datasets using panda data frames and mysql.\n",
      "---\n",
      " worked with json based rest web services.\n",
      "---\n",
      " performed testing using django's test module.\n",
      "---\n",
      " involved in agile methodologies and scrum process.\n",
      "---\n",
      " creating unit test/regression test framework for working/new code.\n",
      "---\n",
      " using subversion version control tool to coordinate team-development.\n",
      "---\n",
      " developed sql queries, stored procedures, and triggers using oracle, sql, pl/sql.\n",
      "---\n",
      " responsible for debugging and troubleshooting the web application.\n",
      "---\n",
      " supported user groups by handling target-related software issues/service requests, identifying/fixing bugs.\n",
      "---\n",
      " configured the django admin site, dashboard and created a custom django dashboard for end users with custom look and feel.\n",
      "---\n",
      " used django apis for database access.\n",
      "---\n",
      " used python for xml, json processing, data exchange and business logic implementation.\n",
      "---\n",
      " used python scripts to update the content in database and manipulate files.\n",
      "---\n",
      " worked through the entire lifecycle of the projects including design, development, and deployment, testing and implementation and support.\n",
      "---\n",
      " communicated with business users, data architects and developers to identify needs, define project scope and detailed functional and non-functional requirements.\n",
      "---\n",
      " gathered and documented requirements throughout the software development life cycle (sdlc)\n",
      "---\n",
      " prepared data mapping documents, use cases, requirements traceability matrix and worked with the user experience team (wireframe preparing team)\n",
      "---\n",
      " created requirement documentation, reviews with user representatives, recommended priorities, gave presentations and walkthroughs and obtained user sign-off.\n",
      "---\n",
      " driven daily stand up meetings offshore, iteration planning meetings and requirement gathering meetings.\n",
      "---\n",
      " extracting data from different public data repositories and from different databases and creating datasets for statistical analysis.\n",
      "---\n",
      " identified multiple dimensions and fact tables. used advance data modeling concepts of degenerated dimension, sub-dimension, factless fact table, aggregate fact tables in the multidimensional model.\n",
      "---\n",
      " extensively used tab admin and tab cmd commands in creating backups and restoring backups of tableau repository.\n",
      "---\n",
      " administered user, user groups and scheduled instances for reports in tableau.\n",
      "---\n",
      " hands-on development in creating and modifying worksheets and data visualization dashboards.\n",
      "---\n",
      " designed & developed various analytical reports from multiple data sources by blending data on a single worksheet in tableau desktop.\n",
      "---\n",
      " developed web application using struts, jsp, servlets, java beans that uses mvc design pattern\n",
      "---\n",
      " created user-friendly gui interface and web pages using html, css and jsp.\n",
      "---\n",
      " used eclipse as ide tool for creating servlets, jsp, and xml.\n",
      "---\n",
      " implemented the struts framework based on mvc design pattern.\n",
      "---\n",
      " wrote sql for jdbc prepared statements to retrieve the data from database.\n",
      "---\n",
      " worked with database administrators to solve the problems generated while creating tables for application\n",
      "---\n",
      " developed and implemented customized maintenance plans to meet the needs and keep up to date with the glitches and fixes for performances issues.\n",
      "---\n",
      " developed various uml diagrams like use cases, class diagrams, interaction diagrams (sequence and collaboration) and activity diagrams\n",
      "---\n",
      " involved in build and deploying the application using ant\n",
      "---\n",
      " environment: java, sql, xml, html, css, jsp, jdbc, eclipse ide. \n",
      "---\n",
      " bachelors in computer science in tools and technologies jntu\n",
      "\n",
      "---\n",
      " efficient data scientist with around 6 years of experience in, statistical modeling, machine learning, data mining with large data sets of structured and unstructured data and performed data acquisition, data validation, predictive modeling and data visualization.\n",
      "---\n",
      " significant industry experience and domain knowledge in healthcare, retail, banking, energy and got some domain knowledge in telecom industries.\n",
      "---\n",
      " experience in feature extraction, creating regression models, classification, predictive data modeling, and cluster analysis.\n",
      "---\n",
      " expertise in python (2.x/3.x) programming with multiple packages including numpy, pandas, matplotlib, scipy, seaborn and scikit-learn.\n",
      "---\n",
      " hands-on experience with all python libraries for data acquisition, data cleaning, data validation, predictive modeling, and data visualization tools.\n",
      "---\n",
      " experience in designing stunning visualizations using tableau software and publishing and presenting dashboards, storyline on web and desktop platforms.\n",
      "---\n",
      " worked on technologies like slack, git, svn and openpyxl for reading and writing.\n",
      "---\n",
      " strong business judgment and ability to take ambiguous problems and solve them in a structured, hypothesis-driven, and data-supported way.\n",
      "---\n",
      " hands on experience in implementing lda, na\\xefve bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " implementation experiences in machine learning and deep learning, including regression, classification, neural network, object tracking, natural language processing (nlp) using packages like tensor flow, keras, nltk, spacy.\n",
      "---\n",
      " highly skilled in advanced regression modelling, time series analysis, correlation and multivariate analysis.\n",
      "---\n",
      " experienced in machine learning classification algorithms like logistic regression, k-nn, svm, kernel svm, naive bayes, and decision tree.\n",
      "---\n",
      " experience in tuning algorithms using methods such as grid search, randomized search, k-fold cross validation and error analysis.\n",
      "---\n",
      " worked with outlier analysis with various methods like z-score value analysis, liner regression, dbscan (density based spatial clustering of applications with noise) and isolation forest\n",
      "---\n",
      " worked on gradient boosting decision trees with xgboostto improve performance and accuracy in solving problems.\n",
      "---\n",
      " also worked with several boosting methodologies like ada boost, gradient boosting and xgboost.\n",
      "---\n",
      " implemented various statistical tests like anova, a/b testing, z-test, t-test for various business cases.\n",
      "---\n",
      " validated the machine learning classifiers using accuracy, auc, roccurves and lift charts.\n",
      "---\n",
      " worked on artificial neural networks and deep learning models using theano and keras packages using python.\n",
      "---\n",
      " implemented and analyzed rnn based approaches for automatically predicting implicit relations in text. the disclosure relation has potential applications in nlp tasks like text parsing, text analytics, text summarization, conversational systems.\n",
      "---\n",
      " worked with various text analytics or word embedding libraries like word2vec, count vectorizer, glove, lda etc.\n",
      "---\n",
      " solid knowledge and experience in deep learning techniques including feed forward neural network, convolutional neural network (cnn), recursive neural network (rnn)\n",
      "---\n",
      " worked with numerous data visualization tools in python like matplotlib, seaborn, ggplot, pygal.\n",
      "---\n",
      " worked and extracted data from various database sources like oracle, sql server, db2, mongodb and teradata.\n",
      "---\n",
      " highly skilled in using hadoop, hbase, spark, and hive for basic analysis and extraction of data in the infrastructure to provide data summarization.\n",
      "---\n",
      " good knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node, secondary name node, mapreduce concepts, and ecosystems including hive and pig.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, mapreduce, and loaded data into hdfs.\n",
      "---\n",
      " experience working with ms word, ms excel, ms powerpoint, ms sharepoint, and ms project. work experience data scientist zoetis inc - kalamazoo, mi september 2018 to present \n",
      "---\n",
      " zoetis inc. is the world's largest producer of medicine and vaccinations for pets and livestock.\n",
      "---\n",
      " zoetis delivers quality medicines, vaccines and diagnostic products, which are complemented by genetic tests, bio devices and a range of services.\n",
      "---\n",
      " the project is to collect data from different sources and create a master data set.\n",
      "---\n",
      " and we do predictions on sales and profits. measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " developing data analytical databases from different sources and create a master data set.\n",
      "---\n",
      " responsible for data identification, collection, exploration, cleaning for modeling\n",
      "---\n",
      " data entry, data auditing, creating data reports and monitoring all data for accuracy\n",
      "---\n",
      " we do predictions on sales and profits using machine learning and deep learning strategies.\n",
      "---\n",
      " performed time series analysis on sales data to consider what measures to be taken for improve the sales.\n",
      "---\n",
      " manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " analysis of biological and spatial data to develop insights into precision animal management and precision medicine\n",
      "---\n",
      " implementing analytics algorithms in python, reprogramming languages\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, to visualization of the data after removing missing and outliers to fit in the model\n",
      "---\n",
      " applied isolation forest, local outlier factor from sklearn, where local filters are used unsupervised outlier detection and score each sample.\n",
      "---\n",
      " applied deep learning libraries (tensor flow, theano, torch, etc.) and scalable event stream processing architectures (e.g. lambda, cep, etc.)\n",
      "---\n",
      " performed training natural language models and reinforcement learning engines to optimize intelligent agents that automate task execution.\n",
      "---\n",
      " worked with dimensionality reduction techniques like pca, lda and ica\n",
      "---\n",
      " performed k-means clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering, customized and priority service, to improve existing profitable relationships and to avoid customer churn, etc using python.\n",
      "---\n",
      " applying clustering algorithms to group the data on their similar behavior patterns.\n",
      "---\n",
      " performed animal medicines and vaccine's sales predictive modelling by using decision trees and regressions in order to get the risk involved by giving individual scores to the customers\n",
      "---\n",
      " work with data analytics team to develop time series and optimization.\n",
      "---\n",
      " performed time series analysis on animal medicine and vaccine product sales datain order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values.\n",
      "---\n",
      " used expert level understanding of different databases in combinations for data extraction and loading, joiningdata extracted from different databases and loading to a specific database in sql\n",
      "---\n",
      " performed advanced sql queries for script executions like update, insert, and delete.\n",
      "---\n",
      " worked on hadoop ecosystem components like hadoopmapreduce, hdfs, hbase, hive, sqoop, pig including their installation and configuration.\n",
      "---\n",
      " used hive to store the data and perform data cleaning steps for huge datasets.\n",
      "---\n",
      " used self- service environment cloudera data science workbench (cdsw) to manage the data analytics pipelines, including built-in scheduling, monitoring, and email alerting\n",
      "---\n",
      " created various proof of concepts (poc) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.\n",
      "---\n",
      " implemented agile methodology for building an internal application\n",
      "---\n",
      " used tableau to generate reports with internal records, secondary sources of data, json, csv and more.which helped the support team for better marketing. data scientist mars solutions group, wi march 2017 to august 2018 the client is the largest healthcare company and offers health care products, insurance services, data analytics, payment integrity, and the project was to build predictive models for customer value analysis by applying machine learning methods, principal component analysis, and regression on large dataset.\n",
      "---\n",
      " creating statistical machine learning models for implementing customer churn, ticket routing techniques, invoice premium predictions and claim classification.\n",
      "---\n",
      " collaborated with other departments to collect and understand client business requirements.\n",
      "---\n",
      " collaborated with data engineers to gathered business requirements and filtered the data according to project requirements.\n",
      "---\n",
      " worked in importing and cleansing of data from various sources like teradata, oracle, flat files, sqlserver 2005 with high volume data\n",
      "---\n",
      " performed feature engineering including feature intersection generating, feature normalize and labelencoding with scikit-learn preprocessing.\n",
      "---\n",
      " congregated data from multiple sources and performed resampling to handle the issue of imbalanced data.\n",
      "---\n",
      " treated missing values and outliers with several techniques boxplots, z-score and db scan\n",
      "---\n",
      " explored and visualized the data to check the pattern, distribution, descriptive statistic, and correlation using python, matplotlib, and seaborn.\n",
      "---\n",
      " performed nlp tasks with nlp library corenlp, nltk and gensim.\n",
      "---\n",
      " performed text representation techniques (such as n-grams, bag of words, sentiment analysis etc.),\n",
      "---\n",
      " installed hdfs storage and data analysis tools in amazon web services (aws) cloud environment computing infrastructure.\n",
      "---\n",
      " developed etl processes for data conversions and construction of data warehouse using informatica.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " import/export data from teradata database to hdfs using sqoop.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve data from oracle database\n",
      "---\n",
      " creating data pipelines using big data tools like hadoop, spark etc.\n",
      "---\n",
      " good knowledge on hadoop components such as hdfs, job tracker, tasktracker, name node, data node, and map reduce concepts.\n",
      "---\n",
      " responsible for managing and reviewing hadoop log files.\n",
      "---\n",
      " created bucketing and partitions in hive to handle the data.\n",
      "---\n",
      " applied different dimensionality reduction techniques like principle component analysis (pca) and t-stochastic neighborhood embedding (t-sne) on feature matrix\n",
      "---\n",
      " worked with various customer analytics such as segmenting the customers, product recommendations and nlp tasks\n",
      "---\n",
      " worked with clustering algorithms like k-means, k-means++, dbscan and agglomerative hierarchical clustering to target specific group of customers to generate profitable revenue.\n",
      "---\n",
      " using nlp to sorting the email to automatically updating the records in customer relationship management (crm)\n",
      "---\n",
      " we can run natural language processing algorithms against the data and automatically extract the features or risk factors from the notes in the medical record.\n",
      "---\n",
      " performed multinomial logistic regression, random forest, decision tree, svm and more machine learning algorithms\n",
      "---\n",
      " using graphical packages produced roc curve to visually represent true positive rate versus falsepositive rate.\n",
      "---\n",
      " equally produced visualization of precision recall curve for area under the curve.\n",
      "---\n",
      " used market basket analysis, association rules analysis to identified patterns, data quality issues and leveraged insights\n",
      "---\n",
      " addressed over fitting by implementing of the algorithm regularization methods like l2 and l1\n",
      "---\n",
      " improved model's accuracy by using gradient boosting technique like light gbm and gained around 82% accuracy with random forest and 77% with logistic regression.\n",
      "---\n",
      " used k-fold cross validation technique to increase the model performance and worked with hyper parameter tuning methods like grid search.\n",
      "---\n",
      " worked with visualization tools like tableau, cognos and micro strategy to create business reports for higher management and used python visualization libraries like seaborn, matplotlib and ggplot depending on business requirements.\n",
      "---\n",
      " provided schedules, status reports, and issue resolutions to the project team, business users, and project managers data analyst / data scientist cms energy - jackson, mi january 2016 to february 2017 \n",
      "---\n",
      " cms energy is an energy company that is focused principally on utility operations.\n",
      "---\n",
      " i was responsible for building a new data science department with the help of other departments and i was able to learn how the business is operated and helped the company to grow and stay ahead of the competition. by using machine learning we improvised the predictive algorithm for pricing strategy. and we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me.\n",
      "---\n",
      " worked on data manipulation & visualization, machine learning, python, sql, nosql, mongodb, hadoop\n",
      "---\n",
      " performed advanced sql queries for script executions like update, insert, and delete.\n",
      "---\n",
      " used expert level understanding of different databases in combinations for data extraction and loading, joining data extracted from different databases and loading to a specific database in sql\n",
      "---\n",
      " programmed utilities in python that uses packages like scipy, numpy, pandas, stats model, scikit learn, xg boost, matplotlib, plotly, nltk, seaborn, bokeh.\n",
      "---\n",
      " transformed the business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " have done normalization& denormalization techniques for optimum performance in relational and dimensional database environments.\n",
      "---\n",
      " worked on customer segmentation using an unsupervised learning technique - clustering.\n",
      "---\n",
      " implemented classification using supervised learninglike logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " built models using statistical techniques and machine learning classification models like xg boost, svm, and random forest.\n",
      "---\n",
      " created various proof of concepts (poc) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.\n",
      "---\n",
      " used jupyter notebook for spark to make data manipulations\n",
      "---\n",
      " developed etl processes for data conversions and construction of data warehouse using informatica.\n",
      "---\n",
      " worked on hadoop ecosystem components like hadoop mapreduce, hdfs, hbase, hive, sqoop and pig including their installation and configuration.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " used hive to store the data and perform data cleaning steps for huge datasets.\n",
      "---\n",
      " extracted data from source xml in hdfs, preparing data for exploratory analysis using data munging\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements and work with other members of the it organization to deliver data visualization and reporting solutions to address those needs\n",
      "---\n",
      " used visualization tools like tableau for the interactive graphs\n",
      "---\n",
      " used python libraries matplotlib and seaborn for creating dashboards data analyst karvy financial services limited november 2014 to december 2015 \n",
      "---\n",
      " karvy financial services limited is a company which has been playing a very proactive role in the economic growth of india by providing loans to micro & small business segments and individuals like credit for the requirements of different sectors of economy.\n",
      "---\n",
      " industries, exports, trading, agriculture, infrastructure and the individual segments. we worked on various projects which handle customer analytics\n",
      "---\n",
      " credit risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans, identify and prevent fraud detection for transactions.\n",
      "---\n",
      " compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results\n",
      "---\n",
      " applied concepts of probability, distribution, and statistical inference on the given dataset to unearth interesting findings using comparison, t-test, f-test, r-squared, p-value etc\n",
      "---\n",
      " applied linear regression, multiple regressions, ordinary least square method, mean-variance, the theory of large numbers, logistic regression, dummy variable, residuals, poisson distribution, naive bayes, fitting function etc to data with help of scikit, scipy, numpy and pandas module of python.\n",
      "---\n",
      " applied principal component analysis (pca) based unsupervised technique to determine unusual vpn log-on time.\n",
      "---\n",
      " performed clustering with historical, demographic and behavioral data as features to implement the personalized marketing to the customers\n",
      "---\n",
      " also created classification model using logistic regression, random forests to classify dependent variable into two classes which are risky and okay\n",
      "---\n",
      " used f-score, precision, recall evaluating model performance\n",
      "---\n",
      " built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as decision trees, random forests and svm\n",
      "---\n",
      " real time analysis of customer's financial profile and providing recommendation for financial products best suited.\n",
      "---\n",
      " collected historical data and third-party data from different data sources and performed data integration using alteryx.\n",
      "---\n",
      " forecasted demand for loans and interest rates using time series analysis like arimax, varmax and holt-winters.\n",
      "---\n",
      " obtained better predictive performance of 81% accuracy using ensemble methods like bootstrap aggregation (bagging) and boosting (light gbm, gradient)\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " developed visualizations and dashboards using ggplot, tableau\n",
      "---\n",
      " prepared and presented data quality report to stakeholders to give understanding of data. python developer / data analyst symbiosys technologies - visakhapatnam, andhra pradesh january 2014 to october 2014 \n",
      "---\n",
      " genius brands international is our client and we performed exploratory data analysis on corporate purchase orders, contracts and projects data using sampling and statistical methods. identified strata, improved precision and accuracy.\n",
      "---\n",
      " works with other team members, including dba's, other etl developers, technical architects, qa, and business analysts & project managers\n",
      "---\n",
      " the work will involve the development of workflows triggered by events from other systems.\n",
      "---\n",
      " develop easy to use documentation for the frameworks and tools developed for adaption by other teams.\n",
      "---\n",
      " applied k-means and hierarchical clustering on the data.\n",
      "---\n",
      " identified and analyzed business insights.\n",
      "---\n",
      " developed hive udfs and pig udfs using python in microsoft hdinsight environment\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and customized visualization tools using python, r, hadoop and mongodb.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in python for developing various machine learning algorithms.\n",
      "---\n",
      " performed data profiling to merge the data from multiple data sources\n",
      "---\n",
      " worked on csv, json, excel different types of files for the data cleaning and data analysis.\n",
      "---\n",
      " used python for statistical operations on the data and ggplot2 for the visualizing the data.\n",
      "---\n",
      " participated in feature engineering such as feature intersection generating for adding potential powerful features, plotting feature correlation matrix for feature selection and reducing, feature normalization for ease to implement machine algorithms, principal component analysis (pca) for dimensionality reduction and label encoding with scikit-learn preprocessing.\n",
      "---\n",
      " worked with several use cases like campaign sales analysis, forecasting sales, kpi analysis and nlp models\n",
      "---\n",
      " worked with clustering algorithms to target specific group of customers to generate profitable revenue\n",
      "---\n",
      " worked with word embedding techniques like word2vec, glove for sentiment analysis and text classifications\n",
      "---\n",
      " worked with text to vector representation methods including counter vectorizer, tf-idf and latent dirichlet allocation (lda) for topic modeling\n",
      "---\n",
      " performed time series analysis using tableau\n",
      "---\n",
      " developed and executed ad hoc reporting's according to the business needs.\n",
      "---\n",
      " managed offshore projects and coordinated work for 24 hour productivity cycle. etl developer sutherland global services - hyderabad, telangana february 2013 to december 2013 \n",
      "---\n",
      " sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics.\n",
      "---\n",
      " sutherland has customers across industries like financial services to healthcare.\n",
      "---\n",
      " my role is to assist analytics department for the data extraction and cleaning as a data preprocessing steps to build models.\n",
      "---\n",
      " involved with business analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications\n",
      "---\n",
      " involved in data mapping specifications to create and execute detailed system test plans. the data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n",
      "---\n",
      " managed full sdlc processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " designed ssis packages to extract, transform and load existing data into sql server, used lots of components of ssis, such as pivot transformation, fuzzy lookup, merge, merge join, data conversion, row count, sort, derived columns, conditional split, execute sql task, data flow task and execute package task.\n",
      "---\n",
      " created ssis packages that involved dealing with different source formats (flat files, excel, xml, ole db) and different destination formats\n",
      "---\n",
      " debugged and troubleshot the etl packages by using a breakpoint, analyzing the process, catching error information by sql command in ssis\n",
      "---\n",
      " developed sql queries in sql server management studio, toad and generated complex reports forth end users.\n",
      "---\n",
      " automated and scheduled recurring reporting processes using unix shell scripting and teradata utilities such as mload, bteq, and fast load\n",
      "---\n",
      " experience with perl.\n",
      "---\n",
      " performed data analysis and data profiling using complex sql on various sources systems including oracle and teradata \n",
      "---\n",
      " master of science in information technology management campbellsville university bachelor of technology in electronics and communication engineering jawaharlal nehru technological university - kakinada, andhra pradesh\n",
      "\n",
      "---\n",
      " efficient data scientist with 10 plus years of experience in, statistical modeling, machine learning, data mining with large data sets of structured and unstructured data and performed data acquisition, data validation, predictive modeling and data visualization.\n",
      "---\n",
      " expertise in python (2.x/3.x) programming with multiple packages including numpy, pandas, matplotlib, scipy, seaborn and scikit-learn.\n",
      "---\n",
      " hands on experience in implementing lda, na\\xefve bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " implementation experiences in machine learning and deep learning, including regression, classification, object tracking, natural language processing (nlp) using packages like nltk, spacy.\n",
      "---\n",
      " experience in tuning algorithms using methods such as grid search, randomized search, k-fold cross validation and error analysis.\n",
      "---\n",
      " also worked with several boosting methodologies like ada boost, gradient boosting and xgboost.\n",
      "---\n",
      " validated the machine learning classifiers using accuracy, auc, roc curves and lift charts.\n",
      "---\n",
      " worked with various text analytics or word embedding libraries like word2vec, count vectorizer, glove, lda etc.\n",
      "---\n",
      " solid knowledge and experience in deep learning techniques including feedforward neural network, convolutional neural network (cnn), recursive neural network (rnn).\n",
      "---\n",
      " worked with numerous data visualization tools in python like matplotlib, seaborn, ggplot, pygal.\n",
      "---\n",
      " worked and extracted data from various database sources like oracle, sql server,and mongodb.\n",
      "---\n",
      " highly skilled in using hadoop, hbase, spark, and hive for basic analysis and extraction of data in the infrastructure to provide data summarization.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " experience on working with different operating systems like unix, linux, and windows.\n",
      "---\n",
      " experience working with ms word, ms excel, ms powerpoint, ms sharepoint, and ms project. authorized to work in the us for any employer work experience data scientist/data engineer zoetis inc september 2018 to present zoetis inc. is the world's largest producer of medicine and vaccinations for pets and livestock. zoetis delivers quality medicines, vaccines and diagnostic products, which are complemented by genetic tests, bio devices and a range of services. the project is to collect data from different sources and create a master data set. and we do predictions on sales and profits. measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " developing data analytical databases from different sources and create a master data set.\n",
      "---\n",
      " responsible for data identification, collection, exploration, cleaning for modeling.\n",
      "---\n",
      " we do predictions on sales and profits using machine learning and deep learning strategies.\n",
      "---\n",
      " updated and manipulated content and files by using python scripts. worked on python open stack api's.\n",
      "---\n",
      " performed time series analysis on sales data to consider what measures to be taken for improve the sales.\n",
      "---\n",
      " determined customer satisfaction and helped enhance customer experience using nlp.\n",
      "---\n",
      " manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " analysis of biological and spatial data to develop insights into precision animal management and precision medicine.\n",
      "---\n",
      " implementing analytics algorithms in python.\n",
      "---\n",
      " performed training natural language models and reinforcement learning engines to optimize intelligent agents that automate task execution.\n",
      "---\n",
      " worked with dimensionality reduction techniques like pca, lda and ica.\n",
      "---\n",
      " performed k-means clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering, customized and priority service, to improve existing profitable relationships and to avoid customer churn, etc using python.\n",
      "---\n",
      " performed text analytics on unstructured email data using natural language processing tool kit (nltk).\n",
      "---\n",
      " applying clustering algorithms to group the data on their similar behavior patterns.\n",
      "---\n",
      " work with data analytics team to develop time series and optimization.\n",
      "---\n",
      " performed time series analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values.\n",
      "---\n",
      " experienced in data scraping.\n",
      "---\n",
      " used pyspark machine learning library to build and evaluate different models.\n",
      "---\n",
      " created various proof of concepts (poc) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.\n",
      "---\n",
      " experienced in agile methodology.\n",
      "---\n",
      " used tableau to generate reports with internal records, secondary sources of data, json, csv and more. which helped the support team for better marketing.\n",
      "---\n",
      " environment: python 3.7.0, pyspark, nltk, sql server , microsoft excel, sql, aws, qlikview, sqoop, etl, agile.\n",
      "---\n",
      " mars solutions group, wi\n",
      "---\n",
      " role: python/data scientist aug20016-sep2018\n",
      "---\n",
      " the client is the largest healthcare provider and offers health care products, insurance services, data analytics, payment integrity, and the project was to build predictive models for customer value analysis by applying machine learning methods, principal component analysis, and regression on large data set.\n",
      "---\n",
      " worked on machine learning, data mining with large data sets of structured and unstructured data, data acquisition, data validation, predictive modeling, data visualization.\n",
      "---\n",
      " performed multinomial logistic regression, random forest machine learning algorithms.\n",
      "---\n",
      " used aws to manage the data in the cloud.\n",
      "---\n",
      " good knowledge on hadoop components such as hdfs, job tracker, task tracker, name node, data node, and mapreduce concepts.\n",
      "---\n",
      " maintained updated log files using python.\n",
      "---\n",
      " used machine learning algorithms like logistic regression , knn, decision trees, random forest to make the data to fit for the desired output.\n",
      "---\n",
      " interact and brain-storm with multifunctional teams to explore the opportunities of using data to improve business and health-care outcomes.\n",
      "---\n",
      " developed machine-learning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected business/health-care goals.\n",
      "---\n",
      " design, coding, unit testing of etl package source marts and subject marts using informatica etl processes for oracle database.\n",
      "---\n",
      " worked in agile methodology.\n",
      "---\n",
      " generated reports with internal records, secondary sources of data, json, csv and more.\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data.\n",
      "---\n",
      " provided schedules, status reports, and issue resolutions to the project team, business users, and project managers.\n",
      "---\n",
      " environment: python 3.x, linux, spark, sql server 2012, microsoft excel, , spark sql, aws, qlikview ,sqoop, etl, agile. data analyst / data scientist cms energy - jackson, mi july 2014 to august 2016 cms energy is an energy company that is focused principally on utility operations. i was responsible for building a new data science department with the help of other departments and i was able to learn how the business is operated and helped the company to grow and stay ahead of the competition. by using machine learning we improvised the predictive algorithm for pricing strategy. and we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me.\n",
      "---\n",
      " worked on data manipulation & visualization, machine learning, python, and sql.\n",
      "---\n",
      " transformed the business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " worked on customer segmentation using an unsupervised learning technique - clustering.\n",
      "---\n",
      " implemented classification using supervised learning like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " built models using statistical techniques and machine learning classification models like xg boost, svm, and random forest.\n",
      "---\n",
      " improv ed model's accuracy by using gradient boosting technique like light gbm and gained around 82% accuracy with random forest and 77% with logistic regression.\n",
      "---\n",
      " used jupyter notebook for spark to make data manipulations.\n",
      "---\n",
      " developed etl processes for data conversions and construction of data warehouse using informatica.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements and work with other members of the it organization to deliver data visualization and reporting solutions to address those needs.\n",
      "---\n",
      " environment: tableau 10.05, aws, git, python 3.5.2 , anaconda-navigator , hadoop, nosql, random forest, mongodb, hdfs, , nltk, xml, mapreduce, informatica. data analyst/ python karvy financial services limited june 2012 to july 2014 karvy financial services limited is a company which has been playing a very proactive role in the economic growth by providing loans to micro & small business segments and individuals like credit for the requirements of different sectors of economy. industries, exports, trading, agriculture, infrastructure and the individual segments. we worked on various projects which handles customer analytics, credit risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans, identify and prevent fraud detection for transactions.\n",
      "---\n",
      " compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results.\n",
      "---\n",
      " applied concepts of probability, distribution, and statistical inference on the given dataset to unearth interesting findings using comparison, t-test, f-test, r-squared, p-value etc.\n",
      "---\n",
      " applied linear regression, multiple regression, ordinary least square method, mean-variance, the theory of large numbers, logistic regression, dummy variable, residuals, poisson distribution, naive bayes, fitting function etc to data with help of scikit, scipy, numpy and pandas module of python.\n",
      "---\n",
      " applied principal component analysis (pca) based unsupervised technique to determine unusual vpn log-on time.\n",
      "---\n",
      " performed clustering with historical, demographic and behavioral data as features to implement the personalized marketing to the customers.\n",
      "---\n",
      " also created classification model using logistic regression, random forests to classify dependent variable into two classes which are risky and okay.\n",
      "---\n",
      " used f-score, precision, recall evaluating model performance.\n",
      "---\n",
      " built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as decision trees, random forests and svm.\n",
      "---\n",
      " real time analysis of customer financial profile and providing recommendation for financial products best suited.\n",
      "---\n",
      " performed sentimental analysis (nlp) on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by long-short term memory (lstm) cells in recurrent neural networks(rnn).\n",
      "---\n",
      " forecasted demand for loans and interest rates using time series analysis like arimax, varmax and holt-winters.\n",
      "---\n",
      " obtained better predictive performance of 81% accuracy using ensemble methods like bootstrap aggregation (bagging) and boosting (light gbm, gradient).\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " performed financial data ingestion to the spark distribution environment, using kafka.\n",
      "---\n",
      " developed visualizations and dashboards using ggplot, tableau.\n",
      "---\n",
      " prepared and presented data quality report to stakeholders to give understanding of data.\n",
      "---\n",
      " environment: tableau 10.05, git, python 3.5.2 , anaconda-navigator, hadoop, spark, kafka, nosql, random forest, mongodb, hdfs, , nlp. python developer / data analyst symbiosys technologies may 2011 to june 2012 genius brands international is our client and we performed exploratory data analysis on corporate purchase orders, contracts and projects data using sampling and statistical methods. identified strata, improved precision and accuracy. works with other team members, including dba's, other etl developers, technical architects, qa, and business analysts & project managers.\n",
      "---\n",
      " participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.\n",
      "---\n",
      " used python to place data into json files for testing django websites.\n",
      "---\n",
      " updated and manipulated content and files by using python scripts. worked on python open stack api's.\n",
      "---\n",
      " used python scripts to update content in the database and manipulate files. generated python django forms to record data of online users.\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and customized visualization tools using python, r, hadoop and mongodb.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in python for developing various machine learning algorithms.\n",
      "---\n",
      " worked on csv, json, excel different types of files for the data cleaning and data analysis.\n",
      "---\n",
      " used r for statistical operations on the data and ggplot2 for the visualizing the data.\n",
      "---\n",
      " application of various ml algorithms and statistical modeling like decision trees, regression models, random forest , svm, clustering to identify volume using scikit-learn package in python.\n",
      "---\n",
      " performed classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " performed data profiling to merge the data from multiple data sources.\n",
      "---\n",
      " extracted data from hdfs (hadoop distributed file system) and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " performed time series analysis using tableau.\n",
      "---\n",
      " knowledge of other relational database platforms such as oracle, db2, nosql.\n",
      "---\n",
      " managed offshore projects and coordinated work for 24 hour productivity cycle.\n",
      "---\n",
      " environment: python 2.7, django 1.4, r, mongodb, github, sql server, hdfs, hive. etl developer sutherland global services april 2009 to may 2011 sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics. sutherland has customers across industries like financial services to healthcare. my role is to assist analytics department for the data extraction and cleaning as a data preprocessing steps to build models.\n",
      "---\n",
      " involved with business analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications.\n",
      "---\n",
      " involved in data mapping specifications to create and execute detailed system test plans. the data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n",
      "---\n",
      " managed full sdlc processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model.\n",
      "---\n",
      " developed etls to pull data from various sources and transform it for reporting applications using pl/sql.\n",
      "---\n",
      " designed ssis packages to extract, transform and load existing data into sql server, used lots of components of ssis, such as pivot transformation, fuzzy lookup, merge, merge join, data conversion, row count, sort, derived columns, conditional split, execute sql task, data flow task and execute package task.\n",
      "---\n",
      " created ssis packages that involved dealing with different source formats (flat files, excel, xml) and different destination formats.\n",
      "---\n",
      " debugged and troubleshot the etl packages by using a breakpoint, analyzing the process, catching error information by sql command in ssis\n",
      "---\n",
      " developed sql queries in sql server management studio, toad and generated complex reports forth end users.\n",
      "---\n",
      " automated and scheduled recurring reporting processes using unix shell scripting and teradata utilities such as mload, bteq, and fast load\n",
      "---\n",
      " experience with perl.\n",
      "---\n",
      " performed data analysis and data profiling using complex sql on various sources systems including oracle and teradata.\n",
      "---\n",
      " environment: etl tools, sdlc, github, sql server, pl/sql, excel, xml, sql. \n",
      "---\n",
      " bachelor's\n",
      "\n",
      "---\n",
      " professional qualified data scientist/software engineer with over 9 years of experience in data science and analytics including artificial intelligence/deep learning/machine learning, big data, data mining and statistical analysis and demonstrated leadership.\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, random forest, xgboost, knn, svm, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova\n",
      "---\n",
      " extensively worked on python 3.5/2.7 (numpy, pandas, matplotlib, nltk and scikit-learn) and scala\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, caret, dplyr) and excel [ ]\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008, nosql databases like aws dynamodb, mongodb\n",
      "---\n",
      " developed api libraries and coded business logic using c#, xml and designed web pages using .net framework, c#, python, django, html, ajax\n",
      "---\n",
      " strong experience in big data technologies like spark, sparksql, pyspark, hadoop, hdfs, hive\n",
      "---\n",
      " experience in visualization tools like, tableau 9.x, 10.x for creating dashboards\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " worked on containerization using docker and managed the containers in kubernetes\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " worked on message queues such as amazon sns, sqs, google pub sub and integrated with other services like aws lambda, storage using bash scripts\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " proficient in predictive modeling, data mining methods, factor analysis, anova, hypothetical testing, normal distribution and other advanced statistical and econometric techniques.\n",
      "---\n",
      " developed predictive models using decision tree, random forest, na\\xefve bayes, logistic regression, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, numpy, scipy and pandas for data analysis.\n",
      "---\n",
      " worked with complex applications such as r, sas, matlab and spss to develop neural network, cluster analysis.\n",
      "---\n",
      " strong sql and t sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data ingestion, data manipulation,data architecture, data modelling and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experienced in visual basic for applications and vb programming languages c#, .net framework to work with developing applications.\n",
      "---\n",
      " experienced in big data with hadoop, hdfs, mapreduce, and spark.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql and python and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. authorized to work in the us for any employer work experience data scientist/data science manager american express/home depot july 2017 to present description: american express delivers predictive analytics as a service, and offers hosted, cloud-based systems for specific business problems, e.g., predicting the behavior of individual consumers, stopping revenue leakage in hospitals, warning of threats to corporate security or brand health, etc. home depot is an american home improvement supplies retailing company that sells tools, construction products, and services. home depot is the largest home improvement retailer in the united states, ahead of rival lowe's\n",
      "---\n",
      " demonstrated leadership skills by successfully managing, mentoring and delivering 3 data science projects at srishti biz.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana etc\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " implemented statistical modeling with xgboost machine learning software package using python to determine the predicted probabilities of each model.\n",
      "---\n",
      " created master data for modelling by combining various tables and derived fields from client data and students lors, essays and various performance metrics.\n",
      "---\n",
      " hands on experience on spark mllib using both python and scala.\n",
      "---\n",
      " worked on containerization of different modules of the project like python apis, web framework written in javascript using docker and managed it using kubernetes\n",
      "---\n",
      " formulated a basis for variable selection and gridsearch, kfold for optimal hyperparameters\n",
      "---\n",
      " utilized boosting algorithms to build a model for predictive analysis of student's behaviour who took usmle exam apply for residency.\n",
      "---\n",
      " used numpy, scipy, pandas, nltk(natural language processing toolkit),matplotlib to build the model.\n",
      "---\n",
      " formulated several graphs to show the performance of the students by demographics and their mean score in different usmle exams.\n",
      "---\n",
      " application of various artificial intelligence(ai)/machine learning algorithms and statistical modeling like decision trees,text analytics, natural language processing(nlp), supervised and unsupervised, regression models.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frameworks.\n",
      "---\n",
      " scheduled jobs to run sql statements to transfer data from google bigquery to mysql using airflow.\n",
      "---\n",
      " created deep learning models using tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students.\n",
      "---\n",
      " used xgb classifier if the feature is a categorical variable and xgb regressor for continuous variables and combined it using featureunion and functiontransfomer methods of natural language processing.\n",
      "---\n",
      " used onevsrest classifier to fit each classifier against all other classifiers and used it on multiclass classification problems.\n",
      "---\n",
      " implemented application of various machine learning algorithms and statistical modeling like decision tree, text analytics, sentiment analysis, naive bayes, logistic regression and linear regression using python to determine the accuracy rate of each model.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n",
      "---\n",
      " generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using signal hub.\n",
      "---\n",
      " created data layers as signals to signal hub to predict new unseen data with performance not less than the static model build using deep learning framework.\n",
      "---\n",
      " environment: python 2.x,3.x, hive, aws, linux, tableau desktop, microsoft excel, nlp, deep learning frameworks such as tensorflow, keras, boosting algorithms etc data scientist january 2016 to november 2016 description: the goal of this project is short text summarization on the comment stream of a message from social network services. the users of the social sites always desire to get a brief understanding of a comment stream without reading the whole comment list.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, time, date and time etc.\n",
      "---\n",
      " application of various artificial intelligence(ai)/machine learning algorithms and statistical modeling like decision trees,text analytics, natural language processing(nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab\n",
      "---\n",
      " categorised comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " analyze traffic patterns by calculating autocorrelation with different time lags.\n",
      "---\n",
      " sets of comment streams are processed by nlp module in stages like stemming, punctuation removal etc. to categorize into vector model and input into incremental short text summarization (ists) algorithm. clusters of comments with same pattern will be grouped together and presented visually to the end user.\n",
      "---\n",
      " ensured that the model has low false positive rate and text classification and sentiment analysis for unstructured and semi-structured data.\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using sap predictive analytics\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau data scientist first data/accenture july 2011 to july 2015 description: first data corporation is a global payment processing company headquartered in atlanta, georgia, united states. the company's portfolio includes merchant transaction processing services\n",
      "---\n",
      " credit, debit, private-label, gift, payroll and other prepaid card offerings\n",
      "---\n",
      " fraud protection and authentication solutions.\n",
      "---\n",
      " provided configuration management and build support for more than 5 different applications, built and deployed to the production and lower environments.\n",
      "---\n",
      " explored and extracted data from source xml in hdfs, used etl for preparing data for exploratory analysis using data munging.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata, text mining and building models using topic analysis, sentiment analysis for both semi-structured and unstructured data.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " write ad-hoc queries using t sql for data analysis on microsoft sql server\n",
      "---\n",
      " used r and python for exploratory data analysis, a/b testing, hql, vql, data lake, aws redshift, oozie, pyspark, anova test and hypothesis test to compare and identify the effectiveness of creative campaigns.\n",
      "---\n",
      " computing a/b testing frameworks, clickstream\n",
      "---\n",
      " created clusters to control and test groups and conducted group campaigns using text analytics.\n",
      "---\n",
      " created positive and negative clusters from merchant's transaction using sentiment analysis to test the authenticity of transactions and resolve any chargebacks.\n",
      "---\n",
      " analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n",
      "---\n",
      " created and developed classes and web page elements using c# and ajax. jsp was used for validating client side responses and connected c# to database to retrieve sql data\n",
      "---\n",
      " developed linuxshell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using pl/sql\n",
      "---\n",
      " created various types of data visualizations using r, c#, python and tableau/spotfire also connected pipeline pilot with spotfire to create more interactive business driven layouts.\n",
      "---\n",
      " used python, r, sql to create statistical algorithms involving multivariate regression, linear regression, logistic regression, pca, random forest models, decision trees, support vector machine for estimating the risks of welfare dependency.\n",
      "---\n",
      " identified and targeted welfare high-risk groups with machine learning/deep learning algorithms.\n",
      "---\n",
      " conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity-relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " created multiple custom sql queries in teradata sql workbench to prepare the right data sets for tableau dashboards\n",
      "---\n",
      " perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using sas programming.\n",
      "---\n",
      " environment: r 3.x, hdfs, c#, hadoop 2.3, pig, hive, linux, r-studio, tableau 10, sql server, ms excel, pyspark. python developer cenvien technologies - hyderabad, telangana january 2011 to june 2011 description: cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products. it is highly qualified and strongly dedicated developing team that produces unique solutions.\n",
      "---\n",
      " developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " implemented the presentation layer with html, css and javascript.\n",
      "---\n",
      " involved in writing stored procedures using oracle.\n",
      "---\n",
      " optimized the database queries to improve the performance.\n",
      "---\n",
      " designed and developed data management system using oracle.\n",
      "---\n",
      " environment: mysql, oracle, html5, css3, javascript, shell, linux & windows, django, python data analyst pennar industries limited - hyderabad, telangana march 2009 to december 2010 description: as a backend developer of web applications and data science infrastructure. the main area of focus is to come up with comprehensive solutions that need massive capacity and throughput.\n",
      "---\n",
      " effectively communicated with the stakeholders to gather requirements for different projects\n",
      "---\n",
      " used mysql db package and python-mysql connector for writing and executing several mysql database queries from python.\n",
      "---\n",
      " implemented client/server applications using c#, jsp and sql\n",
      "---\n",
      " created functions, triggers, views and stored procedures using my sql.\n",
      "---\n",
      " worked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " involved in the code review meetings.\n",
      "---\n",
      " environment: python, mysql, c#. \n",
      "---\n",
      " master of computer science in computer science lamar university - beaumont, tx 2015 to 2017 skills business intelligence, sql, access, testing, excel, data science (9 years), python (9 years) links https://www.linkedin.com/in/pavithra-kumar-22b753173 certifications/licenses acadgild - masters in data science february 2019 to present assessments data analysis \\x97 proficient august 2019 measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. full results: https://share.indeedassessments.com/share_assignment/5p0ejlgzwei-m6ro problem solving \\x97 expert august 2019 measures a candidate's ability to analyze relevant information when solving problems. full results: https://share.indeedassessments.com/share_assignment/dso7visv0vjm2cfc indeed assessments provides skills tests that are not indicative of a license or certification, or continued development in any professional field.\n",
      "\n",
      "---\n",
      " experiencedin facilitating the entire life-cycle of a data science project: data extraction, data pre-processing, feature engineering, dimensionality reduction, algorithm implementation, back testing and validation.\n",
      "---\n",
      " proficient in data transformations using log, square-root, reciprocal, differencing and complete box-cox transformation depending upon the dataset.\n",
      "---\n",
      " knowledge of normality tests like shapiro-wilk, anderson-darling.\n",
      "---\n",
      " adept at analysis of missing data by exploring correlations and similarities, introducing dummy variables for missingness, and choosing from imputation methods such iterative imputer on python.\n",
      "---\n",
      " experienced inmachine learning techniques such as regression and classification models like linear, polynomial, support vector, decision trees, logistic regression, support vector machines.\n",
      "---\n",
      " experienced in ensemble learning usingbagging, boosting & random forests\n",
      "---\n",
      " clustering like k-means.\n",
      "---\n",
      " in-depth knowledge of dimensionality reduction (pca, lda), hyper-parameter tuning, model regularization (ridge, lasso, elastic net) and grid search techniques to optimize model performance.\n",
      "---\n",
      " adept with python and oop concepts such as inheritance, polymorphism, abstraction, association, etc.\n",
      "---\n",
      " experienced in developing algorithms to create artificial neural networks, deep learning, convolution neural networks to implement ai solutions.\n",
      "---\n",
      " expertise in creating executive tableau dashboards for data visualization and deploying it to the servers\n",
      "---\n",
      " skilled in using tidyversein r and pandasin python for performing exploratory data analysis.\n",
      "---\n",
      " proficient in data visualization tools such as tableau and powerbi, big data tools such as hadoop hdfs, spark and mapreduce, mysql,oracle sql and redshift sqland microsoft excel (vlookup, pivot tables)\n",
      "---\n",
      " skilled in big data technologies like spark, spark sql, pyspark, hdfs (hadoop), mapreduce &kafka.\n",
      "---\n",
      " experience in web data mining with python's scrapy and beautifulsoup packages along with working knowledge of natural language processing (nlp) to analyze text patterns.\n",
      "---\n",
      " excellent exposure to data visualization with tableau, powerbi, seaborn, matplotlib and ggplot2.\n",
      "---\n",
      " experience with python libraries including numpy, pandas, scipy, scikit-learn& statsmodels, matplotlib, seaborn,nltkand r libraries like ggplot2, dplyr.\n",
      "---\n",
      " working knowledge of database creation and maintenance of physical data models with oracle, db2 and sql server databases as well as normalizing databases up to third form using sql functions. authorized to work in the us for any employer work experience data scientist vf corporation - greensboro, nc august 2017 to present description:\n",
      "---\n",
      " vf corporation is an american worldwide apparel and footwear company. worked on the recommender system by implementing sentiment analysis on other people reviews and extract the best product for the user to buy.\n",
      "---\n",
      " reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives.\n",
      "---\n",
      " extracted data by web-scraping through the reviews using beautiful soup.\n",
      "---\n",
      " involved in various pre-processing phases of text-data like tokenization, stemming, lemmatization and converting the raw text data to structured data.\n",
      "---\n",
      " performed data collection, data cleaning, feature scaling, feature engineering, validation, visualization, report findings, develop strategic uses of data by python libraries like numpy, pandas, scipy, matplotlib, scikit-learn.\n",
      "---\n",
      " used tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data.\n",
      "---\n",
      " implemented various statistical techniques to manipulate the data like missing data imputation, principal component analysis for dimension-reduction.\n",
      "---\n",
      " worked with customer churn models including lasso regression, along with pre-processing of the data.\n",
      "---\n",
      " constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like bag of words model, tf-idf, word2vec.\n",
      "---\n",
      " employed statistical methodologies such as a/b test, experiment design and hypothesis testing and deployed models on docker.\n",
      "---\n",
      " performed na\\xefve bayes, knn, logistic regression, random forest, svm and kmeans to categorize customers into certain groups.\n",
      "---\n",
      " employed various metrics such as cross-validation, logloss function, confusion matrix, roc and auc to evaluate the performance of each model.\n",
      "---\n",
      " using nlp developed deep learning algorithms for analyzing text, over the existing dictionary-based approaches.\n",
      "---\n",
      " created distributed environment of tensorflow across multiple devices and ran them in parallel.\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), tensorflow, nlp data scientist siriusxm - washington, dc april 2015 to july 2017 description:\n",
      "---\n",
      " siriusxm is a broadcasting company that provides satellite radio and online radio. the project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account.\n",
      "---\n",
      " performed data collection, data cleaning, data visualization using python, deep feature synthesis and extracted key statistical findings to develop business strategies.\n",
      "---\n",
      " since sound is represented in the form of audio signals, parameters like frequency, decibel, timbre, pitch were usedfor analysis.\n",
      "---\n",
      " used librosa ( python library) to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound.\n",
      "---\n",
      " cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold.\n",
      "---\n",
      " created 2-d convolution neural networks using keras on gpu's by extracting different number of mfcc features using librosa.\n",
      "---\n",
      " used global-temporal pooling layer to effectively compute statistics of learned features across time.\n",
      "---\n",
      " implemented regularization methods like dropout, lasso regression and ridge regression to prevent the model from overfitting.\n",
      "---\n",
      " final model was selected by evaluating them using various metrics like accuracy, confusion matrix, precision, recall.\n",
      "---\n",
      " environment: python (pandas, scikit, numpy), tensorflow, keras, librosa. data scientist swiggy - gurgaon, haryana january 2013 to march 2015 description:\n",
      "---\n",
      " swiggy is online food delivery company in india. the project was predicting deals and coupons for frequent customers of the company.\n",
      "---\n",
      " participated in all phases of project life cycle including data collection, data mining, data cleaning, developing models, validation and creating reports.\n",
      "---\n",
      " performed data cleaning on a huge dataset which had missing data and extreme outliers from hadoop workbooks and explored data to draw relationships and correlations between variables.\n",
      "---\n",
      " performed data-preprocessing on messy data including imputation, normalization, scaling, and feature engineering using scikit-learn.\n",
      "---\n",
      " conducted exploratory data analysis using python matplotlib and seaborn to identify underlying patterns and correlations between features.\n",
      "---\n",
      " build classification models based on logistic regression, decision trees, support vector machine to predict the probability of a customer using the application.\n",
      "---\n",
      " employed ensemble learning techniques such as random forests and ada gradient boosting to improve the model performance by 10%.\n",
      "---\n",
      " used various metrics such as f-score, roc and auc to evaluate the performance of each model and 5-fold cross validation to test the models with different batches of data to optimize the models.\n",
      "---\n",
      " implemented and tested the model on aws ec2 and collaborated with development team to get the best algorithms and parameters.\n",
      "---\n",
      " prepared data-visualization designed dashboards with tableau, and generated complex reports including summaries and graphs to interpret the findings to the team.\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), amazon web services, jupyter notebook, tableau data analyst - python developer bigbasket - bengaluru, karnataka september 2010 to december 2012 description:\n",
      "---\n",
      " bigbasket is the indian online grocery delivery service. my responsibilities included working on restful web services on python flask and working on a team building a predictive model to enhance the online shopping for the users.\n",
      "---\n",
      " worked on both legacy data and new data mostly built around the user experience and grocery inventory available.\n",
      "---\n",
      " performeddata analysis on target data after transfer to data warehouse.\n",
      "---\n",
      " created etl solution using ms sql server and worked with agile and test-driven development within sdlc.\n",
      "---\n",
      " worked on restful web services on python flask and built primary functions for classification.\n",
      "---\n",
      " conducted data preparation and outlier detection using pythonand implemented logistic regression, random forest, na\\xefve bayes classifier for classification for recommendation.\n",
      "---\n",
      " employed k-fold cross-validation to test and verify the model accuracy.\n",
      "---\n",
      " worked with the team to host data and certain web interfaces on amazon web services ec2 and store data on s3 bucket.\n",
      "---\n",
      " worked with team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run.\n",
      "---\n",
      " presented executive dashboards and scorecards to visualize and present trends in the data using excel and python (matplotlib).\n",
      "---\n",
      " environment: python(numpy, pandas, matplotlib), amazon web services, python flask, rest apis, linux \n",
      "---\n",
      " bachelor's skills amazon web services, hadoop, hdfs, mapreduce, python, ggplot2, matplotlib, anova, mapreduce, kafka, data visualization, hadoop, mongodb, snowflake schema, data modeling, database, microsoft sql server, sql server, mysql, oracle\n",
      "\n",
      "---\n",
      " data scientist with 6 plus years of experience in, statistical modeling, machine learning, data mining with structured and unstructured data.\n",
      "---\n",
      " performed data acquisition, data validation, predictive modeling and data visualization.\n",
      "---\n",
      " expertise in python (2.x/3.x) programming with multiple packages including numpy, pandas, matplotlib, scipy, seaborn and scikit-learn.\n",
      "---\n",
      " hands on experience in implementing lda, na\\xefve bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " implementation experiences in machine learning and deep learning, including regression, classification, natural language processing (nlp) using packages like nltk, spacy.\n",
      "---\n",
      " experience in tuning algorithms using methods such as grid search, randomized search, k-fold cross validation and error analysis.\n",
      "---\n",
      " also worked with several boosting methodologies like ada boost, gradient boosting and xgboost.\n",
      "---\n",
      " validated the machine learning classifiers using accuracy, auc, roc curves and lift charts.\n",
      "---\n",
      " worked with various text analytics or word embedding libraries like word2vec, count vectorizer, glove, lda etc.\n",
      "---\n",
      " solid knowledge and experience in deep learning techniques including feedforward neural network, convolutional neural network (cnn), recursive neural network (rnn).\n",
      "---\n",
      " worked with numerous data visualization tools in python like matplotlib, seaborn, ggplot, pygal.\n",
      "---\n",
      " worked and extracted data from various database sources like oracle, sql server,and mongodb.\n",
      "---\n",
      " highly skilled in using hadoop, hbase, spark, and hive for basic analysis and extraction of data in the infrastructure to provide data summarization.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " experience on working with different operating systems like unix, linux, and windows.\n",
      "---\n",
      " experience working with ms word, ms excel, ms powerpoint, ms sharepoint, and ms project. work experience data scientist zoetis inc september 2018 to present zoetis inc. is the world's largest producer of medicine and vaccinations for pets and livestock. zoetis delivers quality medicines, vaccines and diagnostic products, which are complemented by genetic tests, bio devices and a range of services. the project is to collect data from different sources and create a master data set and doing predictions on sales and profits. measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " developing analytical databases from different sources and create a master data set.\n",
      "---\n",
      " responsible for data identification, collection, exploration, cleaning for modeling.\n",
      "---\n",
      " performed time series analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values.\n",
      "---\n",
      " determined customer satisfaction and helped enhance customer experience using nlp.\n",
      "---\n",
      " manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " analysis of biological and spatial data to develop insights into precision animal management and precision medicine.\n",
      "---\n",
      " implementing analytics algorithms in python.\n",
      "---\n",
      " performed training natural language models and reinforcement learning engines to optimize intelligent agents that automate task execution.\n",
      "---\n",
      " performed k-means clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering, customized and priority service, to improve existing profitable relationships and to avoid customer churn, etc using python.\n",
      "---\n",
      " performed text analytics on unstructured email data using natural language processing tool kit (nltk).\n",
      "---\n",
      " worked with text to vector representation methods including counter vectorizer, tf-idf for topic modelling.\n",
      "---\n",
      " applying clustering algorithms to group the data on their similar behavior patterns.\n",
      "---\n",
      " work with data analytics team to develop time series and optimization.\n",
      "---\n",
      " experienced in data scraping.\n",
      "---\n",
      " used pyspark machine learning library to build and evaluate different models.\n",
      "---\n",
      " created various proof of concepts (poc) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.\n",
      "---\n",
      " experienced in agile methodology.\n",
      "---\n",
      " used tableau to generate reports with internal records, secondary sources of data, json, csv and more. which helped the support team for better marketing.\n",
      "---\n",
      " environment: python 3.7.0, pyspark, nltk, sql server , microsoft excel, sql, aws, tablau, sqoop, etl, agile. data scientist mars solutions group, wi march 2017 to august 2018 the client is the largest healthcare provider and offers health care products, insurance services, data analytics, payment integrity, and the project was to build predictive models for customer value analysis by applying machine learning methods, principal component analysis, and regression on large data set.\n",
      "---\n",
      " worked on machine learning, data mining with large data sets of structured and unstructured data, data acquisition, data validation, predictive modeling, data visualization.\n",
      "---\n",
      " performed multinomial logistic regression, random forest machine learning algorithms.\n",
      "---\n",
      " used aws to manage the data in cloud.\n",
      "---\n",
      " good knowledge on hadoop components such as hdfs, job tracker, task tracker, name node, data node, and mapreduce concepts.\n",
      "---\n",
      " maintained updated log files using python.\n",
      "---\n",
      " alteryx was used for data preparation.\n",
      "---\n",
      " used machine learning algorithms like logistic regression , knn, decision trees, random forest to make the data to fit for the desired output.\n",
      "---\n",
      " interact and brain-storm with multifunctional teams to explore the opportunities of using data to improve business and health-care outcomes.\n",
      "---\n",
      " developed machine-learning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected business/health-care goals.\n",
      "---\n",
      " design, coding, unit testing of etl package source marts and subject marts using informatica etl processes for oracle database.\n",
      "---\n",
      " worked in agile methodology.\n",
      "---\n",
      " generated reports with internal records, secondary sources of data, json, csv and more.\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data.\n",
      "---\n",
      " provided schedules, status reports, and issue resolutions to the project team, business users, and project managers.\n",
      "---\n",
      " environment: python 3.x, linux, spark, sql server 2012, microsoft excel, , spark sql, aws, qlikview ,sqoop, etl, agile. data analyst / data scientist cms energy - jackson, mi january 2016 to february 2017 cms energy is an energy company that is focused principally on utility operations. i was responsible for building a new data science department with the help of other departments and i was able to learn how the business is operated and helped the company to grow and stay ahead of the competition. by using machine learning we improvised the predictive algorithm for pricing strategy. and we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me.\n",
      "---\n",
      " worked on data manipulation & visualization, machine learning, python, and sql.\n",
      "---\n",
      " transformed the business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " worked on customer segmentation using an unsupervised learning technique - clustering.\n",
      "---\n",
      " implemented classification using supervised learning like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " built models using statistical techniques and machine learning classification models like xg boost, svm, and random forest.\n",
      "---\n",
      " improved model's accuracy by using gradient boosting technique like light gbm and gained around 82% accuracy with random forest and 77% with logistic regression.\n",
      "---\n",
      " used jupyter notebook for spark to make data manipulations.\n",
      "---\n",
      " developed etl processes for data conversions and construction of data warehouse using informatica.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements and work with other members of the it organization to deliver data visualization and reporting solutions to address those needs.\n",
      "---\n",
      " environment: tableau 10.05, aws, git, python 3.5.2 , anaconda-navigator , hadoop, nosql, random forest, mongodb, hdfs, , nltk, xml, mapreduce, informatica. data analyst karvy financial services limited november 2014 to december 2015 karvy financial services limited is a company which has been playing a very proactive role in the economic growth of india by providing loans to micro & small business segments and individuals like credit for the requirements of different sectors of economy. industries, exports, trading, agriculture, infrastructure and the individual segments. we worked on various projects which handles customer analytics, credit risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans, identify and prevent fraud detection for transactions.\n",
      "---\n",
      " compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results.\n",
      "---\n",
      " applied concepts of probability, distribution, and statistical inference on the given dataset to unearth interesting findings using comparison, t-test, f-test, r-squared, p-value etc.\n",
      "---\n",
      " applied linear regression, multiple regression, ordinary least square method, mean-variance, the theory of large numbers, logistic regression, dummy variable, residuals, poisson distribution, naive bayes, fitting function etc to data with help of scikit, scipy, numpy and pandas module of python.\n",
      "---\n",
      " applied principal component analysis (pca) based unsupervised technique to determine unusual vpn log-on time.\n",
      "---\n",
      " performed clustering with historical, demographic and behavioral data as features to implement the personalized marketing to the customers.\n",
      "---\n",
      " also created classification model using logistic regression, random forests to classify dependent variable into two classes which are risky and okay.\n",
      "---\n",
      " used f-score, precision, recall evaluating model performance.\n",
      "---\n",
      " built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as decision trees, random forests and svm.\n",
      "---\n",
      " real time analysis of customer financial profile and providing recommendation for financial products best suited.\n",
      "---\n",
      " performed sentimental analysis (nlp) on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by long-short term memory (lstm) cells in recurrent neural networks(rnn).\n",
      "---\n",
      " forecasted demand for loans and interest rates using time series analysis like arimax, varmax and holt-winters.\n",
      "---\n",
      " obtained better predictive performance of 81% accuracy using ensemble methods like bootstrap aggregation (bagging) and boosting (light gbm, gradient).\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " performed financial data ingestion to the spark distribution environment, using kafka.\n",
      "---\n",
      " developed visualizations and dashboards using ggplot, tableau.\n",
      "---\n",
      " prepared and presented data quality report to stakeholders to give understanding of data.\n",
      "---\n",
      " environment: tableau 10.05, git, python 3.5.2 , anaconda-navigator, hadoop, spark, kafka, nosql, random forest, mongodb, hdfs, , nlp. python developer / data analyst symbiosys technologies - visakhapatnam, andhra pradesh january 2014 to october 2014 genius brands international is our client and we performed exploratory data analysis on corporate purchase orders, contracts and projects data using sampling and statistical methods. identified strata, improved precision and accuracy. works with other team members, including dba's, other etl developers, technical architects, qa, and business analysts & project managers.\n",
      "---\n",
      " participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.\n",
      "---\n",
      " used python to place data into json files for testing django websites.\n",
      "---\n",
      " updated and manipulated content and files by using python scripts. worked on python open stack api's.\n",
      "---\n",
      " used python scripts to update content in the database and manipulate files. generated python django forms to record data of online users.\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and customized visualization tools using python, r, hadoop and mongodb.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in python for developing various machine learning algorithms.\n",
      "---\n",
      " worked on csv, json, excel different types of files for the data cleaning and data analysis.\n",
      "---\n",
      " used r for statistical operations on the data and ggplot2 for the visualizing the data.\n",
      "---\n",
      " application of various ml algorithms and statistical modeling like decision trees, regression models, random forest , svm, clustering to identify volume using scikit-learn package in python.\n",
      "---\n",
      " performed classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " performed data profiling to merge the data from multiple data sources.\n",
      "---\n",
      " extracted data from hdfs (hadoop distributed file system) and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " performed time series analysis using tableau.\n",
      "---\n",
      " knowledge of other relational database platforms such as oracle, db2, nosql.\n",
      "---\n",
      " managed offshore projects and coordinated work for 24 hour productivity cycle.\n",
      "---\n",
      " environment: python 2.7, django 1.4, r, oracle, github, sql server, hdfs, hive. etl developer sutherland global services - hyderabad, telangana february 2013 to december 2013 sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics. sutherland has customers across industries like financial services to healthcare. my role is to assist analytics department for the data extraction and cleaning as a data preprocessing steps to build models.\n",
      "---\n",
      " involved with business analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications.\n",
      "---\n",
      " involved in data mapping specifications to create and execute detailed system test plans. the data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n",
      "---\n",
      " managed full sdlc processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model.\n",
      "---\n",
      " developed etls to pull data from various sources and transform it for reporting applications using pl/sql.\n",
      "---\n",
      " designed ssis packages to extract, transform and load existing data into sql server, used lots of components of ssis, such as pivot transformation, fuzzy lookup, merge, merge join, data conversion, row count, sort, derived columns, conditional split, execute sql task, data flow task and execute package task.\n",
      "---\n",
      " created ssis packages that involved dealing with different source formats (flat files, excel, xml) and different destination formats.\n",
      "---\n",
      " debugged and troubleshot the etl packages by using a breakpoint, analyzing the process, catching error information by sql command in ssis\n",
      "---\n",
      " developed sql queries in sql server management studio, toad and generated complex reports forth end users.\n",
      "---\n",
      " automated and scheduled recurring reporting processes using unix shell scripting and teradata utilities such as mload, bteq, and fast load\n",
      "---\n",
      " experience with perl.\n",
      "---\n",
      " performed data analysis and data profiling using complex sql on various sources systems including oracle and teradata.\n",
      "---\n",
      " environment: etl tools, sdlc, github, sql server, pl/sql, excel, xml, sql. \n",
      "---\n",
      " master of science in information technology management in information technology management campbellsville university bachelors in electronics and communication engineering in electronics and communication engineering jawaharlal nehru technological university\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with experience in data science and analytics including data mining , deep learning/machine learning and statistical analysis.\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionand datavisualization with large data sets of structured and unstructured data, created er diagrams and schema..\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda jupiter notebook 4.x, r 3.0 (ggplot2, , dplyr, caret) and excel..\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008/2010/2012, nosql databases like mongodb 3.2\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " developed predictive models using decision tree, naive bayes, logistic regression, random forest, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, scipy, numpy and pandas for data analysis.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with nosqldatabase including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, mapreduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssisand ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql andpython and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. work experience data resarcher/ data scientist usaa - san antonio, tx april 2019 to present description: the united services automobile association (usaa) is a san antonio, texas-based fortune 500 diversified financial services group of companies including a texas department of insurance-regulated reciprocal inter-insurance exchange and subsidiaries offering banking, investing, and insurance to people and families who serve, or served, in the united states armed forces.at the end of 2017, there were 12.4 million members.\n",
      "---\n",
      " test and determine whether new technology is potentially useful for usaa\n",
      "---\n",
      " extracting opearations data from workday and storing it in hadoop\n",
      "---\n",
      " getting the data in pickle file format and parsing it by csv.\n",
      "---\n",
      " build a graphic database using datastax in python (object-oriented programming)\n",
      "---\n",
      " query data/information from graphic database using gremlin to support model validation.\n",
      "---\n",
      " work in a group to develop machine learning guidance for model development and validation\n",
      "---\n",
      " build analytic tools to prepare data and graphs using power query and graphx.\n",
      "---\n",
      " draft the procedures for reporting.\n",
      "---\n",
      " worked on etl tools such as apache airflow\n",
      "---\n",
      " build a auto dag system which would automatically trigger the workflow whenever the airflow will\n",
      "---\n",
      " receive http request.\n",
      "---\n",
      " write python scripts to parse documents.\n",
      "---\n",
      " take online courses as a means to continuously learn new subjects\n",
      "---\n",
      " did intensive research on the tools like graph networks, scheduling tools and data wrangling tools\n",
      "---\n",
      " to determine the best tool available in the market that would be best fit for company's need.\n",
      "---\n",
      " environment: windows, python 3, datastax, graphx, apache airflow, sql. data scientist/ machine learning rauxa - new york, ny august 2018 to march 2019 description: makers of results, rauxa applies data, technology, and content to create measurable impact at maximum speed for clients that include gap inc., tgi fridays, and verizon. the country's largest woman-owned independent advertising agency.\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xgboost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation&visualization, web scraping, machine learning, python programming, sql, git, mongodb, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used tensorflow, a deep learning framework\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olap databases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, nosql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and mapreduce. data scientist charter communication - st. louis, mo may 2017 to july 2018 description: charter communications, inc. is an american telecommunications and mass media company that offers its services to consumers and businesses under the branding of spectrum.\n",
      "---\n",
      " utilized scala, hadoop, pyspark, data lake, tensorflow, mongodb, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " developed sentiment analysis framework for email conversation and customer satisfaction score(csat) correlation.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " sentiment analytics engine was completely built in python3. email chain was considered for analysis. emails were pulled into db from a tool email2db.\n",
      "---\n",
      " each email transactions was pre-processed, sentiment calculation, subjectivity, polarity was taken\n",
      "---\n",
      " and linked to the csat score for dashboarding.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " rapidminer and predictionio was used as tool of choice for predictive analysis.\n",
      "---\n",
      " itsm data was taken and build a engine for predicting cpu failure, memory issue, disk space\n",
      "---\n",
      " issue, server failure errors.\n",
      "---\n",
      " training data was build from either of proactive and reactive ticket data which was in turn to be\n",
      "---\n",
      " used to make classification model for prediction.\n",
      "---\n",
      " identifying and evaluating potential vendors for technology fitments, performing proof of value exercise, conducting commercial and contract negotiations.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, date and time etc.\n",
      "---\n",
      " categorized comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve datafrom oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " exploring dag's, their dependencies and logs using airflow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as tensorflow and pytorch\n",
      "---\n",
      " developed spark/scala,r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data analyst edac technologies corp - cheshire, ct january 2016 to april 2017 description: edac technologies corporation provides design, manufacturing, and services for tooling, fixtures, molds, jet engine components, and machine spindles in the aerospace, industrial, semiconductor, and medical device markets\n",
      "---\n",
      " worked with bi team in gathering the report requirements and also sqoop to export data into hdfs and hive\n",
      "---\n",
      " involved in the below phases of analytics using r, python and jupyter notebook.\n",
      "---\n",
      " data collection and treatment: analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\n",
      "---\n",
      " data mining: used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers, market basket analysis used for customer purchasing behaviour and part/product association.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " assisted with data capacity planning and node forecasting.\n",
      "---\n",
      " installed, configured and managed flume infrastructure\n",
      "---\n",
      " worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n",
      "---\n",
      " worked on performing major upgrade of cluster from cdh3u6 to cdh4.4.0\n",
      "---\n",
      " developed map reduce programs to extract and transform the data sets and results were exported back to rdbms using sqoop.\n",
      "---\n",
      " patterns were observed in fraudulent claims using text mining in r and hive.\n",
      "---\n",
      " exported the data required information to rdbms using sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n",
      "---\n",
      " developed map reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " created tables in hive and loaded the structured (resulted from map reduce jobs) data\n",
      "---\n",
      " created hive queries that helped market analysts spot emerging trends by comparing fresh data with edw reference tables and historical metrics.\n",
      "---\n",
      " enabled speedy reviews and first mover advantages by using oozie to automate data loading into the hadoop distributed file system and pig to pre-process the data.\n",
      "---\n",
      " provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n",
      "---\n",
      " managed and reviewed hadoop log files.\n",
      "---\n",
      " tested raw data and executed performance scripts.\n",
      "---\n",
      " environment: hdfs, pig, hive, map reduce, linux, hbase, flume, sqoop, r, vmware, eclipse, cloudera, python. data analyst dorman products inc - colmar, pa march 2014 to december 2015 description: dorman products, inc. supplies automotive replacement parts, automotive hardware, and brake products to the automotive aftermarket and mass merchandise markets in the united states, canada, mexico, europe, the middle east, and australia.\n",
      "---\n",
      " created and maintained logical and physical models for the data mart. created partitions and indexes for the tables in the data mart.\n",
      "---\n",
      " performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " developed sql scripts for creating tables , sequences , triggers , views and materialized views\n",
      "---\n",
      " worked on query optimization and performance tuning using sql profiler and performance monitoring.\n",
      "---\n",
      " developed mappings to load fact and dimension tables, scd type 1 and scd type 2 dimensions and incremental loading and unit tested the mappings.\n",
      "---\n",
      " utilized erwin's forward / reverse engineering tools and target database schema conversion process.\n",
      "---\n",
      " worked on creating enterprise wide model edm for products and services in teradata environment based on the data from pdm. conceived, designed, developed and implemented this model from the scratch.\n",
      "---\n",
      " building, publishing customized interactive reports and dashboards, report scheduling using tableau server\n",
      "---\n",
      " write sql scripts to test the mappings and developed traceability matrix of business requirements mapped to test scripts to ensure any change control in requirements leads to test case update.\n",
      "---\n",
      " responsible for development and testing of conversion programs for importing data from text files into map oracle database utilizing perl shell scripts &sql\n",
      "---\n",
      "loader.\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " developed and executed load scripts using teradata client utilities multiload, fastload and bteq.\n",
      "---\n",
      " exporting and importing the data between different platforms such as sas, ms-excel.\n",
      "---\n",
      " generated periodic reports based on the statistical analysis of the data using sql server reporting services (ssrs).\n",
      "---\n",
      " worked with the etl team to document the transformation rules for data migration from oltp to warehouse environment for reporting purposes.\n",
      "---\n",
      " created sql scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.\n",
      "---\n",
      " formatting the data sets read into sas by using format statement in the data step as well as proc format.\n",
      "---\n",
      " applied business objects best practices during development with a strong focus on reusability and better performance.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity - relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " designed different type of star schemas for detailed data marts and plan data marts in the olap environment.\n",
      "---\n",
      " environment: erwin, ms sql server 2008, db2, oracle sql developer, pl/sql, business objects, erwin, ms office suite, windows xp, toad, sql\n",
      "---\n",
      "plus, sql\n",
      "---\n",
      "loader, teradata, netezza, sas, tableau, business objects, ssrs, tableau, sql assistant, informatica, xml.. python developer dabur india ltd - ghaziabad, uttar pradesh december 2012 to february 2014 description: dabur is one of the india's largest ayurvedic medicine & natural consumer products manufacturer. dabur demerged its pharma business in 2003 and hived it off into a separate company, dabur pharma ltd. german company fresenius se bought a 73.27% equity stake in dabur pharma in june 2008 at rs 76.50 a share.\n",
      "---\n",
      " involved in the design, development and testing phases of application using agile methodology.\n",
      "---\n",
      " designed and maintained databases using python and developed python based api (restful web service) using flask, sqlalchemy and postgresql.\n",
      "---\n",
      " designed and developed the ui of the website using html, xhtml, ajax, css and javascript.\n",
      "---\n",
      " participated in requirement gathering and worked closely with the architect in designing and modeling.\n",
      "---\n",
      " worked on restful web services which enforced a stateless client server and support json few changes from soap to restful technology involved in detailed analysis based on the requirement documents.\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " responsible for setting up python rest api framework and spring frame work using django\n",
      "---\n",
      " develope consumer based features and applications using python, django, html, behavior driven development (bdd) and pair based programming.\n",
      "---\n",
      " designed and developed components using python with django framework. implemented code in python to retrieve and manipulate data.\n",
      "---\n",
      " involved in development of the enterprise social network application using python, twisted, and cassandra.\n",
      "---\n",
      " used python and django creating graphics, xml processing of documents, data exchange and business logic implementation between servers.\n",
      "---\n",
      " orked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " designed and developed the ui for the website with html, xhtml, css, java script and ajax\n",
      "---\n",
      " used ajax&json communication for accessing restfulweb services data payload.\n",
      "---\n",
      " designed dynamic client-side javascript codes to build web forms and performed simulations for web application page.\n",
      "---\n",
      " created and implemented sql queries, stored procedures, functions, packages and triggers in sql server.\n",
      "---\n",
      " successfully implemented auto complete/auto suggest functionality using jquery, ajax, web service and json.\n",
      "---\n",
      " environment: python 2.5, java/j2ee, django1.0, html,css linux, shell scripting, java script, ajax, jquery, json, xml, postgresql, jenkins, ant, maven, subversion, python data analyst kotak mahindra bank - mumbai, maharashtra january 2011 to november 2012 description: kotak mahindra bank is an indian private sector bank headquartered in mumbai, maharashtra, india. in february 2003, reserve bank of india issued the licence to kotak mahindra finance ltd., the group's flagship company, to carry on banking business.\n",
      "---\n",
      " analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n",
      "---\n",
      " analyzed and designed best fit logical and physical data models and relational database definitions using db2. generated reports of data definitions.\n",
      "---\n",
      " involved in normalization/de-normalization, normal form and database design methodology.\n",
      "---\n",
      " maintained existing etl procedures, fixed bugs and restored software to production environment.\n",
      "---\n",
      " developed the code as per the client's requirements using sql, pl/sql and data warehousing concepts.\n",
      "---\n",
      " involved in dimensional modeling (star schema) of the data warehouse and used erwin to design the business process, dimensions and measured facts.\n",
      "---\n",
      " worked with data warehouse extract and load developers to design mappings for data capture, staging, cleansing, loading, and auditing.\n",
      "---\n",
      " developed enterprise data model management process to manage multiple data models developed by different groups\n",
      "---\n",
      " designed and created data marts as part of a data warehouse.\n",
      "---\n",
      " wrote complex sql queries for validating the data against different kinds of reports generated by business objects xir2.\n",
      "---\n",
      " using erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n",
      "---\n",
      " coordinated with dba in implementing the database changes and also updating data models with changes implemented in development, qa and production. worked extensively with dba and reporting team for improving the report performance with the use of appropriate indexes and partitioning.\n",
      "---\n",
      " developed data mapping, transformation and cleansing rules for the master data management architecture involved oltp, ods and olap.\n",
      "---\n",
      " tuned and coded optimization using different techniques like dynamic sql, dynamic cursors, and tuning sql queries, writing generic procedures, functions and packages.\n",
      "---\n",
      " experienced in gui, relational database management system (rdbms), designing of olap system environment as well as report development.\n",
      "---\n",
      " extensively used sql, t-sql and pl/sql to write stored procedures, functions, packages and triggers.\n",
      "---\n",
      " analyzed of data report were prepared weekly, biweekly, monthly using ms excel, sql & unix\n",
      "---\n",
      " environment: er studio, informatica power center 8.1/9.1, power connect/ power exchange, oracle 11g, mainframes,db2 ms sql server 2008, sql,pl/sql, xml, windows nt 4.0, tableau, workday, spss, sas, business objects, xml, tableau, unix shell scripting, teradata, netezza, aginity \n",
      "---\n",
      " bachelor's\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with experience in data science and analytics including data mining , deep learning/machine learning and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionand datavisualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda jupiter notebook 4.x, r 3.0 (ggplot2, , dplyr, caret) and excel\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008/2010/2012, nosql databases like mongodb 3.2\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " developed predictive models using decision tree, naive bayes, logistic regression, random forest, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, scipy, numpy and pandas for data analysis.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with nosqldatabase including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, mapreduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssisand ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql andpython and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. authorized to work in the us for any employer work experience data resarcher/ data scientist usaa - san antonio, tx april 2019 to present description: the united services automobile association (usaa) is a san antonio, texas-based fortune 500 diversified financial services group of companies including a texas department of insurance-regulated reciprocal inter-insurance exchange and subsidiaries offering banking, investing, and insurance to people and families who serve, or served, in the united states armed forces.at the end of 2017, there were 12.4 million members.\n",
      "---\n",
      " test and determine whether new technology is potentially useful for usaa\n",
      "---\n",
      " extracting opearations data from workday and storing it in hadoop\n",
      "---\n",
      " getting the data in pickle file format and parsing it by csv.\n",
      "---\n",
      " build a graphic database using datastax in python (object-oriented programming)\n",
      "---\n",
      " query data/information from graphic database using gremlin to support model validation.\n",
      "---\n",
      " work in a group to develop machine learning guidance for model development and validation\n",
      "---\n",
      " build analytic tools to prepare data and graphs using power query and graphx.\n",
      "---\n",
      " draft the procedures for reporting.\n",
      "---\n",
      " worked on etl tools such as apache airflow\n",
      "---\n",
      " build a auto dag system which would automatically trigger the workflow whenever the airflow will\n",
      "---\n",
      " receive http request.\n",
      "---\n",
      " write python scripts to parse documents.\n",
      "---\n",
      " take online courses as a means to continuously learn new subjects\n",
      "---\n",
      " did intensive research on the tools like graph networks, scheduling tools and data wrangling tools\n",
      "---\n",
      " to determine the best tool available in the market that would be best fit for company's need.\n",
      "---\n",
      " environment: windows, python 3, datastax, graphx, apache airflow, sql. data scientist/ machine learning rauxa - new york, ny august 2018 to march 2019 description: makers of results, rauxa applies data, technology, and content to create measurable impact at maximum speed for clients that include gap inc., tgi fridays, and verizon. the country's largest woman-owned independent advertising agency.\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xgboost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation&visualization, web scraping, machine learning, python programming, sql, git, mongodb, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used tensorflow, a deep learning framework\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olap databases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, nosql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and mapreduce. data scientist charter communication - st. louis, mo may 2017 to july 2018 description: charter communications, inc. is an american telecommunications and mass media company that offers its services to consumers and businesses under the branding of spectrum.\n",
      "---\n",
      " utilized scala, hadoop, pyspark, data lake, tensorflow, mongodb, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " developed sentiment analysis framework for email conversation and customer satisfaction score(csat) correlation.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " sentiment analytics engine was completely built in python3. email chain was considered for analysis. emails were pulled into db from a tool email2db.\n",
      "---\n",
      " each email transactions was pre-processed, sentiment calculation, subjectivity, polarity was taken\n",
      "---\n",
      " and linked to the csat score for dashboarding.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " rapidminer and predictionio was used as tool of choice for predictive analysis.\n",
      "---\n",
      " itsm data was taken and build a engine for predicting cpu failure, memory issue, disk space\n",
      "---\n",
      " issue, server failure errors.\n",
      "---\n",
      " training data was build from either of proactive and reactive ticket data which was in turn to be\n",
      "---\n",
      " used to make classification model for prediction.\n",
      "---\n",
      " identifying and evaluating potential vendors for technology fitments, performing proof of value exercise, conducting commercial and contract negotiations.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, date and time etc.\n",
      "---\n",
      " categorized comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve datafrom oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " exploring dag's, their dependencies and logs using airflow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as tensorflow and pytorch\n",
      "---\n",
      " developed spark/scala,r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data analyst edac technologies corp - cheshire, ct january 2016 to april 2017 description: edac technologies corporation provides design, manufacturing, and services for tooling, fixtures, molds, jet engine components, and machine spindles in the aerospace, industrial, semiconductor, and medical device markets\n",
      "---\n",
      " worked with bi team in gathering the report requirements and also sqoop to export data into hdfs and hive\n",
      "---\n",
      " involved in the below phases of analytics using r, python and jupyter notebook.\n",
      "---\n",
      " data collection and treatment: analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\n",
      "---\n",
      " data mining: used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers, market basket analysis used for customer purchasing behaviour and part/product association.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " assisted with data capacity planning and node forecasting.\n",
      "---\n",
      " installed, configured and managed flume infrastructure\n",
      "---\n",
      " worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n",
      "---\n",
      " worked on performing major upgrade of cluster from cdh3u6 to cdh4.4.0\n",
      "---\n",
      " developed map reduce programs to extract and transform the data sets and results were exported back to rdbms using sqoop.\n",
      "---\n",
      " patterns were observed in fraudulent claims using text mining in r and hive.\n",
      "---\n",
      " exported the data required information to rdbms using sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n",
      "---\n",
      " developed map reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " created tables in hive and loaded the structured (resulted from map reduce jobs) data\n",
      "---\n",
      " created hive queries that helped market analysts spot emerging trends by comparing fresh data with edw reference tables and historical metrics.\n",
      "---\n",
      " enabled speedy reviews and first mover advantages by using oozie to automate data loading into the hadoop distributed file system and pig to pre-process the data.\n",
      "---\n",
      " provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n",
      "---\n",
      " managed and reviewed hadoop log files.\n",
      "---\n",
      " tested raw data and executed performance scripts.\n",
      "---\n",
      " environment: hdfs, pig, hive, map reduce, linux, hbase, flume, sqoop, r, vmware, eclipse, cloudera, python. data analyst dorman products inc - colmar, pa march 2014 to december 2015 description: dorman products, inc. supplies automotive replacement parts, automotive hardware, and brake products to the automotive aftermarket and mass merchandise markets in the united states, canada, mexico, europe, the middle east, and australia.\n",
      "---\n",
      " created and maintained logical and physical models for the data mart. created partitions and indexes for the tables in the data mart.\n",
      "---\n",
      " performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " developed sql scripts for creating tables , sequences , triggers , views and materialized views\n",
      "---\n",
      " worked on query optimization and performance tuning using sql profiler and performance monitoring.\n",
      "---\n",
      " developed mappings to load fact and dimension tables, scd type 1 and scd type 2 dimensions and incremental loading and unit tested the mappings.\n",
      "---\n",
      " utilized erwin's forward / reverse engineering tools and target database schema conversion process.\n",
      "---\n",
      " worked on creating enterprise wide model edm for products and services in teradata environment based on the data from pdm. conceived, designed, developed and implemented this model from the scratch.\n",
      "---\n",
      " building, publishing customized interactive reports and dashboards, report scheduling using tableau server\n",
      "---\n",
      " write sql scripts to test the mappings and developed traceability matrix of business requirements mapped to test scripts to ensure any change control in requirements leads to test case update.\n",
      "---\n",
      " responsible for development and testing of conversion programs for importing data from text files into map oracle database utilizing perl shell scripts &sql\n",
      "---\n",
      "loader.\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " developed and executed load scripts using teradata client utilities multiload, fastload and bteq.\n",
      "---\n",
      " exporting and importing the data between different platforms such as sas, ms-excel.\n",
      "---\n",
      " generated periodic reports based on the statistical analysis of the data using sql server reporting services (ssrs).\n",
      "---\n",
      " worked with the etl team to document the transformation rules for data migration from oltp to warehouse environment for reporting purposes.\n",
      "---\n",
      " created sql scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.\n",
      "---\n",
      " formatting the data sets read into sas by using format statement in the data step as well as proc format.\n",
      "---\n",
      " applied business objects best practices during development with a strong focus on reusability and better performance.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity - relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " designed different type of star schemas for detailed data marts and plan data marts in the olap environment.\n",
      "---\n",
      " environment: erwin, ms sql server 2008, db2, oracle sql developer, pl/sql, business objects, erwin, ms office suite, windows xp, toad, sql\n",
      "---\n",
      "plus, sql\n",
      "---\n",
      "loader, teradata, netezza, sas, tableau, business objects, ssrs, tableau, sql assistant, informatica, xml.. python developer dabur india ltd - ghaziabad, uttar pradesh december 2012 to february 2014 description: dabur is one of the india's largest ayurvedic medicine & natural consumer products manufacturer. dabur demerged its pharma business in 2003 and hived it off into a separate company, dabur pharma ltd. german company fresenius se bought a 73.27% equity stake in dabur pharma in june 2008 at rs 76.50 a share.\n",
      "---\n",
      " involved in the design, development and testing phases of application using agile methodology.\n",
      "---\n",
      " designed and maintained databases using python and developed python based api (restful web service) using flask, sqlalchemy and postgresql.\n",
      "---\n",
      " designed and developed the ui of the website using html, xhtml, ajax, css and javascript.\n",
      "---\n",
      " participated in requirement gathering and worked closely with the architect in designing and modeling.\n",
      "---\n",
      " worked on restful web services which enforced a stateless client server and support json few changes from soap to restful technology involved in detailed analysis based on the requirement documents.\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " responsible for setting up python rest api framework and spring frame work using django\n",
      "---\n",
      " develope consumer based features and applications using python, django, html, behavior driven development (bdd) and pair based programming.\n",
      "---\n",
      " designed and developed components using python with django framework. implemented code in python to retrieve and manipulate data.\n",
      "---\n",
      " involved in development of the enterprise social network application using python, twisted, and cassandra.\n",
      "---\n",
      " used python and django creating graphics, xml processing of documents, data exchange and business logic implementation between servers.\n",
      "---\n",
      " orked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " designed and developed the ui for the website with html, xhtml, css, java script and ajax\n",
      "---\n",
      " used ajax&json communication for accessing restfulweb services data payload.\n",
      "---\n",
      " designed dynamic client-side javascript codes to build web forms and performed simulations for web application page.\n",
      "---\n",
      " created and implemented sql queries, stored procedures, functions, packages and triggers in sql server.\n",
      "---\n",
      " successfully implemented auto complete/auto suggest functionality using jquery, ajax, web service and json.\n",
      "---\n",
      " environment: python 2.5, java/j2ee, django1.0, html,css linux, shell scripting, java script, ajax, jquery, json, xml, postgresql, jenkins, ant, maven, subversion, python data analyst kotak mahindra bank - mumbai, maharashtra january 2011 to november 2012 description: kotak mahindra bank is an indian private sector bank headquartered in mumbai, maharashtra, india. in february 2003, reserve bank of india issued the licence to kotak mahindra finance ltd., the group's flagship company, to carry on banking business.\n",
      "---\n",
      " analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n",
      "---\n",
      " analyzed and designed best fit logical and physical data models and relational database definitions using db2. generated reports of data definitions.\n",
      "---\n",
      " involved in normalization/de-normalization, normal form and database design methodology.\n",
      "---\n",
      " maintained existing etl procedures, fixed bugs and restored software to production environment.\n",
      "---\n",
      " developed the code as per the client's requirements using sql, pl/sql and data warehousing concepts.\n",
      "---\n",
      " involved in dimensional modeling (star schema) of the data warehouse and used erwin to design the business process, dimensions and measured facts.\n",
      "---\n",
      " worked with data warehouse extract and load developers to design mappings for data capture, staging, cleansing, loading, and auditing.\n",
      "---\n",
      " developed enterprise data model management process to manage multiple data models developed by different groups\n",
      "---\n",
      " designed and created data marts as part of a data warehouse.\n",
      "---\n",
      " wrote complex sql queries for validating the data against different kinds of reports generated by business objects xir2.\n",
      "---\n",
      " using erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n",
      "---\n",
      " coordinated with dba in implementing the database changes and also updating data models with changes implemented in development, qa and production. worked extensively with dba and reporting team for improving the report performance with the use of appropriate indexes and partitioning.\n",
      "---\n",
      " developed data mapping, transformation and cleansing rules for the master data management architecture involved oltp, ods and olap.\n",
      "---\n",
      " tuned and coded optimization using different techniques like dynamic sql, dynamic cursors, and tuning sql queries, writing generic procedures, functions and packages.\n",
      "---\n",
      " experienced in gui, relational database management system (rdbms), designing of olap system environment as well as report development.\n",
      "---\n",
      " extensively used sql, t-sql and pl/sql to write stored procedures, functions, packages and triggers.\n",
      "---\n",
      " analyzed of data report were prepared weekly, biweekly, monthly using ms excel, sql & unix\n",
      "---\n",
      " environment: er studio, informatica power center 8.1/9.1, power connect/ power exchange, oracle 11g, mainframes,db2 ms sql server 2008, sql,pl/sql, xml, windows nt 4.0, tableau, workday, spss, sas, business objects, xml, tableau, unix shell scripting, teradata, netezza, aginity \n",
      "---\n",
      " bachelor of engineering in databases oriental institute of science and technology 2010 to 2012\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with experience in data science and analytics including data mining , deep learning/machine learning and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionand datavisualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda jupiter notebook 4.x, r 3.0 (ggplot2, , dplyr, caret) and excel\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008/2010/2012, nosql databases like mongodb 3.2\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " developed predictive models using decision tree, naive bayes, logistic regression, random forest, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, scipy, numpy and pandas for data analysis.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with nosqldatabase including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, mapreduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssisand ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql andpython and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. work experience data resarcher/ data scientist usaa - san antonio, tx april 2019 to present description: the united services automobile association (usaa) is a san antonio, texas-based fortune 500 diversified financial services group of companies including a texas department of insurance-regulated reciprocal inter-insurance exchange and subsidiaries offering banking, investing, and insurance to people and families who serve, or served, in the united states armed forces.at the end of 2017, there were 12.4 million members.\n",
      "---\n",
      " test and determine whether new technology is potentially useful for usaa\n",
      "---\n",
      " extracting opearations data from workday and storing it in hadoop\n",
      "---\n",
      " getting the data in pickle file format and parsing it by csv.\n",
      "---\n",
      " build a graphic database using datastax in python (object-oriented programming)\n",
      "---\n",
      " query data/information from graphic database using gremlin to support model validation.\n",
      "---\n",
      " work in a group to develop machine learning guidance for model development and validation\n",
      "---\n",
      " build analytic tools to prepare data and graphs using power query and graphx.\n",
      "---\n",
      " draft the procedures for reporting.\n",
      "---\n",
      " worked on etl tools such as apache airflow\n",
      "---\n",
      " build a auto dag system which would automatically trigger the workflow whenever the airflow will\n",
      "---\n",
      " receive http request.\n",
      "---\n",
      " write python scripts to parse documents.\n",
      "---\n",
      " take online courses as a means to continuously learn new subjects\n",
      "---\n",
      " did intensive research on the tools like graph networks, scheduling tools and data wrangling tools\n",
      "---\n",
      " to determine the best tool available in the market that would be best fit for company's need.\n",
      "---\n",
      " environment: windows, python 3, datastax, graphx, apache airflow, sql. data scientist/ machine learning rauxa - new york, ny august 2018 to march 2019 description: makers of results, rauxa applies data, technology, and content to create measurable impact at maximum speed for clients that include gap inc., tgi fridays, and verizon. the country's largest woman-owned independent advertising agency.\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xgboost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation&visualization, web scraping, machine learning, python programming, sql, git, mongodb, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used tensorflow, a deep learning framework\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olap databases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, nosql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and mapreduce. data scientist charter communication - st. louis, mo may 2017 to july 2018 description: charter communications, inc. is an american telecommunications and mass media company that offers its services to consumers and businesses under the branding of spectrum.\n",
      "---\n",
      " utilized scala, hadoop, pyspark, data lake, tensorflow, mongodb, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " developed sentiment analysis framework for email conversation and customer satisfaction score(csat) correlation.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " sentiment analytics engine was completely built in python3. email chain was considered for analysis. emails were pulled into db from a tool email2db.\n",
      "---\n",
      " each email transactions was pre-processed, sentiment calculation, subjectivity, polarity was taken\n",
      "---\n",
      " and linked to the csat score for dashboarding.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " rapidminer and predictionio was used as tool of choice for predictive analysis.\n",
      "---\n",
      " itsm data was taken and build a engine for predicting cpu failure, memory issue, disk space\n",
      "---\n",
      " issue, server failure errors.\n",
      "---\n",
      " training data was build from either of proactive and reactive ticket data which was in turn to be\n",
      "---\n",
      " used to make classification model for prediction.\n",
      "---\n",
      " identifying and evaluating potential vendors for technology fitments, performing proof of value exercise, conducting commercial and contract negotiations.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, date and time etc.\n",
      "---\n",
      " categorized comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve datafrom oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " exploring dag's, their dependencies and logs using airflow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as tensorflow and pytorch\n",
      "---\n",
      " developed spark/scala,r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data analyst edac technologies corp - cheshire, ct january 2016 to april 2017 description: edac technologies corporation provides design, manufacturing, and services for tooling, fixtures, molds, jet engine components, and machine spindles in the aerospace, industrial, semiconductor, and medical device markets\n",
      "---\n",
      " worked with bi team in gathering the report requirements and also sqoop to export data into hdfs and hive\n",
      "---\n",
      " involved in the below phases of analytics using r, python and jupyter notebook.\n",
      "---\n",
      " data collection and treatment: analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\n",
      "---\n",
      " data mining: used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers, market basket analysis used for customer purchasing behaviour and part/product association.\n",
      "---\n",
      " emite was used as a tool of choice for visualization.\n",
      "---\n",
      " assisted with data capacity planning and node forecasting.\n",
      "---\n",
      " installed, configured and managed flume infrastructure\n",
      "---\n",
      " worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n",
      "---\n",
      " worked on performing major upgrade of cluster from cdh3u6 to cdh4.4.0\n",
      "---\n",
      " developed map reduce programs to extract and transform the data sets and results were exported back to rdbms using sqoop.\n",
      "---\n",
      " patterns were observed in fraudulent claims using text mining in r and hive.\n",
      "---\n",
      " exported the data required information to rdbms using sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n",
      "---\n",
      " developed map reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " created tables in hive and loaded the structured (resulted from map reduce jobs) data\n",
      "---\n",
      " created hive queries that helped market analysts spot emerging trends by comparing fresh data with edw reference tables and historical metrics.\n",
      "---\n",
      " enabled speedy reviews and first mover advantages by using oozie to automate data loading into the hadoop distributed file system and pig to pre-process the data.\n",
      "---\n",
      " provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n",
      "---\n",
      " managed and reviewed hadoop log files.\n",
      "---\n",
      " tested raw data and executed performance scripts.\n",
      "---\n",
      " environment: hdfs, pig, hive, map reduce, linux, hbase, flume, sqoop, r, vmware, eclipse, cloudera, python. data analyst dorman products inc - colmar, pa march 2014 to december 2015 description: dorman products, inc. supplies automotive replacement parts, automotive hardware, and brake products to the automotive aftermarket and mass merchandise markets in the united states, canada, mexico, europe, the middle east, and australia.\n",
      "---\n",
      " created and maintained logical and physical models for the data mart. created partitions and indexes for the tables in the data mart.\n",
      "---\n",
      " performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " developed sql scripts for creating tables , sequences , triggers , views and materialized views\n",
      "---\n",
      " worked on query optimization and performance tuning using sql profiler and performance monitoring.\n",
      "---\n",
      " developed mappings to load fact and dimension tables, scd type 1 and scd type 2 dimensions and incremental loading and unit tested the mappings.\n",
      "---\n",
      " utilized erwin's forward / reverse engineering tools and target database schema conversion process.\n",
      "---\n",
      " worked on creating enterprise wide model edm for products and services in teradata environment based on the data from pdm. conceived, designed, developed and implemented this model from the scratch.\n",
      "---\n",
      " building, publishing customized interactive reports and dashboards, report scheduling using tableau server\n",
      "---\n",
      " write sql scripts to test the mappings and developed traceability matrix of business requirements mapped to test scripts to ensure any change control in requirements leads to test case update.\n",
      "---\n",
      " responsible for development and testing of conversion programs for importing data from text files into map oracle database utilizing perl shell scripts &sql\n",
      "---\n",
      "loader.\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " developed and executed load scripts using teradata client utilities multiload, fastload and bteq.\n",
      "---\n",
      " exporting and importing the data between different platforms such as sas, ms-excel.\n",
      "---\n",
      " generated periodic reports based on the statistical analysis of the data using sql server reporting services (ssrs).\n",
      "---\n",
      " worked with the etl team to document the transformation rules for data migration from oltp to warehouse environment for reporting purposes.\n",
      "---\n",
      " created sql scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.\n",
      "---\n",
      " formatting the data sets read into sas by using format statement in the data step as well as proc format.\n",
      "---\n",
      " applied business objects best practices during development with a strong focus on reusability and better performance.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity - relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " designed different type of star schemas for detailed data marts and plan data marts in the olap environment.\n",
      "---\n",
      " environment: erwin, ms sql server 2008, db2, oracle sql developer, pl/sql, business objects, erwin, ms office suite, windows xp, toad, sql\n",
      "---\n",
      "plus, sql\n",
      "---\n",
      "loader, teradata, netezza, sas, tableau, business objects, ssrs, tableau, sql assistant, informatica, xml.. python developer dabur india ltd - ghaziabad, uttar pradesh december 2012 to february 2014 description: dabur is one of the india's largest ayurvedic medicine & natural consumer products manufacturer. dabur demerged its pharma business in 2003 and hived it off into a separate company, dabur pharma ltd. german company fresenius se bought a 73.27% equity stake in dabur pharma in june 2008 at rs 76.50 a share.\n",
      "---\n",
      " involved in the design, development and testing phases of application using agile methodology.\n",
      "---\n",
      " designed and maintained databases using python and developed python based api (restful web service) using flask, sqlalchemy and postgresql.\n",
      "---\n",
      " designed and developed the ui of the website using html, xhtml, ajax, css and javascript.\n",
      "---\n",
      " participated in requirement gathering and worked closely with the architect in designing and modeling.\n",
      "---\n",
      " worked on restful web services which enforced a stateless client server and support json few changes from soap to restful technology involved in detailed analysis based on the requirement documents.\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " responsible for setting up python rest api framework and spring frame work using django\n",
      "---\n",
      " develope consumer based features and applications using python, django, html, behavior driven development (bdd) and pair based programming.\n",
      "---\n",
      " designed and developed components using python with django framework. implemented code in python to retrieve and manipulate data.\n",
      "---\n",
      " involved in development of the enterprise social network application using python, twisted, and cassandra.\n",
      "---\n",
      " used python and django creating graphics, xml processing of documents, data exchange and business logic implementation between servers.\n",
      "---\n",
      " orked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " designed and developed the ui for the website with html, xhtml, css, java script and ajax\n",
      "---\n",
      " used ajax&json communication for accessing restfulweb services data payload.\n",
      "---\n",
      " designed dynamic client-side javascript codes to build web forms and performed simulations for web application page.\n",
      "---\n",
      " created and implemented sql queries, stored procedures, functions, packages and triggers in sql server.\n",
      "---\n",
      " successfully implemented auto complete/auto suggest functionality using jquery, ajax, web service and json.\n",
      "---\n",
      " environment: python 2.5, java/j2ee, django1.0, html,css linux, shell scripting, java script, ajax, jquery, json, xml, postgresql, jenkins, ant, maven, subversion, python data analyst kotak mahindra bank - mumbai, maharashtra january 2011 to november 2012 description: kotak mahindra bank is an indian private sector bank headquartered in mumbai, maharashtra, india. in february 2003, reserve bank of india issued the licence to kotak mahindra finance ltd., the group's flagship company, to carry on banking business.\n",
      "---\n",
      " analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n",
      "---\n",
      " analyzed and designed best fit logical and physical data models and relational database definitions using db2. generated reports of data definitions.\n",
      "---\n",
      " involved in normalization/de-normalization, normal form and database design methodology.\n",
      "---\n",
      " maintained existing etl procedures, fixed bugs and restored software to production environment.\n",
      "---\n",
      " developed the code as per the client's requirements using sql, pl/sql and data warehousing concepts.\n",
      "---\n",
      " involved in dimensional modeling (star schema) of the data warehouse and used erwin to design the business process, dimensions and measured facts.\n",
      "---\n",
      " worked with data warehouse extract and load developers to design mappings for data capture, staging, cleansing, loading, and auditing.\n",
      "---\n",
      " developed enterprise data model management process to manage multiple data models developed by different groups\n",
      "---\n",
      " designed and created data marts as part of a data warehouse.\n",
      "---\n",
      " wrote complex sql queries for validating the data against different kinds of reports generated by business objects xir2.\n",
      "---\n",
      " using erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n",
      "---\n",
      " coordinated with dba in implementing the database changes and also updating data models with changes implemented in development, qa and production. worked extensively with dba and reporting team for improving the report performance with the use of appropriate indexes and partitioning.\n",
      "---\n",
      " developed data mapping, transformation and cleansing rules for the master data management architecture involved oltp, ods and olap.\n",
      "---\n",
      " tuned and coded optimization using different techniques like dynamic sql, dynamic cursors, and tuning sql queries, writing generic procedures, functions and packages.\n",
      "---\n",
      " experienced in gui, relational database management system (rdbms), designing of olap system environment as well as report development.\n",
      "---\n",
      " extensively used sql, t-sql and pl/sql to write stored procedures, functions, packages and triggers.\n",
      "---\n",
      " analyzed of data report were prepared weekly, biweekly, monthly using ms excel, sql & unix\n",
      "---\n",
      " environment: er studio, informatica power center 8.1/9.1, power connect/ power exchange, oracle 11g, mainframes,db2 ms sql server 2008, sql,pl/sql, xml, windows nt 4.0, tableau, workday, spss, sas, business objects, xml, tableau, unix shell scripting, teradata, netezza, aginity \n",
      "---\n",
      " bachelor's skills sql\n",
      "\n",
      "---\n",
      " ? extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python.\n",
      "---\n",
      " ? data driven and highly analytical with working knowledge and statistical model approaches and methodologies (clustering, regression analysis, hypothesis testing, decision trees, machine learning), rules and ever-evolving regulatory environment.\n",
      "---\n",
      " ? professional working experience in machine learning algorithms such as linear regression, logistic regression, naive bayes, decision trees, k-means clustering and association rules.\n",
      "---\n",
      " ? expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data.\n",
      "---\n",
      " ? experience with data visualization using tools like ggplot, matplotlib, seaborn, tableau and using tableau software to publish and presenting dashboards, storyline on web and desktop platforms.\n",
      "---\n",
      " ? experienced in python data manipulation for loading and extraction as well as with python libraries such as numpy, scipy and pandas for data analysis and numerical computations.\n",
      "---\n",
      " ? well experienced in normalization, de-normalization and standardization techniques for optimal performance in relational and dimensional database environments.\n",
      "---\n",
      " ? experience in multiple software tools and languages to provide data-driven analytical solutions to decision makers or research teams.\n",
      "---\n",
      " ? familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks, and ensemble methods like bagging, boosting and random forest to improve the efficiency of the predictive model.\n",
      "---\n",
      " ? worked on text mining and sentimental analysis for extracting the unstructured data from various social media platforms like facebook, twitter, and reddit.\n",
      "---\n",
      " ? develop, maintain and teach new tools and methodologies related to data science and high-performance computing.\n",
      "---\n",
      " ? extensive hands-on experience and high proficiency with structures, semi-structured and unstructured data, using a broad range of data science programming languages and big data tools including r, python, spark, sql, scikit learn, hadoop mapreduce\n",
      "---\n",
      " ? strong experience in software development life cycle (sdlc) including requirements analysis, design specification and testing as per cycle in both waterfall and agile methodologies.\n",
      "---\n",
      " ? adept in statistical programming languages like r and python including big data technologies like hadoop, hive.\n",
      "---\n",
      " ? hands on experience with rstudio for doing data pre-processing and building machine learning algorithms on different datasets.\n",
      "---\n",
      " ? collaborated with the lead data architect to model the data warehouse in accordance with fsldm subject areas, 3nf format, snowflake schema.\n",
      "---\n",
      " ? worked and extracted data from various database sources like oracle, sql server, and db2.\n",
      "---\n",
      " ? implemented machine learning algorithms on large datasets to understand hidden patterns and capture insights.\n",
      "---\n",
      " ? predictive modelling algorithms: logistic regression, linear regression, decision trees, k-nearest neighbours, bootstrap aggregation (bagging), naive bayes classifier, random forests, boosting, support vector machines.\n",
      "---\n",
      " ? flexible with unix/linux and windows environments, working with operating systems like centos5/6, ubuntu13/14, cosmos. authorized to work in the us for any employer work experience data scientist nielsen - new york, ny august 2018 to present nielsen company is the leading provider of entertainment metadata and media recognition technology that powers discovery features and discover the music, tv shows, movies and sports they love across the world's most popular entertainment platforms and devices, from amazon, apple, facebook, google, time warner cable, tesla and others. our project deliver mission is using critical data to help our clients (cnn, ) grow their business with our extensive and quality data verified by real consumers\n",
      "---\n",
      " ? plays a key role in data adoption projects from beginning to end\n",
      "---\n",
      " including developing plan, running analyses, summarizing results, and communicating with client\n",
      "---\n",
      " ? consulting with clients to explain methodology, data impacts, and insights along with proper use guidelines and limitations related to new and enhanced services\n",
      "---\n",
      " ? used pyspark, sparksql in making pipelines to extract data coming from mdl in spark environment.\n",
      "---\n",
      " ? used postgre sql and also worked with hadoop.\n",
      "---\n",
      " ? develop nlp(natural learning program) and other data mining algorithms to extract useful information from large data sets\n",
      "---\n",
      " ? build data pipelines and machine learning(ml) models that run in production in collaboration with software engineers using the tools like numpy, scikit\n",
      "---\n",
      " ? used machine learning concepts- common families of models, feature engineering & selection, cross-validation and parameter tuning\n",
      "---\n",
      " ? worked with devops tools and ci/cd workflows including github, jenkins, and docker swarm\n",
      "---\n",
      " ? used statistical techniques, market research methodologies, research processes, operations to analyze the complexity of consumer businesses, complex analytical challenges and client needs to enable better decisions using the data\n",
      "---\n",
      " ? worked on aws platform for implementing cloud based solutions.\n",
      "---\n",
      " ? helping to solve client challenges such as performance management, product or methodology evolution poc using the advanced techniques and tools common to the data science world like mllib and scikit-learn\n",
      "---\n",
      " ? categorized the clients data and their needs using the linear regression, time series regression, k mean, neural networks, decision trees, classification.,.\n",
      "---\n",
      " ? uses statistical methodologies to analyze the data using python ,r, sas, spss, matlab and to improve the survey quality in production environment\n",
      "---\n",
      " ? executed the company's data mining and modeling activities in support of our clients' online targeting and digital media marketing goals\n",
      "---\n",
      " ? utilizes tools such as python, tableau, r etc. to perform complex data analysis and visualizations\n",
      "---\n",
      " ? works closely with internal customers and it personnel to improve current processes and engineer new methods. this includes support with writing new software, testing and end-user requirements\n",
      "---\n",
      " environment: nlp, machine learning, deep learning, python, r, tableau, sas, matlab, aws, neural network data scientist progressive insurance, oh march 2017 to august 2018 the progressive corporation is one of the largest providers of car insurance in the united states. the company also insures motorcycles, boats, rvs and commercial vehicles, and provides home insurance through select companies. progressive has expanded internationally as well, offering car insurance in australia.\n",
      "---\n",
      " ? setup storage and data analysis tools in amazon web services cloud computing infrastructure.\n",
      "---\n",
      " ? used pandas, numpy, seaborn, scipy, matplotlib, sci-kit-learn, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " ? installed and used caffedeep learning framework\n",
      "---\n",
      " ? worked on different data formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " ? worked as data architects and it architects to understand the movement of data and its storage and er studio 9.7\n",
      "---\n",
      " ? participated in all phases of datamining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " ? data manipulation and aggregation from a different source using nexus, toad, business objects, powerball, and smart view.\n",
      "---\n",
      " ? implemented agile methodology for building an internal application.\n",
      "---\n",
      " ? good knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node, secondary name node, and map reduce concepts.\n",
      "---\n",
      " ? as architect delivered various complex olap databases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " ? programmed by a utility in python that used multiple packages (scipy, numpy, pandas)\n",
      "---\n",
      " ? implemented classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " ? responsible for design and development of advanced r/ python programs to prepare to transform and harmonize data sets in preparation for modelling.\n",
      "---\n",
      " ? worked on different data formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " ? worked as data architects and it architects to understand the movement of data and its storage and er studio 9.7\n",
      "---\n",
      " ? participated in all phases of datamining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " ? data manipulation and aggregation from a different source using nexus, toad, business objects, powerbl,and smart view.\n",
      "---\n",
      " ? updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " ? data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " ? handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " ? interaction with business analyst, smes, and other data architects to understand business needs and functionality for various project solutions\n",
      "---\n",
      " ? researched, evaluated, architected, and deployed new tools, frameworks, and patterns to build sustainable big data platforms for the clients.\n",
      "---\n",
      " ? updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " ? data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " ? handled importing data from various data sources, performed transformations using hive, mapreduce, and loaded data into hdfs.\n",
      "---\n",
      " ? identifying and executing process improvements, hands-on in various technologies such as oracle, informatica, business objects.\n",
      "---\n",
      " ? designed both 3nf data models for ods, oltp systems and dimensional data models using star and snowflake schemas.\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " environment: r 9.0, ods, oltp, bigdata, oracle 10g, hive, olap, db2, metadata, python, ms excel, mainframes ms vision, rational rose. data scientist essendant - deerfield, il october 2015 to february 2017 essendant, formerly known as united stationers, is a national wholesale distributor of workplace essentials, with consolidated net sales of $5.3 billion. in 2013, it ranked 484 (478 in 2012\n",
      "---\n",
      " 467 in 2011) out of the fortune 500 companies.\n",
      "---\n",
      " ? performed data profiling to learn about behavior with various features such as traffic pattern, location, and time, date and time etc.\n",
      "---\n",
      " ? application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " ? utilized spark, scala, hadoop, hbase, cassandra, mongodb, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " ? developed spark/scala, python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources. used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " ? evaluated models using cross validation, log loss function, roc curves and used auc for feature selection.\n",
      "---\n",
      " ? developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " ? addressed overfitting by implementing the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " ? used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " ? identified and targeted welfare high-risk groups with machine learning algorithms.\n",
      "---\n",
      " ? conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n",
      "---\n",
      " ? analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n",
      "---\n",
      " ? performed multinomial logistic regression, random forest, decision tree, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " ? used mllib, spark's machine learning library to build and evaluate different models.\n",
      "---\n",
      " ? implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n",
      "---\n",
      " ? performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " ? developed map reduce pipeline for feature extraction using hive.\n",
      "---\n",
      " ? created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau.\n",
      "---\n",
      " environment: python 2.x, hdfs, hadoop 2.3, hive, linux, spark, tableau desktop, sql server 2012, microsoft excel, matlab, spark sql, pyspark. data scientist coventry health care - downers grove, il december 2014 to september 2015 coventry offers workers' compensation, auto, and disability care- and cost-management solutions for employers, insurance carriers, and third-party administrators. with roots in both clinical and network services, coventry leverages 30+ years of industry experience, knowledge, and data analytics\n",
      "---\n",
      " ? responsible for data exploration, cleaning for modeling, participate in model development.\n",
      "---\n",
      " ? used principal component analysis and factor analysis in feature engineering to analyze high dimensional data in python.\n",
      "---\n",
      " ? performed data cleaning, factorization, feature engineering and feature scaling.\n",
      "---\n",
      " ? visualize, interpret, report findings, and develop strategic uses of data by python libraries like numpy, pandas, scipy, scikit-learn.\n",
      "---\n",
      " ? missing value treatment, outlier capping and anomalies treatment using statistical methods.\n",
      "---\n",
      " ? evaluated models using cross validation, log loss function, roc curves and auc for feature selection.\n",
      "---\n",
      " ? worked with several r packages including ggplot, dplyr, and knitr.\n",
      "---\n",
      " ? strong skills in data visualization like matplotlib and seaborn.\n",
      "---\n",
      " ? created different charts such as heatmaps, bar charts, line charts etc.\n",
      "---\n",
      " ? data mining using the state-of-the-art methods and dimensionality reduction using principal component analysis, t-sne for visualizing high dimensional data.\n",
      "---\n",
      " ? involved in various pre-processing phases of text data like tokenizing, stemming, lemmatization and converting the raw text data to structured data.\n",
      "---\n",
      " ? participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed gap analysis.\n",
      "---\n",
      " ? constructing the new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like bag of words, tf-idf, word2vec, and average word2vec.\n",
      "---\n",
      " ? implemented bi-directional recurrent neural networks acts as encoder to process the input and as decoder to generate the output.\n",
      "---\n",
      " ? used recurrent neural networks with lstm cells to protect the sequence information.\n",
      "---\n",
      " ? lstm cells are implemented in the recurrent neural network to get the longer-term dependencies.\n",
      "---\n",
      " ? used testing methods like a/b testing, multi-variate to measure impact on new initiatives.\n",
      "---\n",
      " ? applied binary classification and parse trees to identify key features of radiology related sentences using natural language processing (nlp).\n",
      "---\n",
      " ? using nlp developed deep learning algorithms for analyzing text, over their existing dictionary-based approaches.\n",
      "---\n",
      " ? worked on customer segmentation using unsupervised clustering techniques.\n",
      "---\n",
      " ? implemented lstm layer network of moderate depth to gain the information in the sequence with help of tensor flow.\n",
      "---\n",
      " ? created distributed environment of tensor flow across multiple devices (cpus and gpus) and run them in parallel.\n",
      "---\n",
      " ? implemented machine learning algorithms like logistic regression, softmax classifier, random forest, decision trees.\n",
      "---\n",
      " environment: cluster analysis, regression, natural language processing, spark ml lib, logistic regression, softmax classifier, random forest, python, sql, oracle 12c, nltk, recurrent neural networks, lstm cells, natural language toolkit, numpy, scipy, pandas, matplotlib, seaborn, scikit-learn, tensor flow, keras. python developer valence health - chicago, il november 2013 to november 2014 valence health works with clients to design build and manage value-based care models customized for each client including clinically integrated networks, bundled payments, risk-based contracts, accountable care organizations and provider-sponsored health plans. the project is to create an etl process and collect data to do analytics and generate reports.\n",
      "---\n",
      " ? taken part in software development life cycle (sdlc) of the tracking systems requirements, gathering, analysis, detail design, development, system testing and user acceptance testing.\n",
      "---\n",
      " ? created ui using html, css, javascript, ajax, json, and jquery.\n",
      "---\n",
      " ? implemented business logic using pythonweb frame work django.\n",
      "---\n",
      " ? designed applications implementing mvc architecture in pyramid, zopeframeworks.\n",
      "---\n",
      " ? actively involved in developing the methods for create, read, update and delete (crud) in active record.\n",
      "---\n",
      " ? designing mobile search application system requirements and coded back-end and front-end in python.\n",
      "---\n",
      " ? analysis and design of application.\n",
      "---\n",
      " ? implemented modelviewcontrol architecture in developing web applications using django frame work.\n",
      "---\n",
      " ? created backend database t-sql stored procedures and jasper reports.\n",
      "---\n",
      " ? worked with millions of database records on a daily basis, finding common errors and bad data patterns and fixing them.\n",
      "---\n",
      " ? exported/imported data between different data sources using sql server management studio.\n",
      "---\n",
      " ? maintained program libraries, users' manuals and technical documentation.\n",
      "---\n",
      " ? managed large datasets using panda data frames and mysql.\n",
      "---\n",
      " ? wrote and executed various mysql database queries from python using python-mysql connector and mysql db package.\n",
      "---\n",
      " ? carried out various mathematical operations for calculation purpose using python libraries.\n",
      "---\n",
      " ? built various graphs for business decision making using python matplotlib library.\n",
      "---\n",
      " ? fetched twitter feeds for certain important keyword using python-twitter library.\n",
      "---\n",
      " ? used python library beautifulsoup for web scrapping.\n",
      "---\n",
      " ? developed applications especially in unix environment and familiar with all of its commands.\n",
      "---\n",
      " ? deployed the project into heroku using git version control system.\n",
      "---\n",
      " ? performed troubleshooting, fixed and deployed many python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team.\n",
      "---\n",
      " ? implement code in python to retrieve and manipulate data.\n",
      "---\n",
      " environment: python 2.7, django, html5/css, pyramid, zope, mysql, ms sql, t-sql, jasper reports, javascript, eclipse, git, linux, shell scripting. data analyst ediko systems inc february 2011 to october 2013 ediko systems integrators, an ibm premier business partner, is a specialist company delivering world-class business solutions leveraging ibm technologies. ediko ensures the delivery of high-quality business integration solutions through the application of sound software architecture principles and using the latest ibm technologies together with agile project management techniques.\n",
      "---\n",
      " ? created new reports based on requirements.\n",
      "---\n",
      " ? responsible for generating weekly ad-hoc reports\n",
      "---\n",
      " ? planned, coordinated, and monitored project levels of performance and activities to ensure project completion in time.\n",
      "---\n",
      " ? automated and scheduled recurring reporting processes using unix shell scripting and teradata utilities such as mload, bteq, and fast load\n",
      "---\n",
      " ? experience with perl.\n",
      "---\n",
      " ? worked in a scrum agile process & writing stories with two-week iterations delivering a product for each iteration\n",
      "---\n",
      " ? worked on transferring the data files to the vendor through sftp&ftp process\n",
      "---\n",
      " ? involved in defining and constructing the customer to customer relationships based on association with an account & customer\n",
      "---\n",
      " ? created action filters, parameters and calculated sets for preparing dashboards and worksheets in tableau.\n",
      "---\n",
      " ? experience in performing tableau administering by using tableau admin commands.\n",
      "---\n",
      " ? worked with architects and, assisting in the development of current and target state enterprise-level data architectures\n",
      "---\n",
      " ? worked with project team representatives to ensure that logical and physical data models were developed in line with corporate standards and guidelines.\n",
      "---\n",
      " ? involved in defining the source to target data mappings, business rules, and data definitions.\n",
      "---\n",
      " ? responsible for defining the key identifiers for each mapping/interface.\n",
      "---\n",
      " ? performed data analysis and data profiling using complex sql on various sources systems including oracle and teradata.\n",
      "---\n",
      " ? migrated three critical reporting systems to business objects and web intelligence on a teradata platform\n",
      "---\n",
      " ? created excel charts and pivot tables for the adhoc data pull.\n",
      "---\n",
      " environment: ms office suite, ms visio, ms sharepoint, test management tool, ms project, crystal report, html. data analyst hidden brains july 2010 to january 2011 hidden brains infotech pvt. ltd is an enterprise web & mobile apps development company. with an industry experience of over a decade, we offer a plethora of client-centric services by enabling customers to achieve competitive advantage through flexible and next generation global delivery models.\n",
      "---\n",
      " ? processed data received from vendors and loading them into the database. the process was carried out on weekly basis and reports were delivered on a bi-weekly basis. the extracted data had to be checked for integrity.\n",
      "---\n",
      " ? documented requirements and obtained signoffs.\n",
      "---\n",
      " ? coordinated between the business users and development team in resolving issues.\n",
      "---\n",
      " ? documented data cleansing and data profiling.\n",
      "---\n",
      " ? wrote sql scripts to meet the business requirement.\n",
      "---\n",
      " ? analyzed views and produced reports.\n",
      "---\n",
      " ? tested cleansed data for integrity and uniqueness.\n",
      "---\n",
      " ? automated the existing system to achieve faster and accurate data loading.\n",
      "---\n",
      " ? generated weekly, bi-weekly reports to be sent to client business team using business objects and documented them too.\n",
      "---\n",
      " ? learned to create business process models.\n",
      "---\n",
      " ? ability to manage multiple projects simultaneously tracking them towards varying timelines effectively through a combination of business and technical skills.\n",
      "---\n",
      " ? good understanding of clinical practice management, medical and laboratory billing and insurance claim with processing with process flow diagrams.\n",
      "---\n",
      " ? assisted qa team in creating test scenarios that cover a day in a life of the patient for inpatient and ambulatory workflows.\n",
      "---\n",
      " environment: sql, data profiling, data loading, qa team. \n",
      "---\n",
      " bachelor's skills apache cassandra (1 year), apache hbase (1 year), asteradata (1 year), cassandra (1 year), database (5 years), databases (1 year), excel (5 years), hbase (1 year), linux (2 years), matlab (2 years), mongodb (1 year), ms office (2 years), ms sql server (2 years), python (5 years), scala (1 year), sql (7 years), sql server (2 years), unix (3 years), visio (2 years), xml (1 year)\n",
      "\n",
      "---\n",
      " over 2 years of experience in software development with focus on python development and java development.\n",
      "---\n",
      " extensive experience in developing web crawlers using python. worked with libraries such as beautiful soup, requests, xpath and scrapy.\n",
      "---\n",
      " hands on experience in installing, configuring and using hadoop ecosystem components like hdfs, mapreduce programming, hive and spark.\n",
      "---\n",
      " in depth understanding of spark architecture including spark core, spark sql, data frames, spark streaming, spark mlib and sparkml.\n",
      "---\n",
      " hands on knowledge in writing mapreduce programs in java to process large data sets using map and reduce tasks.\n",
      "---\n",
      " hands-on experience in big data ecosystems such as hadoop using java and spark using python (pyspark).\n",
      "---\n",
      " hands-on experience in developing applications in machine learning using sparkml(machine learning libraries in spark).\n",
      "---\n",
      " have strong knowledge in using various python libraries such as numpy, matplotlib and scipy.\n",
      "---\n",
      " hands on experience in using github and project management tool git kraken.\n",
      "---\n",
      " have strong knowledge on developing responsive websites with html, css and javascript.\n",
      "---\n",
      " strong knowledge in databases such as sql, mysql, cassandra.\n",
      "---\n",
      " a dedicated team player with excellent communication, organizational and interpersonal skills.\n",
      "---\n",
      " very quick learner and keen to adopt new technologies. authorized to work in the us for any employer work experience python developer/ machine learning engineer/ data scientist suny poly january 2017 to may 2018 worked as a graduate research assistant at suny poly where the aim of the project was to determine the extent to which the structure of customer reviews on e-commerce websites drive the number of helpful votes on each review. \n",
      "---\n",
      " master's skills apache hadoop hdfs (less than 1 year), c++ (less than 1 year), css (less than 1 year), database (less than 1 year), python (2 years), html 5 (less than 1 year), java (less than 1 year), javascript (less than 1 year), r (less than 1 year), django, aws, linux, numpy additional information technical skills\n",
      "---\n",
      " programming languages: python, java, c, c++, r\n",
      "---\n",
      " web technologies: html 5, css 3, javascript\n",
      "---\n",
      " big data ecosystems: hadoop, hdfs, spark core, spark mlib, sparkml\n",
      "---\n",
      " database: sql, mysql\n",
      "---\n",
      " platforms and tools: eclipse, pycharm, knime, visual studio, netbeans, jupyter, enthought canopy\n",
      "\n",
      "---\n",
      " data scientist/data analyst around 8+ years of experience in data science and analytics including data mining, statistical analysis with domain knowledge in retail, healthcare and banking industries.\n",
      "---\n",
      " involved in data science project life cycle, including data cleaning, data extraction, visualization, with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experience with machine learning algorithms such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means.\n",
      "---\n",
      " good experience in text analytics, developing different statistical machine learning, data mining solutions, to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, dplyr, caret) and excel\n",
      "---\n",
      " experienced the full software lifecycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova\n",
      "---\n",
      " good knowledge and experience in deep learning algorithms such as artificial neural network (ann), convolutional neural network (cnn) and recurrent neural network (rnn), lstm and rnn based speech recognition using tensorflow.\n",
      "---\n",
      " working experience on python 3.5/2.7 such as numpy, sqlalchemy, beautiful soup, pickle, pyside, pymongo, scipy, pytables.\n",
      "---\n",
      " ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008, nosql databases like mongodb 3.2\n",
      "---\n",
      " experience in big data technologies like spark 1.6, spark sql, pyspark, hadoop 2.x, hdfs, hive 1.x.\n",
      "---\n",
      " experience in data warehousing including data modeling, data architecture, data integration (etl/elt) and business intelligence.\n",
      "---\n",
      " good experience in using various python libraries (beautiful soup, numpy, scipy, matplotlib, python-twitter, pandas, mysql db for database connectivity).\n",
      "---\n",
      " having experienced in big data technologies including apache spark, hdfs, hive, mongodb.\n",
      "---\n",
      " used the version control tools like git2.x and build tools like apache maven/ant.\n",
      "---\n",
      " worked on machine learning algorithms like classification and regression with knn model, decision tree model, na\\xefve bayes model, logistic regression, svm model and latent factor model.\n",
      "---\n",
      " experience and knowledge in provisioning virtual clusters under aws cloud which includes services like ec2, s3, and emr.\n",
      "---\n",
      " good knowledge on microsoft azure.\n",
      "---\n",
      " knowledge and understanding of devops(dockers).\n",
      "---\n",
      " experience in writing sub queries, stored procedures, triggers, cursors, and functions on mysql and postgresql database.\n",
      "---\n",
      " understanding of python best practices (pep-8) and package management system (pip) in python.\n",
      "---\n",
      " extensive experience in data visualization tools like, tableau 9.x, 10.x for creating dashboards.\n",
      "---\n",
      " experience in development and designing of etl methodology for supporting data transformations and processing in a corporate-wide environment using teradata, mainframes, and unix shell scripting\n",
      "---\n",
      " used sql queries and stored procedures extensively in retrieving the contents from mysql.\n",
      "---\n",
      " good in implementing sql tuning techniques such as join indexes (ji), aggregate join indexes (aji's), statistics and table changes including index.\n",
      "---\n",
      " sql loader for direct and parallel load of data from raw file to database tables.\n",
      "---\n",
      " experience in development of t-sql, olap, pl/sql, stored procedures, triggers, functions, packages, performance tuning and optimization for business logic implementation.\n",
      "---\n",
      " strong sql server programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " good at handling complex processes using sas/ base, sas/ sql, sas/ stat sas/graph, merge, join and set statements, sas/ ods.\n",
      "---\n",
      " good industry knowledge, analytical &problem solving skills and ability to work well with in a team as well as an individual.\n",
      "---\n",
      " great team player and ability to work collaboratively and independently as required.\n",
      "---\n",
      " experience with biological data sets (genomic, transcriptomic, microbiome, etc.,) authorized to work in the us for any employer work experience data scientist l'oreal usa - berkeley heights, nj september 2017 to present python\n",
      "---\n",
      " description: l'oreal group is one of the world's largest cosmetics and beauty company. online product trading (b2b & b2c) brings dealers, distributors, manufacturers, fleet owners and traders from all around the world together and facilitates trade in an easy, secure, transparent and cost efficient way. buyers can find sellers, and sellers can find the best possible buyers. this application can be used both by customers and manufacturers. customer can browse and select their choice of l'oreal product or put their specifications if not getting the matching choice, so the manufacturers can make their choice of product and delivered to them by l'oreal product trader.\n",
      "---\n",
      " communicated and coordinated with other departments to gather business requirements.\n",
      "---\n",
      " gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n",
      "---\n",
      " participated in the installation of sas/ebi on linux platform.\n",
      "---\n",
      " worked on data modeling tools erwin data modeler to design the data models.\n",
      "---\n",
      " designed tables and implemented the naming conventions for logical and physical data models in erwin 7.0\n",
      "---\n",
      " worked on development of data warehouse, data lake and etl systems using relational and non-relational tools like sql, no sql.\n",
      "---\n",
      " created sql tables with referential integrity and developed queries using sql, sql\n",
      "---\n",
      "plus, and pl/sql.\n",
      "---\n",
      " design, coding, unit testing of etl package source marts and subject marts using informatica etl processes for oracle database\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " interaction with business analyst, smes, and other data architects to understand business needs and functionality for various project solutions.\n",
      "---\n",
      " identifying and executing process improvements, hands-on in various technologies such as oracle, informatica, business objects.\n",
      "---\n",
      " worked on data cleaning and ensured data quality, consistency, integrity using pandas, numpy.\n",
      "---\n",
      " participated in feature engineering such as feature intersection generating, feature normalize and label encoding with scikit-learn pre-processing.\n",
      "---\n",
      " improved fraud prediction performance by using random forest and gradient boosting for feature selection with python scikit-learn.\n",
      "---\n",
      " used python (numpy, scipy, pandas, scikit-learn, seaborn), and spark 2.0 (pyspark, mllib) to develop variety of models and algorithms for analytic purposes.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " implemented, tuned, and tested the model on aws ec2 to get the best algorithm and parameters.\n",
      "---\n",
      " setup storage and data analysis tools in amazon web services cloud computing infrastructure.\n",
      "---\n",
      " designed and developed machine learning models in apache - spark (mllib).\n",
      "---\n",
      " used nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " implemented deep learning algorithms such as artificial neural network (ann) and recurrent neural network (rnn), tuned hyper-parameter and improved models with python packages tensorflow.\n",
      "---\n",
      " installed and used caffe deep learning framework.\n",
      "---\n",
      " modified selected machine learning models with real-time data in in spark (pyspark).\n",
      "---\n",
      " worked with architect to improve cloud hadoop architecture as needed for research.\n",
      "---\n",
      " worked on different formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " participated in all phases of datamining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " worked very close with data architects and dba team to implement data model changes in the database in all environments.\n",
      "---\n",
      " used pandas library for statistical analysis.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 3.2/2.7, hive, oozie, tableau, informatica 9.0, html5, css, xml, mysql, ms sql server 2008/2012, javascript, aws, s3, ec2, linux, jupyter notebook, rnn, ann, spark, hadoop. data analyst cvs health - boston, ma september 2015 to august 2017 description: worked with pharmacy chain client. as the retail pharmacy, it sells prescription drugs and a wide assortment of general merchandise, including over-the-counter drugs, beauty products and cosmetics, film and photo finishing services, our goal of the project was to design, develop, and field data mining solutions that have direct impact to patients and janssen.\n",
      "---\n",
      " investigated market sizing, competitive analysis and positioning for product feasibility.\n",
      "---\n",
      " conducted research on development and designing of sample methodologies, and analyzed data for pricing of client's products.\n",
      "---\n",
      " collaborated with database engineers to implement etl process, wrote and optimized sql queries to perform data extraction and merging from sql server database.\n",
      "---\n",
      " worked on business forecasting, segmentation analysis and data mining.\n",
      "---\n",
      " developed machine learning algorithm to diagnose blood loss.\n",
      "---\n",
      " generated graphs and reports using ggplot2 package in r-studio for analytical models.\n",
      "---\n",
      " developed and implemented r and shiny application which showcases machine learning for business forecasting.\n",
      "---\n",
      " developed predictive models using decision tree, random forest and na\\xefve bayes.\n",
      "---\n",
      " performed time series analysis using tableau.\n",
      "---\n",
      " developed various workbooks in tableau from multiple data sources.\n",
      "---\n",
      " created dashboards and visualizations using tableau desktop.\n",
      "---\n",
      " later used alteryx to blend the data.\n",
      "---\n",
      " performed analysis using jmp.\n",
      "---\n",
      " perform validation on machine learning output from r.\n",
      "---\n",
      " written connectors to extract data from databases.\n",
      "---\n",
      " environment: r, python 2.x, excel 2010, machine learning, tableau, quick view, jmp, segmentation analysis data scientist the cellular connection july 2014 to august 2015 description: tcc is the largest premium wireless retailer company. i work with the lead data scientist to perform statistical analyses and predictive models on datasets to address various business problems in benefits and benefit operations through leveraging advanced statistical modeling, operations research, machine learning, or data mining techniques.\n",
      "---\n",
      " involved in data profiling to learn about user behavior and merge data from multiple data sources.\n",
      "---\n",
      " participated in big data processing applications to collect, clean and normalization large volumes of open data using hadoop ecosystems such as pig, hive, and hbase.\n",
      "---\n",
      " designed the prototype of the data mart and documented possible outcome from it for end-user\n",
      "---\n",
      " worked as analyst to generate data models using erwin and developed a relational database system.\n",
      "---\n",
      " designing and developing various machine learning frameworks using python, r and matlab.\n",
      "---\n",
      " processed huge datasets (over billion data points, over 1 tb of datasets) for data association pairing and provided insights into meaningful data association and trends\n",
      "---\n",
      " participated in all phases of data collection, data cleaning, developing models, validation, and visualization and performed gap analysis.\n",
      "---\n",
      " good knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node, secondary name node, and mapreduce concepts.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, mapreduce, and loaded data into hdfs\n",
      "---\n",
      " collaborate with data engineers to implement etl process, write and optimized sql queries to perform data extraction from cloud and merging from oracle 12c.\n",
      "---\n",
      " collect unstructured data from mongodb 3.3 and completed data aggregation.\n",
      "---\n",
      " conducted analysis of assessing customer consuming behaviors and discover the value of customers with rmf analysis\n",
      "---\n",
      " applied customer segmentation with clustering algorithms such as k-means clustering and hierarchical clustering.\n",
      "---\n",
      " participate in features engineering such as feature intersection generating, feature normalize and label encoding with scikit-learn preprocessing.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, sklearn and nltk (natural language toolkit), in python for developing various machine learning algorithms\n",
      "---\n",
      " utilized machine learning algorithms such as decision tree, linear regression, multivariate regression, naive bayes, random forests, k-means, & knn.\n",
      "---\n",
      " parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n",
      "---\n",
      " determine customer satisfaction and help enhance customer experience using nlp.\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data\n",
      "---\n",
      " perform data integrity checks, data cleaning, exploratory analysis and feature engineer using r 3.4.0\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r\n",
      "---\n",
      " worked on mapreduce/spark python modules for machine learning & predictive analytics in hadoop.\n",
      "---\n",
      " perform data visualizations with tableau 10 and generated dashboards to present the findings.\n",
      "---\n",
      " work on text analytics, na\\xefve bayes, sentiment analysis, creating word clouds, and retrieving data from twitter and other social networking platforms\n",
      "---\n",
      " use git2.6 to apply version control. tracked changes in files and coordinated work on the files among multiple team members.\n",
      "---\n",
      " environment: python 3.2/2.7, hive, tableau, r, qlikview, mysql, ms sql server 2008/2012, aws, s3, ec2, linux, jupyter notebook, rnn, ann, spark, hadoop. data analyst- python pimco - manhattan, ny may 2012 to june 2014 description: pacific investment management company is an american investment management firm provides mutual funds and other portfolio management and asset allocation solutions for millions of investors worldwide. i was involving in developing statistical models and algorithms to predict, classify, quantify, and/or forecast business metrics. partner with business units and analytics colleagues outside the workgroup to simulate and validate processes to maximize success metrics.\n",
      "---\n",
      " used python libraries like beautiful soap, numpy\n",
      "---\n",
      " created various types of data visualizations using python and tableau.\n",
      "---\n",
      " monitoring and tracking process performance using analytics tools like tableau dashboard, r\n",
      "---\n",
      " utilized standard python modules such as csv, robot parser, iterators and pickle for development.\n",
      "---\n",
      " created views in tableau desktop that were published to internal team for review and further data analysis and customization using filters and actions.\n",
      "---\n",
      " worked on python openstack apis and used numpy for numerical analysis.\n",
      "---\n",
      " used python scripts to update content in the database and manipulate files.\n",
      "---\n",
      " used python creating graphics, data exchange and business logic implementation.\n",
      "---\n",
      " performed troubleshooting, fixed and deployed many python bug fixes of the applications and involved in fine tuning of existing processes followed advance patterns and methodologies.\n",
      "---\n",
      " skilled in using collections in python for manipulating and looping through different user defined objects.\n",
      "---\n",
      " installed numerous python packages using pip and easy install.\n",
      "---\n",
      " used ddl and dml for writing triggers, stored procedures, and data manipulation.\n",
      "---\n",
      " interacted with team and analysis, design and develop database using er diagram, involved in design, development and testing of the system\n",
      "---\n",
      " developed sql server stored procedures, tuned sql queries (using indexes)\n",
      "---\n",
      " created views to facilitate easy user interface implementation and triggers on them to facilitate consistent data entry into the database.\n",
      "---\n",
      " implemented exceptional handling.\n",
      "---\n",
      " worked on client requirement and wrote complex sql queries to generate crystal reports.\n",
      "---\n",
      " created different data sources and datasets for the reports.\n",
      "---\n",
      " tuned and optimized sql queries using execution plan and profiler.\n",
      "---\n",
      " rebuilding indexes and tables as part of performance tuning exercise.\n",
      "---\n",
      " involved in performing database backup and recovery.\n",
      "---\n",
      " documented end user requirements for ssrs reports and database design. \n",
      "---\n",
      " bachelor's skills python 3.2/2.7, hive, oozie, tableau, informatica 9.0, html5, css, xml, mysql, ms sql server 2008/2012, javascript, aws, s3, ec2, linux, jupyter notebook, rnn, ann, spark, hadoop (8 years) additional information skills matrix\n",
      "---\n",
      " languages c, c++, xml, r/r studio, sas enterprise guide, sas, r, python 2.x/3.x , java, c, sql, shell scripting\n",
      "---\n",
      " no sql databases cassandra, hbase, mongodb, maria db\n",
      "---\n",
      " statistics\n",
      "---\n",
      " hypothetical testing, anova, confidence intervals, bayes law, mle, fish information, principal component analysis (pca), cross-validation, correlation.\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " tableau, tableau server, tableau reader, splunk, sap business objects, obiee, sap business intelligence, qlikview, amazon redshift, or azure data warehouse\n",
      "---\n",
      " algorithms logistic regression, random forest, xg boost, knn, svm, neural network rk, linear regression, lasso regression, k-means.\n",
      "---\n",
      " big data hadoop, hdfs, hive, putty, spark, scala, sqoop\n",
      "---\n",
      " reporting tools ms office (word/excel/powerpoint/ visio/outlook), crystal reports xi, ssrs, cognos 7.0/6.0.\n",
      "---\n",
      " database design tools and data modeling\n",
      "---\n",
      " ms visio, erwin 4.5/4.0, star schema/snowflake schema modeling, fact & dimensions tables, physical & logical data modeling, normalization and de-normalization techniques, kimball &inmon methodologies\n",
      "\n",
      "---\n",
      " data scientist/data analyst around 8+ years of experience in data science and analytics including data mining, statistical analysis with domain knowledge in retail, healthcare and banking industries.\n",
      "---\n",
      " involved in data science project life cycle, including data cleaning, data extraction, visualization, with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experience with machine learning algorithms such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means.\n",
      "---\n",
      " good experience in text analytics, developing different statistical machine learning, data mining solutions, to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, dplyr, caret) and excel\n",
      "---\n",
      " experienced the full software lifecycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova\n",
      "---\n",
      " working experience on python 3.5/2.7 such as numpy, sqlalchemy, beautiful soup, pickle, pyside, pymongo, scipy, pytables.\n",
      "---\n",
      " ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008, nosql databases like mongodb 3.2\n",
      "---\n",
      " experience in big data technologies like spark 1.6, spark sql, pyspark, hadoop 2.x, hdfs, hive 1.x.\n",
      "---\n",
      " experience in data warehousing including data modeling, data architecture, data integration (etl/elt) and business intelligence.\n",
      "---\n",
      " good knowledge and experience in deep learning algorithms such as artificial neural network (ann), convolutional neural network (cnn) and recurrent neural network (rnn), lstm and rnn based speech recognition using tensorflow.\n",
      "---\n",
      " good experience in using various python libraries (beautiful soup, numpy, scipy, matplotlib, python-twitter, pandas, mysql db for database connectivity).\n",
      "---\n",
      " having experienced in big data technologies including apache spark, hdfs, hive, mongodb.\n",
      "---\n",
      " used the version control tools like git2.x and build tools like apache maven/ant.\n",
      "---\n",
      " worked on machine learning algorithms like classification and regression with knn model, decision tree model, na\\xefve bayes model, logistic regression, svm model and latent factor model.\n",
      "---\n",
      " experience and knowledge in provisioning virtual clusters under aws cloud which includes services like ec2, s3, and emr.\n",
      "---\n",
      " good knowledge on microsoft azure.\n",
      "---\n",
      " knowledge and understanding of devops(dockers).\n",
      "---\n",
      " experience in writing sub queries, stored procedures, triggers, cursors, and functions on mysql and postgresql database.\n",
      "---\n",
      " understanding of python best practices (pep-8) and package management system (pip) in python.\n",
      "---\n",
      " extensive experience in data visualization tools like, tableau 9.x, 10.x for creating dashboards.\n",
      "---\n",
      " experience in development and designing of etl methodology for supporting data transformations and processing in a corporate-wide environment using teradata, mainframes, and unix shell scripting\n",
      "---\n",
      " used sql queries and stored procedures extensively in retrieving the contents from mysql.\n",
      "---\n",
      " good in implementing sql tuning techniques such as join indexes (ji), aggregate join indexes (aji's), statistics and table changes including index.\n",
      "---\n",
      " sql loader for direct and parallel load of data from raw file to database tables.\n",
      "---\n",
      " experience in development of t-sql, olap, pl/sql, stored procedures, triggers, functions, packages, performance tuning and optimization for business logic implementation.\n",
      "---\n",
      " strong sql server programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " good at handling complex processes using sas/ base, sas/ sql, sas/ stat sas/graph, merge, join and set statements, sas/ ods.\n",
      "---\n",
      " good industry knowledge, analytical &problem solving skills and ability to work well with in a team as well as an individual.\n",
      "---\n",
      " great team player and ability to work collaboratively and independently as required.\n",
      "---\n",
      " experience with biological data sets (genomic, transcriptomic, microbiome, etc.,) authorized to work in the us for any employer work experience data scientist the cellular connection, indiana november 2017 to present description: tcc is the largest premium wireless retailer company. i work with the lead data scientist to perform statistical analyses and predictive models on datasets to address various business problems in benefits and benefit operations through leveraging advanced statistical modeling, operations research, machine learning, or data mining techniques.\n",
      "---\n",
      " involved in data profiling to learn about user behavior and merge data from multiple data sources.\n",
      "---\n",
      " participated in big data processing applications to collect, clean and normalization large volumes of open data using hadoop ecosystems such as pig, hive, and hbase.\n",
      "---\n",
      " designed the prototype of the data mart and documented possible outcome from it for end-user\n",
      "---\n",
      " worked as analyst to generate data models using erwin and developed a relational database system.\n",
      "---\n",
      " designing and developing various machine learning frameworks using python, r and matlab.\n",
      "---\n",
      " processed huge datasets (over billion data points, over 1 tb of datasets) for data association pairing and provided insights into meaningful data association and trends\n",
      "---\n",
      " participated in all phases of data collection, data cleaning, developing models, validation, and visualization and performed gap analysis.\n",
      "---\n",
      " good knowledge of hadoop architecture and various components such as hdfs, job tracker, task tracker, name node, data node, secondary name node, and mapreduce concepts.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, mapreduce, and loaded data into hdfs\n",
      "---\n",
      " collaborate with data engineers to implement etl process, write and optimized sql queries to perform data extraction from cloud and merging from oracle 12c.\n",
      "---\n",
      " collect unstructured data from mongodb 3.3 and completed data aggregation.\n",
      "---\n",
      " conducted analysis of assessing customer consuming behaviors and discover the value of customers with rmf analysis\n",
      "---\n",
      " applied customer segmentation with clustering algorithms such as k-means clustering and hierarchical clustering.\n",
      "---\n",
      " participate in features engineering such as feature intersection generating, feature normalize and label encoding with scikit-learn preprocessing.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, sklearn and nltk (natural language toolkit), in python for developing various machine learning algorithms\n",
      "---\n",
      " utilized machine learning algorithms such as decision tree, linear regression, multivariate regression, naive bayes, random forests, k-means, & knn.\n",
      "---\n",
      " parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n",
      "---\n",
      " determine customer satisfaction and help enhance customer experience using nlp.\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data\n",
      "---\n",
      " perform data integrity checks, data cleaning, exploratory analysis and feature engineer using r 3.4.0\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r\n",
      "---\n",
      " worked on mapreduce/spark python modules for machine learning & predictive analytics in hadoop.\n",
      "---\n",
      " perform data visualizations with tableau 10 and generated dashboards to present the findings.\n",
      "---\n",
      " work on text analytics, na\\xefve bayes, sentiment analysis, creating word clouds, and retrieving data from twitter and other social networking platforms\n",
      "---\n",
      " use git2.6 to apply version control. tracked changes in files and coordinated work on the files among multiple team members.\n",
      "---\n",
      " environment: python 3.2/2.7, hive, tableau, r, qlikview, mysql, ms sql server 2008/2012, aws, s3, ec2, linux, jupyter notebook, rnn, ann, spark, hadoop. data analyst cvs health - boston, ma september 2015 to october 2017 description: worked with pharmacy chain client. as the retail pharmacy, it sells prescription drugs and a wide assortment of general merchandise, including over-the-counter drugs, beauty products and cosmetics, film and photo finishing services, our goal of the project was to design, develop, and field data mining solutions that have direct impact to patients and janssen.\n",
      "---\n",
      " investigated market sizing, competitive analysis and positioning for product feasibility.\n",
      "---\n",
      " conducted research on development and designing of sample methodologies, and analyzed data for pricing of client's products.\n",
      "---\n",
      " collaborated with database engineers to implement etl process, wrote and optimized sql queries to perform data extraction and merging from sql server database.\n",
      "---\n",
      " worked on business forecasting, segmentation analysis and data mining.\n",
      "---\n",
      " developed machine learning algorithm to diagnose blood loss.\n",
      "---\n",
      " generated graphs and reports using ggplot2 package in r-studio for analytical models.\n",
      "---\n",
      " developed and implemented r and shiny application which showcases machine learning for business forecasting.\n",
      "---\n",
      " developed predictive models using decision tree, random forest and na\\xefve bayes.\n",
      "---\n",
      " performed time series analysis using tableau.\n",
      "---\n",
      " developed various workbooks in tableau from multiple data sources.\n",
      "---\n",
      " created dashboards and visualizations using tableau desktop.\n",
      "---\n",
      " later used alteryx to blend the data.\n",
      "---\n",
      " performed analysis using jmp.\n",
      "---\n",
      " perform validation on machine learning output from r.\n",
      "---\n",
      " written connectors to extract data from databases.\n",
      "---\n",
      " environment: r, python 2.x, excel 2010, machine learning, tableau, quick view, jmp, segmentation analysis. data analyst- python pimco - manhattan, ny july 2014 to august 2015 description: pacific investment management company is an american investment management firm provides mutual funds and other portfolio management and asset allocation solutions for millions of investors worldwide. i was involving in developing statistical models and algorithms to predict, classify, quantify, and/or forecast business metrics. partner with business units and analytics colleagues outside the workgroup to simulate and validate processes to maximize success metrics.\n",
      "---\n",
      " used python libraries like beautiful soap, numpy\n",
      "---\n",
      " created various types of data visualizations using python and tableau.\n",
      "---\n",
      " monitoring and tracking process performance using analytics tools like tableau dashboard, r\n",
      "---\n",
      " utilized standard python modules such as csv, robot parser, iterators and pickle for development.\n",
      "---\n",
      " created views in tableau desktop that were published to internal team for review and further data analysis and customization using filters and actions.\n",
      "---\n",
      " worked on python openstack apis and used numpy for numerical analysis.\n",
      "---\n",
      " used python scripts to update content in the database and manipulate files.\n",
      "---\n",
      " used python creating graphics, data exchange and business logic implementation.\n",
      "---\n",
      " performed troubleshooting, fixed and deployed many python bug fixes of the applications and involved in fine tuning of existing processes followed advance patterns and methodologies.\n",
      "---\n",
      " skilled in using collections in python for manipulating and looping through different user defined objects.\n",
      "---\n",
      " installed numerous python packages using pip and easy install.\n",
      "---\n",
      " environment: python 2.7, tableau, r, windows xp, unix, html, sql server 2005. data scientist l'oreal usa - berkeley heights, nj may 2012 to june 2014 python\n",
      "---\n",
      " description: l'oreal group is one of the world's largest cosmetics and beauty company. online product trading (b2b & b2c) brings dealers, distributors, manufacturers, fleet owners and traders from all around the world together and facilitates trade in an easy, secure, transparent and cost efficient way. buyers can find sellers, and sellers can find the best possible buyers. this application can be used both by customers and manufacturers. customer can browse and select their choice of l'oreal product or put their specifications if not getting the matching choice, so the manufacturers can make their choice of product and delivered to them by l'oreal product trader.\n",
      "---\n",
      " communicated and coordinated with other departments to gather business requirements.\n",
      "---\n",
      " gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n",
      "---\n",
      " participated in the installation of sas/ebi on linux platform.\n",
      "---\n",
      " worked on data modeling tools erwin data modeler to design the data models.\n",
      "---\n",
      " designed tables and implemented the naming conventions for logical and physical data models in erwin 7.0\n",
      "---\n",
      " worked on development of data warehouse, data lake and etl systems using relational and non-relational tools like sql, no sql.\n",
      "---\n",
      " created sql tables with referential integrity and developed queries using sql, sql\n",
      "---\n",
      "plus, and pl/sql.\n",
      "---\n",
      " design, coding, unit testing of etl package source marts and subject marts using informatica etl processes for oracle database\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and big data\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " interaction with business analyst, smes, and other data architects to understand business needs and functionality for various project solutions.\n",
      "---\n",
      " identifying and executing process improvements, hands-on in various technologies such as oracle, informatica, business objects.\n",
      "---\n",
      " worked on data cleaning and ensured data quality, consistency, integrity using pandas, numpy.\n",
      "---\n",
      " participated in feature engineering such as feature intersection generating, feature normalize and label encoding with scikit-learn pre-processing.\n",
      "---\n",
      " improved fraud prediction performance by using random forest and gradient boosting for feature selection with python scikit-learn.\n",
      "---\n",
      " used python (numpy, scipy, pandas, scikit-learn, seaborn), and spark 2.0 (pyspark, mllib) to develop variety of models and algorithms for analytic purposes.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " implemented, tuned, and tested the model on aws ec2 to get the best algorithm and parameters.\n",
      "---\n",
      " setup storage and data analysis tools in amazon web services cloud computing infrastructure.\n",
      "---\n",
      " designed and developed machine learning models in apache - spark (mllib).\n",
      "---\n",
      " used nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " implemented deep learning algorithms such as artificial neural network (ann) and recurrent neural network (rnn), tuned hyper-parameter and improved models with python packages tensorflow.\n",
      "---\n",
      " installed and used caffe deep learning framework.\n",
      "---\n",
      " modified selected machine learning models with real-time data in in spark (pyspark).\n",
      "---\n",
      " worked with architect to improve cloud hadoop architecture as needed for research.\n",
      "---\n",
      " worked on different formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " participated in all phases of datamining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " worked very close with data architects and dba team to implement data model changes in the database in all environments.\n",
      "---\n",
      " used pandas library for statistical analysis.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " used ddl and dml for writing triggers, stored procedures, and data manipulation.\n",
      "---\n",
      " interacted with team and analysis, design and develop database using er diagram, involved in design, development and testing of the system\n",
      "---\n",
      " developed sql server stored procedures, tuned sql queries (using indexes)\n",
      "---\n",
      " created views to facilitate easy user interface implementation and triggers on them to facilitate consistent data entry into the database.\n",
      "---\n",
      " implemented exceptional handling.\n",
      "---\n",
      " worked on client requirement and wrote complex sql queries to generate crystal reports.\n",
      "---\n",
      " created different data sources and datasets for the reports.\n",
      "---\n",
      " tuned and optimized sql queries using execution plan and profiler.\n",
      "---\n",
      " rebuilding indexes and tables as part of performance tuning exercise.\n",
      "---\n",
      " involved in performing database backup and recovery.\n",
      "---\n",
      " documented end user requirements for ssrs reports and database design. \n",
      "---\n",
      " bachelor's skills sql, business intelligence, access, testing, excel, python 3.2/2.7, hive, tableau, r, qlikview, mysql, ms sql server 2008/2012, aws, s3, ec2, linux, jupyter notebook, rnn, ann, spark, hadoop. (8 years) additional information skills matrix\n",
      "---\n",
      " languages c, c++, xml, r/r studio, sas enterprise guide, sas, r, python 2.x/3.x , java, c, sql, shell scripting\n",
      "---\n",
      " no sql databases cassandra, hbase, mongodb, maria db\n",
      "---\n",
      " statistics\n",
      "---\n",
      " hypothetical testing, anova, confidence intervals, bayes law, mle, fish information, principal component analysis (pca), cross-validation, correlation.\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " tableau, tableau server, tableau reader, splunk, sap business objects, obiee, sap business intelligence, qlikview, amazon redshift, or azure data warehouse\n",
      "---\n",
      " algorithms logistic regression, random forest, xg boost, knn, svm, neural network rk, linear regression, lasso regression, k-means.\n",
      "---\n",
      " big data hadoop, hdfs, hive, putty, spark, scala, sqoop\n",
      "---\n",
      " reporting tools ms office (word/excel/powerpoint/ visio/outlook), crystal reports xi, ssrs, cognos 7.0/6.0.\n",
      "---\n",
      " database design tools and data modeling\n",
      "---\n",
      " ms visio, erwin 4.5/4.0, star schema/snowflake schema modeling, fact & dimensions tables, physical & logical data modeling, normalization and de-normalization techniques, kimball &inmon methodologies\n",
      "\n",
      "---\n",
      " 8+ years of it experience which includes machine learning, data mining with large datasets of structured and unstructured data, data acquisition, data validation, predictive modeling, data visualization.\n",
      "---\n",
      " a deep understanding of statistical modeling, multivariate analysis, big data analytics and standard procedures highly efficient in dimensionality reduction methods such as pca (principal component analysis), factor analysis etc. implemented bootstrapping methods such as random forests (classification), k-means clustering, knn, naivebayes, svm, decision tree, bfs, linear and logistic regression methods.\n",
      "---\n",
      " the experience of working in text understanding, classification, pattern recognition, recommendation systems, targeting systems and ranking systems using python.\n",
      "---\n",
      " experience with natural language processing (nlp).\n",
      "---\n",
      " proficiency in application of statistical prediction modeling, machine learning classification techniques and econometric forecasting techniques.\n",
      "---\n",
      " in-depth knowledge of statistical procedures that are applied in supervised / unsupervised problems.\n",
      "---\n",
      " experience in the application of neural network, support vector machines (svm), and random forest.\n",
      "---\n",
      " experienced in working with advanced analytical teams to design, build, validate and refresh data models that enable the next generation of sophisticated solutions for global clients.\n",
      "---\n",
      " extensively worked on python 3.5/2.7 (numpy, pandas, matplotlib, nltk and sci-kit learn).\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupyter notebook 4.x, r 3.0(ggplot2) and excel.\n",
      "---\n",
      " experience in designing star schema, snowflake schema for data warehouse, ods architecture.\n",
      "---\n",
      " hands on experience in business understanding, data understanding, and preparation of large databases.\n",
      "---\n",
      " experience in working with relational databases (mysql, oracle) with advanced sql programming skills\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " experience in using various packages in rand python-like ggplot2, caret, dplyr, rweka, gmodels, rcurl, tm, c50, twitter, nlp, reshape2, rjson, dplyr, pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, beautiful soup, rpy2.\n",
      "---\n",
      " extensive experience in text analytics, generating data visualizations using r, python and creating dashboards using tools like tableau.\n",
      "---\n",
      " hands on experience with big data tools like hadoop, spark, hive, pig, impala, pyspark, spark sql.\n",
      "---\n",
      " experience in using one or more cloud computing frameworks, such as aws, azure, google cloud, etc.\n",
      "---\n",
      " mapping and tracing data from system to system in order to establish data hierarchy and lineage.\n",
      "---\n",
      " experience with distributed data/computing tools, map/reduce, hadoop, hive, spark, mysql.\n",
      "---\n",
      " worked on tableau to create dashboards and visualizations.\n",
      "---\n",
      " proficiency in various type of optimization, market mix modeling, segmentation, time series, price promo models etc.\n",
      "---\n",
      " identifies/creates the appropriate algorithm to discover patterns, validate their findings using an experimental and iterative approach.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test,experiment design, hypothesis test, anova, crosstabs, ttests and correlation techniques.\n",
      "---\n",
      " applies advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems.\n",
      "---\n",
      " experience in designing stunning visualizations using tableau software and publishing and presenting dashboards, storyline on web and desktop platforms.\n",
      "---\n",
      " proficiency in sas (base sas, enterprise guide, enterprise miner)\n",
      "---\n",
      " excellent communication skills (verbal and written) to communicate with clients and team, prepare deliver effective presentations.\n",
      "---\n",
      " strong experience in software development life cycle (sdlc) including requirements analysis, design specification and testing as per cycle in both waterfall and agile methodologies.\n",
      "---\n",
      " strong experience in interacting with stakeholders/customers, gathering requirements through interviews, workshops, and existing system documentation or procedures, defining business processes, identifying and analyzing risks using appropriate templates and analysis tools.\n",
      "---\n",
      " closely works with product managers, service development managers, and product development team in productizing the algorithms developed work experience data scientist consultant bbva compass - birmingham, al august 2018 to present description: bbva compass understands that every individual and company has unique dreams and ambitions, needs and wants. we realize that few take the same path in the faster, busier, and more complex world we live in. we get it. whichever path you choose, and whenever you need us, we want to create opportunities for your bright future. from the smallest moment, to the largest personal or professional life event, bbva compass is there for you.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, and time, date and time etc.\n",
      "---\n",
      " extensively used python's multiple data science packages like pandas, numpy, matplotlib, scipy, sci-kit learn and nltk. worked on data cleaning, data preparation and feature engineering with python 3.x.\n",
      "---\n",
      " in depth understanding of nlp for tokenization, lemmatization and stemming using nltk, spacy, pattern libraries.\n",
      "---\n",
      " experience in evaluating and proposing solutions to nlp problems through various approaches.\n",
      "---\n",
      " deep background in information retrieval (ocr, speech-to-text etc.), natural language processing (nlp), knowledge representation or computational linguistics.\n",
      "---\n",
      " understanding of nlp techniques for text representation, semantic extraction techniques, data structures and modeling.\n",
      "---\n",
      " experience in java, python and nlp/mlframeworks and libraries.\n",
      "---\n",
      " worked to apply a broad array of capabilities spanning machine learning, statistics, text-mining/nlp, and modeling to extract insights to structured and unstructured healthcare data sources, pre-clinical, clinical trial and complementary real world information streams to apply a broad array of capabilities spanning machine learning, statistics, text-mining/nlp, and modeling to extract insights to structured and unstructured healthcare data sources, pre-clinical, clinical trial and complementary real world information streams.\n",
      "---\n",
      " developed user defined functions in python for rapid analysis and performed data imputation using sci-kit learnpackage in python.\n",
      "---\n",
      " developed the applications, models, used appropriate algorithms for arriving at the required insights by analyzing business requirements.\n",
      "---\n",
      " worked with unsupervised (k-means, dbscan) and supervised learning techniques (regression, classification) for feature engineering and did principal component analysis for dimensionality reduction of features.\n",
      "---\n",
      " used the classification machine learning algorithms na\\xefve bayes, linear regression, logistic regression, svm, neural networks and used clustering algorithm k means.\n",
      "---\n",
      " performed text classification task using nltk package and implemented various natural language processing techniques.\n",
      "---\n",
      " worked collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science. established partnerships with product and engineering teams and work closely with other teams.\n",
      "---\n",
      " used spark-streamingapis to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from kafka in near real time.\n",
      "---\n",
      " worked on end to end pipe line in spark and on apache spark for analyzing the live streaming data.\n",
      "---\n",
      " worked on spark python modules for machine learning and predictive analytics in spark on aws.\n",
      "---\n",
      " explored and analyzed the customer specific features by using sparksql.\n",
      "---\n",
      " created hive scripts to create external, internal data tables on hive. worked on creating datasets to load data into hive.\n",
      "---\n",
      " worked on tableau for data visualization to create reports, dashboards for insights and business process improvement. created the dashboards and reports in tableau for visualizing the data in required format.\n",
      "---\n",
      " environment: spark, apache spark, hive, machine learning, python, numpy, nltk, pandas, scipy, mysql, tableau, sqoop, hbase, hdfs, tableau, dynamodb, mongo db, sql server, and etl. data scientist visa - austin, tx may 2017 to july 2018 description: visa is a dynamic, global enterprise, and innovation is at the heart of everything we do. we're looking for smart, ambitious people to join our austin team. we are a global payments technology company working to enable consumers, businesses, banks and governments to use digital currency.\n",
      "---\n",
      " analyzed pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, &knn for data analysis.\n",
      "---\n",
      " demonstrated experience in design and implementation of statistical models, predictive models, enterprise data model, metadata solution and data life cycle management in both rdbms, big data environments.\n",
      "---\n",
      " utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n",
      "---\n",
      " developed mapreduce/sparkpython modules for machine learning& predictive analytics in hadoop on aws. implemented a python-based distributed random forest via python streaming.\n",
      "---\n",
      " performed source system analysis, database design, data modeling for the warehouse layer using mldm concepts and package layer using dimensional modeling.\n",
      "---\n",
      " created ecosystem models (e.g. conceptual, logical, physical, canonical) that are required for supporting services within the enterprise data architecture (conceptual data model for defining the major subject areas used, ecosystem logical model for defining standard business meaning for entities and fields, and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem)\n",
      "---\n",
      " developed linuxshell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " designed and implemented system architecture for amazonec2 based cloud-hosted solution for client.\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " hands on database design, relational integrity constraints, olap, oltp, cubes and normalization (3nf) and de-normalization of database.\n",
      "---\n",
      " developed mapreduce/spark python modules for machine learning& predictive analytics in hadoop on aws.\n",
      "---\n",
      " worked on customer segmentation using an unsupervised learning technique - clustering.\n",
      "---\n",
      " worked with various teradata15 tools and utilities like teradataviewpoint, multi load, arc, teradataadministrator, bteq and other teradata utilities.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, sparkstreaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction.\n",
      "---\n",
      " analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n",
      "---\n",
      " environment: erwin r9.6, python, sql, oracle 12c, netezza, sql server, informatica, java, ssrs, pl/sql, t-sql, tableau, mllib, regression, cluster analysis, scala nlp, cassandra, map reduce, spark, kafka, mongo db, logistic regression, hadoop, hive, teradata0, random forest, olap, azure, maria db, sap crm, hdfs, ods, nltk, svm, json, tableau, xml, aws. data scientist black tree healthcare consulting - king of prussia, pa january 2016 to april 2017 description: black tree healthcare consulting provides revenue cycle, outsourcing and clinical consulting services to the home health, hospice and skilled nursing industries. the primary goal of the project was to determine the most efficient input-output ratio through data modeling and analysis, thus providing the best resource combination.\n",
      "---\n",
      " incident and problem management, coordinating resolution of data movement disruptions.\n",
      "---\n",
      " capacity analysis and planning for a sizable application server environment. performance metering and tuning for application servers, database, monitors and alerts.\n",
      "---\n",
      " risk identification and evaluation, managing and improving upon internal controls which mitigate risks\n",
      "---\n",
      " working with development teams to identify enhancement opportunities and resolve code defects\n",
      "---\n",
      " code deployment and configuration management, coordination and execution of changes within a complex testing environment\n",
      "---\n",
      " data collection procedures enhancement to include information that is relevant for building analytic systems processing, cleansing, and verifying the integrity of data used for analysis.\n",
      "---\n",
      " building analytic models using a variety of techniques such as logistic regression, risk scorecards and pattern recognition technologies.\n",
      "---\n",
      " analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data, create variables, build models and test those models.\n",
      "---\n",
      " excellent understanding of machine learning techniques and algorithms, such as logistic regression, svm, random forests, deep learning etc.\n",
      "---\n",
      " in depth understanding and experience in nlp and deep learning.\n",
      "---\n",
      " expertise in sentiment analysis, entity extraction, document classification, topic modeling, natural language understanding (nlu) and natural language generation (nlg).\n",
      "---\n",
      " leads all nlp/nlg driven solutions for the project\n",
      "---\n",
      " design nlp models for searching structured/unstructured data in real/near-real time\n",
      "---\n",
      " interacts regularly with product team members ensuring successful integration of nlp solutions in the product architecture\n",
      "---\n",
      " keeps the solutions updated with the recent developments in nlp/nlg.\n",
      "---\n",
      " perform a proper eda, univariateand bi-variate analysis to understand the intrinsic effect/combined.\n",
      "---\n",
      " performing ad-hoc analysis and presenting results in a clear manner and constant tracking of model performance.\n",
      "---\n",
      " worked with data governance, data quality, data lineage, data architect to design various models.\n",
      "---\n",
      " designed data models and data flow diagrams using erwin and ms visio. independently coded new programs and designed tables to load and test the program effectively for the given poc's.\n",
      "---\n",
      " developed implemented & maintained the conceptual, logical & physical data models using erwin for forward/reverse engineered databases.\n",
      "---\n",
      " experience with common data science toolkits, such as r, python, spark, etc.\n",
      "---\n",
      " applied statistics skills, such as statistical sampling, testing, regression, etc. work with technical and development teams to deploy models. build model performance reports and\n",
      "---\n",
      " designing technical documentation to support each of the models for the product line.\n",
      "---\n",
      " exploratory data analysis and data visualizations using r, and tableau. established data architecture strategy, best practices, standards, and roadmaps.\n",
      "---\n",
      " lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n",
      "---\n",
      " involved in analysis of business requirement, design and development of high level and low-level designs, unit and integration testing. interacted with the other departments to understand and identify data needs and requirements.\n",
      "---\n",
      " worked with several r packages including knitr, dplyr, spark-r, causal infer, space time.\n",
      "---\n",
      " environment: r 3.x, unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop, nosql, mysql, oltp, random forest, olap, hdfs, ods. data analyst grey hound - dallas, tx march 2014 to december 2015 description: greyhound is an intercity bus common carrier serving over 3,800 destinations across north america. behavior of users on our website generate large amount of data, which challenges our ability to gather, parse, analyze and infer from it. so, a major task is to implement new capabilities for reporting from existing data warehouse and empower users with better solutions to increase operational efficiency for data warehousing needs. the aim of the project is to gain more visibility into key metrics of operations in the company and to gain insights into how to drive improvements and take the business to a whole new level with advanced analysis through the application of data science.\n",
      "---\n",
      " involved in requirement gathering, data analysis and interacted with business users to understand the reporting requirements, analyzing bi needs for the user community.\n",
      "---\n",
      " created entity/relationship diagrams, grouped and created the tables, validated the data, identified pks for lookup tables.\n",
      "---\n",
      " involved in modeling (star schema methodologies) in building and designing the logical data model into dimensional models.\n",
      "---\n",
      " created and maintained logical, dimensional data models for different claim types and hipaa standards.\n",
      "---\n",
      " implemented one-many, many-many entity relationships in the data modeling of data warehouse.\n",
      "---\n",
      " experience working with mdm team with various business operations involved within the organization.\n",
      "---\n",
      " identify the primary key, foreign key relationships across the entities and across subject areas.\n",
      "---\n",
      " developed etl routines using ssis packages, to plan an effective package development process and design the control flow within the packages.\n",
      "---\n",
      " worked with big data architects for setting up big data platform in the organization and on hive platform to create hive data models\n",
      "---\n",
      " developed customized training documentation based on each client's technical needs and built a curriculum to help each client learn both basic and advanced techniques for using postgre sql.\n",
      "---\n",
      " took an active role in the design, architecture, and development of user interface objects in qlik view applications. connected to various data sources like sql server, oracle, and flat files.\n",
      "---\n",
      " presented the dashboard to business users and cross-functional teams, define kpis (key performance indicators), and identify data sources.\n",
      "---\n",
      " designed data flows that (etl) extract, transform, and load data by optimizing ssis performance.\n",
      "---\n",
      " deliver end to end mapping from source (guide wire application) to target (cdw) and legacy systems coverages to landing zone and to guide wire reporting pack.\n",
      "---\n",
      " involved in loading the data from source tables to operational data source tables using transformation and cleansing logic.\n",
      "---\n",
      " performed the data accuracy, data analysis, data quality checks before and after loading the data.\n",
      "---\n",
      " resolved the data type inconsistencies between the source systems and the target system using the mapping documents.\n",
      "---\n",
      " generated tableau dashboards for claims with forecast and reference lines.\n",
      "---\n",
      " designed, developed, implemented and maintained informaticapower center and informatica data quality (idq) application for matching and merging process.\n",
      "---\n",
      " created ad-hoc reports to users in tableau by connecting various data sources.\n",
      "---\n",
      " worked on the reporting requirements for the data warehouse.\n",
      "---\n",
      " created support documentation and worked closely with production support and testing team.\n",
      "---\n",
      " environment: erwin8.2, oracle 11g, obiee, crystal reports, toad, sybase power designer, datahub, ms visio, db2, qlikview 11.6, informatica. data analyst unisys global services - bengaluru, karnataka december 2012 to february 2014 description: unisys is a global information technology company that builds high-performance, security-centric solutions for the most demanding businesses and governments on earth. unisys offerings include security software and services\n",
      "---\n",
      " digital transformation and workplace services\n",
      "---\n",
      " industry applications and services\n",
      "---\n",
      " and innovative software operating environments for high-intensity enterprise computing.\n",
      "---\n",
      " implemented and updated analytical methods such as regression modelling, classification tree, statistical tests and data visualization techniques with python.\n",
      "---\n",
      " performed exploratory data analysis, data wrangling and development of algorithms in r and python for data mining and analysis.\n",
      "---\n",
      " analysis of customer data and other operational data in mysql and ms access to provide insights that enable improvements to customer experience.\n",
      "---\n",
      " utilized pandas and numpy packages in python to improve data collection and distribution processes as well as to enhance reporting capabilities to provide clear line of sight into key performance trends and metrics.\n",
      "---\n",
      " performed data analysis, data manipulation, data transformation and data mapping of source data from the mysqlserver.\n",
      "---\n",
      " understanding and adherence to the principles of data quality management including metadata, lineage, and business definitions.\n",
      "---\n",
      " analyzed historical demand, filter out outliers/exceptions, identify the most appropriate statistical forecasting algorithm, develop base plan, understand variance, propose improvement opportunities, and incorporate demand signal into forecast and executed data visualization by using plotly package in python.\n",
      "---\n",
      " participated in all phases of research including data collection, data cleaning, data mining, development models and visualizations.\n",
      "---\n",
      " examine customer feedback and activity for use in detecting or confirming fraud, using a combination of text analytics, statistical modeling, and classification.\n",
      "---\n",
      " deep understanding of software development life cycle (sdlc) as well as agile/scrum methodology to accelerate software development iteration.\n",
      "---\n",
      " improvisations and maintenance to existing automated solutions. used ms visio, ms project to assist the team in project planning, quality plan, risk management, requirements management, change management, defect management, change management and release management.\n",
      "---\n",
      " environment: mysql, statistical modelling, python libraries, pandas, numpy packages, r, ms visio, ms project, ms access. data analyst/ python developer hidden brains - hyderabad, telangana january 2011 to november 2012 description: hidden brains infotech pvt. ltd is an enterprise web & mobile apps development company. with an industry experience of over a decade, we offer a plethora of client-centric services by enabling customers to achieve competitive advantage through flexible and next generation global delivery models.\n",
      "---\n",
      " designed and developed transformation logic for bi tools (informatica) for data transformation into various layers of data warehouse.\n",
      "---\n",
      " performed code development with the help of internal python library that speeds up database querying and allow users to write more resilient etl jobs.\n",
      "---\n",
      " implemented postgresql, sql servers to develop stored procedures, views to create result sets to meet varying reporting requirements.\n",
      "---\n",
      " ensured data integrity using advanced excel formulas (lookup functions, pivot table, if statements etc.) for analyzing data.\n",
      "---\n",
      " hands on experience in writing queries in sql and r to extract, transform and load (etl) data from large datasets using data staging.\n",
      "---\n",
      " participated in the a/b testing conducted by bianalytics team for data extraction and exploratory analysis.\n",
      "---\n",
      " generated dashboards and presented the analysis to researchers explaining insights on the data.\n",
      "---\n",
      " provided analytical insights and decision support tools for executives for accurate decision making by identifying, measuring and recommending improvement strategies for kpis across all business areas.\n",
      "---\n",
      " submitted summaries, charts, and graphs to team and stakeholders that help to interpret findings based on complex excel reports.\n",
      "---\n",
      " responsible for credit data related warehouse creation that could help with risk assessment for commercial loans.\n",
      "---\n",
      " performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis.\n",
      "---\n",
      " worked in manipulating various management reports in ms excel for sales metrics using vlookup and pivot tables.\n",
      "---\n",
      " involved in estimating, defining, implementing, and utilizing business metrics calculations and methodologies.\n",
      "---\n",
      " environment: excel 2010, r, informatica power center 9.0, python, postgresql, ms sql server 200. \n",
      "---\n",
      " bachelor's skills cassandra, hdfs, impala, mapreduce, sqoop, hbase, kafka, elasticsearch, etl, flume, hadoop, informatica, mongodb, nlp, redis, tableau server, teradata, data modeling, database, database design additional information technical skills\n",
      "---\n",
      " programming & scripting languages r, c, c++, java, jcl, cobol, html, css, jsp, java script\n",
      "---\n",
      " databases sql, hive, impala, pig, spark sql, databases sql-server, my sql, ms access, hdfs, hbase, teradata, netezza, mongodb, cassandra.\n",
      "---\n",
      " statistical software spss, r, sas.\n",
      "---\n",
      " web packages\n",
      "---\n",
      " ggplot2, caret, dplyr, rweka, gmodels, rcurl, tm, c50, twitter, nlp, reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplot lib, scikit-learn, beautiful soup, rpy2, sqlalchemy.\n",
      "---\n",
      " big data ecosystem\n",
      "---\n",
      " hdfs, pig, mapreduce, hive, sqoop, flume, hbase, storm, kafka, elastic search, redis, flume, storm, kafka, elastic search, redis, flume, scoop.\n",
      "---\n",
      " statistical methods\n",
      "---\n",
      " time series, regression models, splines, confidence intervals, principal component analysis and dimensionality reduction, bootstrapping\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " tableau, tableau server, tableau reader, sap business objects, obiee, qlikview, sap business intelligence, amazon redshift, or azure data warehouse\n",
      "---\n",
      " database design tools and data modeling erwin r 9.6, 9.5, 9.1, 8.x, rational rose, er/studio, ms visio, sap power designer.\n",
      "---\n",
      " cloud/ etl tools aws, s3, ec2, informatica power centre, ssis.\n",
      "---\n",
      " operating system windows, linux, unix, macintosh hd, red hat.\n",
      "---\n",
      " big data / grid technologies cassandra, coherence, mongo db, zookeeper, titan, elasticsearch, storm, kafka, hadoop\n",
      "---\n",
      " tools and utilities\n",
      "---\n",
      " sql server management studio, sql server enterprise manager, sql server profiler, import & export wizard, visual studio.net, microsoft management console, visual source safe 6.0, dts, crystal reports, power pivot, proclarity, microsoft office, excel power pivot, excel data explorer, tableau, jira,sparkmllib.\n",
      "---\n",
      " project:1 role: data scientist\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with around 8+ years of experience in data science and analytics including data mining , deep learning/machine learning and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionand datavisualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, , dplyr, caret) and excel\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008/2010/2012, nosql databases like mongodb 3.2\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " developed predictive models using decision tree, naive bayes, logistic regression, random forest, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, scipy, numpy and pandas for data analysis.\n",
      "---\n",
      " worked with complex applications such as r,r shiny, sas, plotly, arcgis, matlab and spss to develop neural network, cluster analysis.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with nosqldatabase including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, mapreduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssisand ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql andpython and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. authorized to work in the us for any employer work experience data scientist/ machine learning rauxa - new york, ny august 2018 to present description: makers of results, rauxa applies data, technology, and content to create measurable impact at maximum speed for clients that include gap inc., tgi fridays, and verizon. the country's largest woman-owned independent advertising agency.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xgboost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation&visualization, web scraping, machine learning, python programming, sql, git, mongodb, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used caffe deep learning framework\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " worked as data architects and it architects to understand the movement of data and its storage and er studio 9.7\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " implemented agile methodology for building an internal application.\n",
      "---\n",
      " focus on integration overlap and informatica newer commitment to mdm with the acquisition of identity systems.\n",
      "---\n",
      " coded proprietary packages to analyze and visualize spcfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olapdatabases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " used teradata utilities such as fast export, mload for handling various tasks data migration/etl from oltp source systems to olap target systems\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, nosql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and mapreduce. data scientist charter communication - st. louis, mo may 2017 to july 2018 description: charter communications, inc. is an american telecommunications and mass media company that offers its services to consumers and businesses under the branding of spectrum.\n",
      "---\n",
      " utilized spark, scala, hadoop, hql, vql, oozie, pyspark, data lake, tensorflow, hbase, cassandra, redshift, mongodb, kafka, kinesis, spark streaming, edward, cuda, mllib, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " worked onanalyzing data from google analytics, adwords, facebook etc.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, date and time etc.\n",
      "---\n",
      " categorized comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve datafrom oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " exploring dag's, their dependencies and logs using airflow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as caffe, neon.\n",
      "---\n",
      " developed spark/scala,r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " tracking operations using sensors until certain criteria is met using airflowtechnology.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata using utilities like tpump, fexp,bteq, mload, fload etc\n",
      "---\n",
      " analyze traffic patterns by calculating autocorrelation with different time lags.\n",
      "---\n",
      " ensured that the model has low false positive rate and text classification and sentiment analysis for unstructured and semi-structured data.\n",
      "---\n",
      " addressed over fitting by implementing of the algorithm regularization methods like l1 and l2.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " used mllib, spark's machine learning library to build and evaluate different models.\n",
      "---\n",
      " implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n",
      "---\n",
      " developed mapreduce pipeline for feature extraction using hive and pig.\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data analyst edac technologies corp - cheshire, ct january 2016 to april 2017 description: edac technologies corporation provides design, manufacturing, and services for tooling, fixtures, molds, jet engine components, and machine spindles in the aerospace, industrial, semiconductor, and medical device markets\n",
      "---\n",
      " worked with bi team in gathering the report requirements and also sqoop to export data into hdfs and hive\n",
      "---\n",
      " involved in the below phases of analytics using r, python and jupyter notebook.\n",
      "---\n",
      " a. data collection and treatment: analysed existing internal data and external data, worked on entry errors,classification errors and defined criteria for missing values\n",
      "---\n",
      " b. data mining: used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers, market basket analysis used for customer purchasing behaviour and part/product association.\n",
      "---\n",
      " developed multiple map reduce jobs in java for data cleaning and preprocessing.\n",
      "---\n",
      " assisted with data capacity planning and node forecasting.\n",
      "---\n",
      " installed, configured and managed flume infrastructure\n",
      "---\n",
      " administrator for pig, hive and hbase installing updates patches and upgrades.\n",
      "---\n",
      " worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n",
      "---\n",
      " worked on performing major upgrade of cluster from cdh3u6 to cdh4.4.0\n",
      "---\n",
      " developed map reduce programs to extract and transform the data sets and results were exported back to rdbms using sqoop.\n",
      "---\n",
      " patterns were observed in fraudulent claims using text mining in r and hive.\n",
      "---\n",
      " exported the data required information to rdbms using sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n",
      "---\n",
      " developed map reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " created tables in hive and loaded the structured (resulted from map reduce jobs) data\n",
      "---\n",
      " using hiveql developed many queries and extracted the required information.\n",
      "---\n",
      " created hive queries that helped market analysts spot emerging trends by comparing fresh data with edw reference tables and historical metrics.\n",
      "---\n",
      " was responsible for importing the data (mostly log files) from various sources into hdfs using flume\n",
      "---\n",
      " enabled speedy reviews and first mover advantages by using oozie to automate data loading into the hadoop distributed file system and pig to pre-process the data.\n",
      "---\n",
      " provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n",
      "---\n",
      " managed and reviewed hadoop log files.\n",
      "---\n",
      " tested raw data and executed performance scripts.\n",
      "---\n",
      " environment: hdfs, pig, hive, map reduce, linux, hbase, flume, sqoop, r, vmware, eclipse, cloudera, python. data modeler/data analyst dorman products inc - colmar, pa march 2014 to december 2015 description: dorman products, inc. supplies automotive replacement parts, automotive hardware, and brake products to the automotive aftermarket and mass merchandise markets in the united states, canada, mexico, europe, the middle east, and australia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " created and maintained logical and physical models for the data mart. created partitions and indexes for the tables in the data mart.\n",
      "---\n",
      " performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " developed sql scripts for creating tables , sequences , triggers , views and materialized views\n",
      "---\n",
      " worked on query optimization and performance tuning using sql profiler and performance monitoring.\n",
      "---\n",
      " developed mappings to load fact and dimension tables, scd type 1 and scd type 2 dimensions and incremental loading and unit tested the mappings.\n",
      "---\n",
      " utilized erwin's forward / reverse engineering tools and target database schema conversion process.\n",
      "---\n",
      " worked on creating enterprise wide model edm for products and services in teradata environment based on the data from pdm. conceived, designed, developed and implemented this model from the scratch.\n",
      "---\n",
      " building, publishing customized interactive reports and dashboards, report scheduling using tableau server\n",
      "---\n",
      " write sql scripts to test the mappings and developed traceability matrix of business requirements mapped to test scripts to ensure any change control in requirements leads to test case update.\n",
      "---\n",
      " responsible for development and testing of conversion programs for importing data from text files into map oracle database utilizing perl shell scripts &sql\n",
      "---\n",
      "loader.\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " developed and executed load scripts using teradata client utilities multiload, fastload and bteq.\n",
      "---\n",
      " exporting and importing the data between different platforms such as sas, ms-excel.\n",
      "---\n",
      " generated periodic reports based on the statistical analysis of the data using sql server reporting services (ssrs).\n",
      "---\n",
      " worked with the etl team to document the transformation rules for data migration from oltp to warehouse environment for reporting purposes.\n",
      "---\n",
      " created sql scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.\n",
      "---\n",
      " formatting the data sets read into sas by using format statement in the data step as well as proc format.\n",
      "---\n",
      " applied business objects best practices during development with a strong focus on reusability and better performance.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity - relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " designed different type of star schemas for detailed data marts and plan data marts in the olap environment.\n",
      "---\n",
      " environment: erwin, ms sql server 2008, db2, oracle sql developer, pl/sql, business objects, erwin, ms office suite, windows xp, toad, sql\n",
      "---\n",
      "plus, sql\n",
      "---\n",
      "loader, teradata, netezza, sas, tableau, business objects, ssrs, tableau, sql assistant, informatica, xml.. python developer dabur india ltd - ghaziabad, uttar pradesh december 2012 to february 2014 description: dabur is one of the india's largest ayurvedic medicine & natural consumer products manufacturer. dabur demerged its pharma business in 2003 and hived it off into a separate company, dabur pharma ltd. german company fresenius se bought a 73.27% equity stake in dabur pharma in june 2008 at rs 76.50 a share.\n",
      "---\n",
      " involved in the design, development and testing phases of application using agile methodology.\n",
      "---\n",
      " designed and maintained databases using python and developed python based api (restful web service) using flask, sqlalchemy and postgresql.\n",
      "---\n",
      " designed and developed the ui of the website using html, xhtml, ajax, css and javascript.\n",
      "---\n",
      " participated in requirement gathering and worked closely with the architect in designing and modeling.\n",
      "---\n",
      " worked on restful web services which enforced a stateless client server and support json few changes from soap to restful technology involved in detailed analysis based on the requirement documents.\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " responsible for setting up python rest api framework and spring frame work using django\n",
      "---\n",
      " develope consumer based features and applications using python, django, html, behavior driven development (bdd) and pair based programming.\n",
      "---\n",
      " designed and developed components using python with django framework. implemented code in python to retrieve and manipulate data.\n",
      "---\n",
      " involved in development of the enterprise social network application using python, twisted, and cassandra.\n",
      "---\n",
      " used python and django creating graphics, xml processing of documents, data exchange and business logic implementation between servers.\n",
      "---\n",
      " orked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " designed and developed the ui for the website with html, xhtml, css, java script and ajax\n",
      "---\n",
      " used ajax&json communication for accessing restfulweb services data payload.\n",
      "---\n",
      " designed dynamic client-side javascript codes to build web forms and performed simulations for web application page.\n",
      "---\n",
      " created and implemented sql queries, stored procedures, functions, packages and triggers in sql server.\n",
      "---\n",
      " successfully implemented auto complete/auto suggest functionality using jquery, ajax, web service and json.\n",
      "---\n",
      " environment: python 2.5, java/j2ee, django1.0, html,css linux, shell scripting, java script, ajax, jquery, json, xml, postgresql, jenkins, ant, maven, subversion, python data analyst/data modeler kotak mahindra bank - mumbai, maharashtra january 2011 to november 2012 description: kotak mahindra bank is an indian private sector bank headquartered in mumbai, maharashtra, india. in february 2003, reserve bank of india issued the licence to kotak mahindra finance ltd., the group's flagship company, to carry on banking business.\n",
      "---\n",
      " analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n",
      "---\n",
      " analyzed and designed best fit logical and physical data models and relational database definitions using db2. generated reports of data definitions.\n",
      "---\n",
      " involved in normalization/de-normalization, normal form and database design methodology.\n",
      "---\n",
      " maintained existing etl procedures, fixed bugs and restored software to production environment.\n",
      "---\n",
      " developed the code as per the client's requirements using sql, pl/sql and data warehousing concepts.\n",
      "---\n",
      " involved in dimensional modeling (star schema) of the data warehouse and used erwin to design the business process, dimensions and measured facts.\n",
      "---\n",
      " worked with data warehouse extract and load developers to design mappings for data capture, staging, cleansing, loading, and auditing.\n",
      "---\n",
      " developed enterprise data model management process to manage multiple data models developed by different groups\n",
      "---\n",
      " designed and created data marts as part of a data warehouse.\n",
      "---\n",
      " wrote complex sql queries for validating the data against different kinds of reports generated by business objects xir2.\n",
      "---\n",
      " using erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n",
      "---\n",
      " coordinated with dba in implementing the database changes and also updating data models with changes implemented in development, qa and production. worked extensively with dba and reporting team for improving the report performance with the use of appropriate indexes and partitioning.\n",
      "---\n",
      " developed data mapping, transformation and cleansing rules for the master data management architecture involved oltp, ods and olap.\n",
      "---\n",
      " tuned and coded optimization using different techniques like dynamic sql, dynamic cursors, and tuning sql queries, writing generic procedures, functions and packages.\n",
      "---\n",
      " experienced in gui, relational database management system (rdbms), designing of olap system environment as well as report development.\n",
      "---\n",
      " extensively used sql, t-sql and pl/sql to write stored procedures, functions, packages and triggers.\n",
      "---\n",
      " analyzed of data report were prepared weekly, biweekly, monthly using ms excel, sql & unix\n",
      "---\n",
      " environment: er studio, informatica power center 8.1/9.1, power connect/ power exchange, oracle 11g, mainframes,db2 ms sql server 2008, sql,pl/sql, xml, windows nt 4.0, tableau, workday, spss, sas, business objects, xml, tableau, unix shell scripting, teradata, netezza, aginity \n",
      "---\n",
      " bachelors in computer science royal college of technology 2011 skills linux, solaris, sun, unix additional information operating systems all versions of windows, unix, linux, macintosh hd, sun solaris\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with around 8+ years of experience in data science and analytics including data mining , deep learning/machine learning and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extraction and data visualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, knn, svm, random forest, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, , dplyr, caret) and excel\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008/2010/2012, nosql databases like mongo db 3.2\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " developed predictive models using decision tree, naive bayes, logistic regression, random forest, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, scipy, numpy and pandas for data analysis.\n",
      "---\n",
      " worked with complex applications such as r,r shiny, sas, plotly, arcgis, matlab and spss to develop neural network, cluster analysis.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with no sql database including hbase, cassandra and mongo db.\n",
      "---\n",
      " experienced in big data with hadoop, map reduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssis and ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql and python and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. work experience data scientist/ machine learning charter communication - st. louis, mo august 2018 to present description: charter communications, inc. is an american telecommunications and mass media company that offers its services to consumers and businesses under the branding of spectrum.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xg boost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation and visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation& visualization, web scraping, machine learning, python programming, sql, git, mongo db, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used caffe deep learning framework\n",
      "---\n",
      " worked as different data formats such as json, xml and performed machine learning algorithms in python. data architects and it architects to understand the movement of data and its storage and er studio 9.7\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " implemented agile methodology for building an internal application.\n",
      "---\n",
      " focus on integration overlap and informatica newer commitment to mdm with the acquisition of identity systems.\n",
      "---\n",
      " coded proprietary packages to analyze and visualize spc file data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olap databases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " used teradata utilities such as fast export, mload for handling various tasks data migration/etl from oltp source systems to olap target systems\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, no sql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and map reduce. data scientist microsoft - redmond, wa may 2017 to july 2018 description: microsoft corporation is an american multinational technology company with headquarters in redmond, washington. it develops, manufactures, licenses, supports and sells computer software, consumer electronics, personal computers, and related services.\n",
      "---\n",
      " utilized spark, scala, hadoop, hql, vql, oozie, pyspark, data lake, tensor flow, hbase, cassandra, redshift, mongo db, kafka, kinesis, spark streaming, edward, cuda, mllib, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " worked on analyzing data from google analytics, ad words and facebook etc.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elastic search, kibana.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, date and time etc.\n",
      "---\n",
      " categorized comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve datafrom oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " exploring dag's, their dependencies and logs using air flow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as caffe, neon.\n",
      "---\n",
      " developed spark/scala, r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " tracking operations using sensors until certain criteria is met using air flow technology.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata using utilities like tpump, fexp,bteq, mload, fload etc\n",
      "---\n",
      " analyze traffic patterns by calculating autocorrelation with different time lags.\n",
      "---\n",
      " ensured that the model has low false positive rate and text classification and sentiment analysis for unstructured and semi-structured data.\n",
      "---\n",
      " addressed over fitting by implementing of the algorithm regularization methods like l1 and l2.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " used mllib, spark's machine learning library to build and evaluate different models.\n",
      "---\n",
      " implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n",
      "---\n",
      " developed map reduce pipeline for feature extraction using hive and pig.\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data analyst john deere - waterloo, ia january 2016 to april 2017 description: john deere is the brand name of deere & company, an american corporation that manufactures agricultural, construction, and forestry machinery, diesel engines, drivetrains used in heavy equipment, and lawn care equipment.\n",
      "---\n",
      " worked with bi team in gathering the report requirements and also sqoop to export data into hdfs and hive\n",
      "---\n",
      " involved in the below phases of analytics using r, python and jupyter notebook.\n",
      "---\n",
      " a. data collection and treatment: analysed existing internal data and external data, worked on entry errors, classification errors and defined criteria for missing values\n",
      "---\n",
      " b. data mining: used cluster analysis for identifying customer segments, decision trees used for profitable and non-profitable customers, market basket analysis used for customer purchasing behaviour and part/product association.\n",
      "---\n",
      " developed multiple map reduce jobs in java for data cleaning and preprocessing.\n",
      "---\n",
      " assisted with data capacity planning and node forecasting.\n",
      "---\n",
      " installed, configured and managed flume infrastructure.\n",
      "---\n",
      " administrator for pig, hive and hbase installing updates patches and upgrades.\n",
      "---\n",
      " worked closely with the claims processing team to obtain patterns in filing of fraudulent claims.\n",
      "---\n",
      " worked on performing major upgrade of cluster from cdh3u6 to cdh4.4.0\n",
      "---\n",
      " developed map reduce programs to extract and transform the data sets and results were exported back to rdbms using sqoop.\n",
      "---\n",
      " patterns were observed in fraudulent claims using text mining in r and hive.\n",
      "---\n",
      " exported the data required information to rdbms using sqoop to make the data available for the claims processing team to assist in processing a claim based on the data.\n",
      "---\n",
      " developed map reduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " created tables in hive and loaded the structured (resulted from map reduce jobs) data\n",
      "---\n",
      " using hiveql developed many queries and extracted the required information.\n",
      "---\n",
      " created hive queries that helped market analysts spot emerging trends by comparing fresh data with edw reference tables and historical metrics.\n",
      "---\n",
      " was responsible for importing the data (mostly log files) from various sources into hdfs using flume\n",
      "---\n",
      " enabled speedy reviews and first mover advantages by using oozie to automate data loading into the hadoop distributed file system and pig to pre-process the data.\n",
      "---\n",
      " provided design recommendations and thought leadership to sponsors/stakeholders that improved review processes and resolved technical problems.\n",
      "---\n",
      " managed and reviewed hadoop log files.\n",
      "---\n",
      " tested raw data and executed performance scripts.\n",
      "---\n",
      " environment: hdfs, pig, hive, map reduce, linux, hbase, flume, sqoop, r, vmware, eclipse, cloudera, python. data modeler/data analyst ericsson - atlanta, ga march 2014 to december 2015 description: ericsson is a swedish multinational networking and telecommunications company headquartered in stockholm. the company offers services, software and infrastructure in information and communications technology for telecommunications operators, traditional telecommunications and internet protocol (ip) networking equipment, mobile and fixed broadband, operations and business support services, cable television, iptv, video systems, and an extensive services operation.\n",
      "---\n",
      " created and maintained logical and physical models for the data mart. created partitions and indexes for the tables in the data mart.\n",
      "---\n",
      " performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/designed the relational models.\n",
      "---\n",
      " maintained metadata (data definitions of table structures) and version controlling for the data model.\n",
      "---\n",
      " developed sql scripts for creating tables , sequences , triggers , views and materialized views\n",
      "---\n",
      " worked on query optimization and performance tuning using sql profiler and performance monitoring.\n",
      "---\n",
      " developed mappings to load fact and dimension tables, scd type 1 and scd type 2 dimensions and incremental loading and unit tested the mappings.\n",
      "---\n",
      " utilized erwin's forward / reverse engineering tools and target database schema conversion process.\n",
      "---\n",
      " worked on creating enterprise wide model edm for products and services in teradata environment based on the data from pdm. conceived, designed, developed and implemented this model from the scratch.\n",
      "---\n",
      " building, publishing customized interactive reports and dashboards, report scheduling using tableau server\n",
      "---\n",
      " write sql scripts to test the mappings and developed traceability matrix of business requirements mapped to test scripts to ensure any change control in requirements leads to test case update.\n",
      "---\n",
      " responsible for development and testing of conversion programs for importing data from text files into map oracle database utilizing perl shell scripts &sql\n",
      "---\n",
      "loader.\n",
      "---\n",
      " involved in extensive data validation by writing several complex sql queries and involved in back-end testing and worked with data quality issues.\n",
      "---\n",
      " developed and executed load scripts using teradata client utilities multiload, fastload and bteq.\n",
      "---\n",
      " exporting and importing the data between different platforms such as sas, ms-excel.\n",
      "---\n",
      " generated periodic reports based on the statistical analysis of the data using sql server reporting services (ssrs).\n",
      "---\n",
      " worked with the etl team to document the transformation rules for data migration from oltp to warehouse\n",
      "---\n",
      " environment for reporting purposes.\n",
      "---\n",
      " created sql scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.\n",
      "---\n",
      " formatting the data sets read into sas by using format statement in the data step as well as proc format.\n",
      "---\n",
      " applied business objects best practices during development with a strong focus on reusability and better performance.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity - relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " designed different type of star schemas for detailed data marts and plan data marts in the olap environment.\n",
      "---\n",
      " environment: erwin, ms sql server 2008, db2, oracle sql developer, pl/sql, business objects, erwin, ms office suite, windows xp, toad, sql\n",
      "---\n",
      "plus, sql\n",
      "---\n",
      "loader, teradata, netezza, sas, tableau, business objects, ssrs, tableau, sql assistant, informatica, xml.. python developer penny wise solutions - hyderabad, telangana december 2012 to february 2014 description: penny wise solutions is a full service digital and software solutions provider based in hyderabad, india. we leverage an in-depth understanding of social, mobile, analytics and cloud (smac) to deliver integrated digital solutions.\n",
      "---\n",
      " involved in the design, development and testing phases of application using agile methodology.\n",
      "---\n",
      " designed and maintained databases using python and developed python based api (restful web service) using flask, sql alchemy and postgre sql.\n",
      "---\n",
      " designed and developed the ui of the website using html, xhtml, ajax, css and javascript.\n",
      "---\n",
      " participated in requirement gathering and worked closely with the architect in designing and modeling.\n",
      "---\n",
      " worked on restful web services which enforced a stateless client server and support json few changes from soap to restful technology involved in detailed analysis based on the requirement documents.\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " responsible for setting up python rest api framework and spring frame work using django\n",
      "---\n",
      " develope consumer based features and applications using python, django, html, behavior driven development (bdd) and pair based programming.\n",
      "---\n",
      " designed and developed components using python with django framework. implemented code in python to retrieve and manipulate data.\n",
      "---\n",
      " involved in development of the enterprise social network application using python, twisted, and cassandra.\n",
      "---\n",
      " used python and django creating graphics, xml processing of documents, data exchange and business logic implementation between servers.\n",
      "---\n",
      " orked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " designed and developed the ui for the website with html, xhtml, css, java script and ajax\n",
      "---\n",
      " used ajax&json communication for accessing restful web services data payload.\n",
      "---\n",
      " designed dynamic client-side javascript codes to build web forms and performed simulations for web application page.\n",
      "---\n",
      " created and implemented sql queries, stored procedures, functions, packages and triggers in sql server.\n",
      "---\n",
      " successfully implemented auto complete/auto suggest functionality using jquery, ajax, web service and json.\n",
      "---\n",
      " environment: python, java/j2ee, django, html,css linux, shell scripting, java script, ajax, jquery, json, xml, postgresql, jenkins, ant, maven, subversion, python data analyst/data modeler aspect software - bengaluru, karnataka january 2011 to november 2012 description: aspect software, inc. is an american multinational call center technology and customer experience company headquartered in phoenix, arizona. aspect has offices in africa, asia, oceania, europe, north america, and south america.\n",
      "---\n",
      " analyzed data sources and requirements and business rules to perform logical and physical data modeling.\n",
      "---\n",
      " analyzed and designed best fit logical and physical data models and relational database definitions using db2. generated reports of data definitions.\n",
      "---\n",
      " involved in normalization/de-normalization, normal form and database design methodology.\n",
      "---\n",
      " maintained existing etl procedures, fixed bugs and restored software to production environment.\n",
      "---\n",
      " developed the code as per the client's requirements using sql, pl/sql and data warehousing concepts.\n",
      "---\n",
      " involved in dimensional modeling (star schema) of the data warehouse and used erwin to design the business process, dimensions and measured facts.\n",
      "---\n",
      " worked with data warehouse extract and load developers to design mappings for data capture, staging, cleansing, loading, and auditing.\n",
      "---\n",
      " developed enterprise data model management process to manage multiple data models developed by different groups\n",
      "---\n",
      " designed and created data marts as part of a data warehouse.\n",
      "---\n",
      " wrote complex sql queries for validating the data against different kinds of reports generated by business objects xir2.\n",
      "---\n",
      " using erwin modeling tool, publishing of a data dictionary, review of the model and dictionary with subject matter experts and generation of data definition language.\n",
      "---\n",
      " coordinated with dba in implementing the database changes and also updating data models with changes implemented in development, qa and production. worked extensively with dba and reporting team for improving the report performance with the use of appropriate indexes and partitioning.\n",
      "---\n",
      " developed data mapping, transformation and cleansing rules for the master data management architecture involved oltp, ods and olap.\n",
      "---\n",
      " tuned and coded optimization using different techniques like dynamic sql, dynamic cursors, and tuning sql queries, writing generic procedures, functions and packages.\n",
      "---\n",
      " experienced in gui, relational database management system (rdbms), designing of olap system environment as well as report development.\n",
      "---\n",
      " extensively used sql, t-sql and pl/sql to write stored procedures, functions, packages and triggers.\n",
      "---\n",
      " analyzed of data report were prepared weekly, biweekly, monthly using ms excel, sql & unix\n",
      "---\n",
      " environment: er studio, informatica power center 8.1/9.1, power connect/ power exchange, oracle 11g, mainframes,db2 ms sql server 2008, sql,pl/sql, xml, windows nt 4.0, tableau, workday, spss, sas, business objects, xml, tableau, unix shell scripting, teradata, netezza, aginity \n",
      "---\n",
      " bachelor's skills linux, solaris, sun, unix\n",
      "\n",
      "---\n",
      " and machine learning to provide competitive analysis insights to ibm offering managers\n",
      "---\n",
      " - currently, leading a data science to build a solution for ibm sales enablement, which leverages machine learning to provide sellers with predictions on clients' future needs\n",
      "---\n",
      " - part of the recruiting committee, where i lead various initiatives related to diversity strategy consultant/data science intern ibm may 2017 to december 2017 chief analytics office, ibm\n",
      "---\n",
      " - conduct research and analytics on ibm's current products\n",
      "---\n",
      " - follow agile practices and work collaboratively on github\n",
      "---\n",
      " - back-end development work, which includes building an nlp engine to perform web\n",
      "---\n",
      " scraping, sentiment analysis, and information extraction to gain insight into customers'\n",
      "---\n",
      " experiences with ibm's products\n",
      "---\n",
      " - front-end development work, including the building of a dashboard in javascript/html/css web developer and research analyst the futures initiative 2016 to 2017 - contribute to the administration of a multi-site network (futuresinitiative.org) using the commons in a box platform, built on wordpress and buddypress\n",
      "---\n",
      " - conduct analytics including network analysis and present research in public venues confer- ences\n",
      "---\n",
      " - provide technical support, documentation and digital guidance for city university of new\n",
      "---\n",
      " york students\n",
      "---\n",
      " - conduct research and analysis in content areas relevant to the futures initiative's goals\n",
      "---\n",
      " - offer guidance and ideas on design for futuresinitiative.org and hastac.org python tutorial writer digital ocean 2016 to 2017 - write tutorials for python for the digital ocean community\n",
      "---\n",
      " - portfolio: www.digitalocean.com/community/users/michellemorales visiting research intern - advisor s. scherer usc's institute for creative technologies june 2016 to august 2016 http://ict.usc.edu/\n",
      "---\n",
      " - worked on the detection and computational analysis of psychological signals project\n",
      "---\n",
      " - built an nlp system to automatically assess depression and ptsd using text-based\n",
      "---\n",
      " linguistic markers\n",
      "---\n",
      " - implemented a recurrent neural network (lstm) to predict psychological signals from sentence sequences natural language processing consultant gistly 2015 to 2016 http://gist.ly\n",
      "---\n",
      " - worked on a summarization tool that enables users to get the 'gist' research assistant - advisor a. rosenberg queens college 2013 to 2016 speech lab @ queens college, speech.cs.qc.cuny.edu\n",
      "---\n",
      " - participated in the 2013/2014 compare challenges, building systems to detect physi- cal/cognitive load and severity of parkinson's disease\n",
      "---\n",
      " - created classification system for multi-language gender, native language and occupation\n",
      "---\n",
      " detection of twitter users. delivered software for deft, funded by darpa. fellow magnet 2012 to 2016 the cuny pipeline program, www.diversiphd.com\n",
      "---\n",
      " - serve as an advisor to undergraduate students involved in the cuny pipeline program\n",
      "---\n",
      " - guide students through the graduate school application process methods in computational linguistics graduate assistant the graduate center, cuny september 2014 to november 2014 advanced syntax graduate assistant the graduate center, cuny march 2014 to may 2014 adult esl teacher great neck public schools 2012 to 2013 ny high school esl teacher h. frank carey high school, ny september 2012 to november 2012 elementary school esl teacher p.s. 20 john bowne school, ny march 2012 to may 2012 \n",
      "---\n",
      " ph.d. in computational linguistics in computational linguistics the graduate center, city university of new york 2012 to 2017 m.phil in computational linguistics in computational linguistics the graduate center, city university of new york 2012 to 2015 m.s. in applied linguistics in applied linguistics queens college, city university of new york 2010 to 2012 b.a. in spanish studies and literature marist college 2007 to 2010\n",
      "\n",
      "---\n",
      " professional qualified data scientist with over 7+ years of experience in data science and analytics including machine learning, data mining and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data\n",
      "---\n",
      " having knowledge on apache spark and developing data processing and analysis algorithms using python\n",
      "---\n",
      " understanding of machine learning (\"ml\") concepts and application of algorithms in non-academic environments\n",
      "---\n",
      " strong software development background in functional and object-oriented programming\n",
      "---\n",
      " expertized in developing machine learning algorithm using python\n",
      "---\n",
      " ability to manipulate, transform, and analyze abstract data structures such as dataframes\n",
      "---\n",
      " fundamental understanding of machine learning concepts including training models, as well as understanding precision and recall in the real world\n",
      "---\n",
      " strong programming experience in the following: r, python, matlab.\n",
      "---\n",
      " performed collection, cleansing, and verification of structured and unstructured data\n",
      "---\n",
      " experience in visualization tools like, tableau 9, 10 for creating dashboards\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git\n",
      "---\n",
      " passionate about cleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " enhancing data collection procedures to include information that is relevant for building analytic systems processing, cleansing, and verifying the integrity of data used for analysis\n",
      "---\n",
      " doing ad-hoc analysis and presenting results in a clear manner\n",
      "---\n",
      " constant tracking of model performance\n",
      "---\n",
      " excellent understanding of machine learning techniques and algorithms, such as logistic regression, svm, random forests, deep learning etc.\n",
      "---\n",
      " worked with data governance, data quality, data lineage, data architect to design various models.\n",
      "---\n",
      " independently coded new programs and designed tables to load and test the program effectively for the given poc's.\n",
      "---\n",
      " extending company's data with third party sources of information when needed.\n",
      "---\n",
      " designed data models and data flow diagrams using erwin and ms visio.\n",
      "---\n",
      " as an architect implemented mdm hub to provide clean, consistent data for a soa implementation.\n",
      "---\n",
      " developed implemented & maintained the conceptual, logical & physical data models using erwin for forward/reverse engineered databases.\n",
      "---\n",
      " experience with common data science toolkits, such as r, python, spark, etc.\n",
      "---\n",
      " good applied statistics skills, such as statistical sampling, testing, regression, etc.\n",
      "---\n",
      " build analytic models using a variety of techniques such as logistic regression, risk scorecards and pattern recognition technologies.\n",
      "---\n",
      " analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data, create variables, build models and test those models.\n",
      "---\n",
      " work with technical and development teams to deploy models. build model performance reports and modeling technical documentation to support each of the models for the product line.\n",
      "---\n",
      " performed exploratory data analysis and data visualizations using r, and tableau.\n",
      "---\n",
      " perform a proper eda, univariate and bi-variate analysis to understand the intrinsic effect/combined.\n",
      "---\n",
      " established data architecture strategy, best practices, standards, and roadmaps.\n",
      "---\n",
      " lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n",
      "---\n",
      " involved in analysis of business requirement, design and development of high level and low-level designs, unit and integration testing.\n",
      "---\n",
      " worked with several r packages including knitr, dplyr, sparkr, causalinfer, spacetime.\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements.\n",
      "---\n",
      " communicated and coordinated with other departments to collection business requirement\n",
      "---\n",
      " worked on miss value imputation, outliers identification with statistical methodologies using pandas, numpy\n",
      "---\n",
      " participated in features engineering such as feature creating, feature scaling and one-hot encoding with scikit-learn\n",
      "---\n",
      " tackled highly imbalanced fraud dataset using undersampling with ensemble methods, oversampling and cost sensitive algorithms\n",
      "---\n",
      " improved fraud prediction performance by using random forest and gradient boosting for feature selection with python scikit-learn\n",
      "---\n",
      " implemented machine learning model (logistic regression, xgboost) with python scikit- learn\n",
      "---\n",
      " optimized algorithm with stochastic gradient descent algorithm fine-tuned the algorithm parameter with manual tuning and automated tuning such as bayesian optimization\n",
      "---\n",
      " validated and select models using k-fold cross validation, confusion matrices and worked on optimizing models for high recall rate\n",
      "---\n",
      " implemented ensemble models with majority votes to enhance the efficiency and performance\n",
      "---\n",
      " build analytics systems, data structures, gather and manipulate data, using statistical techniques and predictive modelling to tell people story.\n",
      "---\n",
      " designing suite of interactive dashboards, which provided an opportunity to scale and measure the statistics of the hr dept. which was not possible earlier and schedule and publish reports.\n",
      "---\n",
      " provided and created data presentation to reduce biases and telling true story of people by pulling millions of rows of data using sql, analysis of data.\n",
      "---\n",
      " worked on machine learning to compare the metrics of hr data closely\n",
      "---\n",
      " technology stack: r, machine learning, sql server and tableau.\n",
      "---\n",
      " building data capabilities for hr with respect to measure and resolve attrition issues through metrics.\n",
      "---\n",
      " analyzing recruiting history and employee performance to identify best candidates for hiring. identify employees for training and succession planning.\n",
      "---\n",
      " involved in the process of load, transform and analyze data from various sources into hdfs (hadoop distributed file system) using hive, pig and sqoop.\n",
      "---\n",
      " experienced in handling data from different datasets join and preprocess those using pig join operations.\n",
      "---\n",
      " worked on pig script to count the number of times a particular url was opened in a particular duration.\n",
      "---\n",
      " developed pig udfs for the needed functionality such as custom pigsloader known as timestamp loader.\n",
      "---\n",
      " created hive tables based on the business requirements.\n",
      "---\n",
      " pig scripts and hive queries were used to analyze the large data sets.\n",
      "---\n",
      " scheduling and managing jobs on a hadoop cluster using oozie work flow.\n",
      "---\n",
      " written mapreduce code to process and parsing the data from various sources and storing parsed data into hbase and hive using hbase - hive integration.\n",
      "---\n",
      " worked on developing custom mapreduce programs and user defined functions (udfs) in hive to transform the large volumes of data with respect to business requirement.\n",
      "---\n",
      " involved in creating hive tables and working on them using hive ql.\n",
      "---\n",
      " involved in moving all log files generated from various sources to hdfs for further processing through flume.\n",
      "---\n",
      " implemented frameworks using java and python to automate the ingestion flow.\n",
      "---\n",
      " loading and transforming of large sets of structured and semi structured data.\n",
      "---\n",
      " exported filtered data into hbase for fast query.\n",
      "---\n",
      " involved in the installation, configuration and used the hadoop ecosystem components such as map reduce, hdfs, pig, hive, flume, hbase.\n",
      "---\n",
      " exposure on multi-threading factory to distribute learning process back-testing and the into various worker processes.\n",
      "---\n",
      " different testing methodologies like unit testing, integration testing and web application testing.\n",
      "---\n",
      " design and implemented custom scripts.\n",
      "---\n",
      " extensive use of version controller team foundation server (tfs).\n",
      "---\n",
      " test and validated the custom scripts.\n",
      "---\n",
      " delivered automated solutions for science models.\n",
      "---\n",
      " managed, developed, and designed a dashboard control panel for customers and administrators using oracle db, and vmware api calls.\n",
      "---\n",
      " implemented configuration changes for data models.\n",
      "---\n",
      " maintained and updated existing automated solutions.\n",
      "---\n",
      " handled potential points of failure through error handling and communication of failure.\n",
      "---\n",
      " anticipated potential parts of failure (database, communication points, file system errors).\n",
      "---\n",
      " troubleshoot the process execution and worked with other team members to correct them.\n",
      "---\n",
      " developed gui using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser.\n",
      "---\n",
      " interacted with qa to develop test plans from high-level design documentation\n",
      "---\n",
      " environments: python, mysql, html, javascript, hql, git, web services. \n",
      "---\n",
      " bachelor's skills python (7 years), machine learning (5 years), hadoop (3 years), hadoop (3 years), deep learning (1 year) additional information technical skills:\n",
      "---\n",
      " machine learning: classification, regression, clustering, feature engineering, deep learning, neural networks\n",
      "---\n",
      " programming languages: python (pandas, scikit-learn, numpy, scipy, graphlab create), r, sql, hadoop (mapreduce, sqoop, flume), scala, java, spark (pyspark, mllib)\n",
      "---\n",
      " operating systems: linux (centos, ubuntu, kali linux), windows, macos\n",
      "---\n",
      " software tools: pycharm, jupyter notebook, r studio, tableau, microsoft office, eclipse ide\n",
      "\n",
      "---\n",
      " developed a natural language processing(nlp) python applicalon by concurrent neural network to predict a\n",
      "---\n",
      " sentence by given 3 seed words.\n",
      "---\n",
      " created a python scraper to collect training and teslng arlcle from various new websites. \n",
      "---\n",
      " hands on experience about how to apply pandas data frame for data analysis. python and php developer peekabuy - san mateo, ca august 2016 to august 2016 \n",
      "---\n",
      " created a fully functional image processing python application to crop clothing from photos.\n",
      "---\n",
      " designed and combined algorithms to save 90% running time.\n",
      "---\n",
      " applied rgb and pil to process images to collect related clothing\\x92s vision information.\n",
      "---\n",
      " enhanced accuracy by shrinking image to get main hue, cutting unrelated background, etc.\n",
      "---\n",
      " talked with server to upload all cropped image automatically.\n",
      "---\n",
      " developed a website to communicate with server to show all results. ios/swift2 developer self - eugene, or march 2016 to april 2016 \n",
      "---\n",
      " created a rss reader to show news titles and images from techcrunch.\n",
      "---\n",
      " used nsxmlparser to parse article titles, links, images from techcrunch\\x92s xml page.\n",
      "---\n",
      " applied uitableview to show article titles and images of news.\n",
      "---\n",
      " redirected users to techcrunch\\x92s website via uiwebview.\n",
      "---\n",
      " provided neat ui design with simple operations to make it user friendly to all ages users from 3 to 80.\n",
      "---\n",
      " hands on experience of using external info resource to support app development. java developer self - eugene, or february 2016 to march 2016 \n",
      "---\n",
      " predicted a user\\x92s rating to a specific song\n",
      "---\n",
      " applied java to do data preprocessing, data table combination, cleaning, transformation and selection\n",
      "---\n",
      " compared 6 different algorithms\\x92 results by using 70% clean data for training and the rest of 30% for testing\n",
      "---\n",
      " trained and tested data by each algorithm on weka to get its accuracy value\n",
      "---\n",
      " hands on experience of applying ai algorithms and knew pros and cons of each algorithm php/mysql developer self - eugene, or january 2016 to february 2016 \n",
      "---\n",
      " built a website for hotel managers to edit, update, delete their hotels\\x92 basic info, images on server.\n",
      "---\n",
      " gave managers\\x92 right to access to their accounts to check guests\\x92 contact and booking info. \n",
      "---\n",
      " enhanced 20% safety coefficient by protecting managers\\x92 accounts with encryption technique.\n",
      "---\n",
      " designed extendable database and developed web by mvc pattern to keep its expandability. c++ developer self - eugene, or october 2015 to october 2015 \n",
      "---\n",
      " built a handy cross-platform desktop application runs on mac and windows for users to manage contact info.\n",
      "---\n",
      " cooperated with teammates to construct the project, write design document and guide.\n",
      "---\n",
      " applied qwidget to create all user interface objects\n",
      "---\n",
      " implemented qfile to export, import contact info files to application\n",
      "---\n",
      " hands on experience for qt\\x92s basic, border, flow layout. ios/swift developer self - eugene, or june 2015 to august 2015 \n",
      "---\n",
      " created a travel diary mobile application which can access to device\\x92 photos and geographical location.\n",
      "---\n",
      " implemented core data to store password, diary, photos, location info locally.\n",
      "---\n",
      " enhanced 20% user privacy safety by secure login and only user allowed per device\n",
      "---\n",
      " extensive understanding for corelocation, uiimagepickercontroller, autolayout and hands on experience\n",
      "---\n",
      " using swift (1.2). \n",
      "---\n",
      " bachelor's in computer and information science university of oregon - eugene, or 2012 to 2016 skills python (2 years), java (2 years), php (1 year) links https://github.com/tangzekun https://www.linkedin.com/in/tangzekun https://angel.co/zekun-tang-1 additional information skills\n",
      "---\n",
      " programing languages: swift, python, java, mysql, c, c++, php, html, css, javascript\n",
      "---\n",
      " ide: xcode, eclipse\n",
      "---\n",
      " os: linux, mac, windows\n",
      "---\n",
      " tools: workbench, github, bitbucket, aws\n",
      "\n",
      "---\n",
      " data scientist with 12 plus years of experience in, statistical modeling, machine learning, data mining with structured and unstructured data.\n",
      "---\n",
      " performed data acquisition, data validation, predictive modeling and data visualization.\n",
      "---\n",
      " expertise in python (2.x/3.x) programming with multiple packages including numpy, pandas, matplotlib, scipy, seaborn andscikit-learn.\n",
      "---\n",
      " hands on experience in implementing lda, na\\xefve bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " implementation experiences in machine learning and deep learning, including regression, classification,natural language processing (nlp) using packages like nltk, spacy.\n",
      "---\n",
      " experience in tuning algorithms using methods such as grid search, randomized search, k-fold cross validation and error analysis.\n",
      "---\n",
      " also worked with several boosting methodologies like ada boost, gradient boosting andxgboost.\n",
      "---\n",
      " validated the machine learning classifiers using accuracy, auc, roccurves and lift charts.\n",
      "---\n",
      " worked with various text analytics or word embedding libraries like word2vec, count vectorizer, glove, lda etc.\n",
      "---\n",
      " solid knowledge and experience in deep learning techniques including feedforward neural network, convolutional neural network (cnn), recursive neural network (rnn).\n",
      "---\n",
      " worked with numerous data visualization tools in python like matplotlib, seaborn, ggplot, pygal.\n",
      "---\n",
      " worked and extracted data from various database sources like oracle, sql server,and mongodb.\n",
      "---\n",
      " highly skilled in using hadoop, hbase, spark, and hive for basic analysis and extraction of data in the infrastructure to provide data summarization.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, mapreduce, and loaded data into hdfs.\n",
      "---\n",
      " experience on working with different operating systems like unix, linux, and windows.\n",
      "---\n",
      " experience working with ms word, ms excel, ms powerpoint, ms sharepoint, and ms project. work experience data scientist zoetis inc april 2018 to present zoetis inc. is the world's largest producer of medicine and vaccinations for pets and livestock. zoetis delivers quality medicines, vaccines and diagnostic products, which are complemented by genetic tests, bio devices and a range of services. the project is to collect data from different sources and create a master data set and doing predictions on sales and profits. measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " developing analytical databases from different sources and create a master data set.\n",
      "---\n",
      " responsible for data identification, collection, exploration, cleaning for modeling.\n",
      "---\n",
      " performed time series analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values.\n",
      "---\n",
      " determined customer satisfaction and helped enhance customer experience using nlp.\n",
      "---\n",
      " manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products.\n",
      "---\n",
      " analysis of biological and spatial data to develop insights into precision animal management and precision medicine.\n",
      "---\n",
      " implementing analytics algorithms in python.\n",
      "---\n",
      " performed training natural language models and reinforcement learning engines to optimize intelligent agents that automate task execution.\n",
      "---\n",
      " performed k-means clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering, customized and priority service, to improve existing profitable relationships and to avoid customer churn, etc using python.\n",
      "---\n",
      " performed text analytics on unstructured email data using natural language processing tool kit (nltk).\n",
      "---\n",
      " worked with text to vector representation methods including counter vectorizer, tf-idf for topic modelling.\n",
      "---\n",
      " applying clustering algorithms to group the data on their similar behavior patterns.\n",
      "---\n",
      " work with data analytics team to develop time series and optimization.\n",
      "---\n",
      " experienced in data scraping.\n",
      "---\n",
      " used pyspark machine learning library to build and evaluate different models.\n",
      "---\n",
      " created various proof of concepts (poc) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.\n",
      "---\n",
      " experienced in agile methodology.\n",
      "---\n",
      " used tableau to generate reports with internal records, secondary sources of data, json, csv and more.which helped the support team for better marketing.\n",
      "---\n",
      " environment: python 3.7.0, pyspark, nltk, sql server , microsoft excel, sql, aws, tablau, sqoop, etl, agile. data scientist mars solutions group, wi january 2016 to march 2018 the client is the largest healthcare provider and offers health care products, insurance services, data analytics, payment integrity, and the project was to build predictive models for customer value analysis by applying machine learning methods, principal component analysis, and regression on large dataset.\n",
      "---\n",
      " worked on machinelearning, datamining with largedatasets of structured and unstructureddata, dataacquisition, datavalidation, predictivemodeling, datavisualization.\n",
      "---\n",
      " performed multinomial logistic regression, random forest machine learning algorithms.\n",
      "---\n",
      " used aws to manage the data in cloud.\n",
      "---\n",
      " good knowledge on hadoop components such as hdfs, job tracker, tasktracker, name node, data node, and mapreduce concepts.\n",
      "---\n",
      " maintained updated log filesusing python.\n",
      "---\n",
      " alteryx was used for data preparation.\n",
      "---\n",
      " used machine learning algorithms like logistic regression ,knn, decision trees, random forest to make the data to fit for the desired output.\n",
      "---\n",
      " interact and brain-storm with multifunctional teams to explore the opportunities of using data to improve business and health-care outcomes.\n",
      "---\n",
      " developed machine-learning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected business/health-care goals.\n",
      "---\n",
      " design, coding, unit testing of etl package source marts and subject marts using informatica etl processes for oracledatabase.\n",
      "---\n",
      " worked in agile methodology.\n",
      "---\n",
      " generated reports with internal records, secondary sources of data, json, csv and more.\n",
      "---\n",
      " developed various qlikview data models by extracting and using the data from various sources files, db2, excel, flat files and bigdata.\n",
      "---\n",
      " provided schedules, status reports, and issue resolutions to the project team, business users, and project managers.\n",
      "---\n",
      " environment: python 3.x, linux, spark, sql server 2012, microsoft excel, , spark sql, aws, qlikview ,sqoop, etl, agile. data analyst / data scientist cms energy - jackson, mi september 2013 to december 2015 cms energy is an energy company that is focused principally on utility operations. i was responsible for building a new data science department with the help of other departments and i was able to learn how the business is operated and helped the company to grow and stay ahead of the competition. by using machine learning we improvised the predictive algorithm for pricing strategy. and we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me.\n",
      "---\n",
      " worked on data manipulation & visualization, machine learning, python, and sql.\n",
      "---\n",
      " transformed the business requirementsinto analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " worked on customer segmentation using an unsupervised learning technique - clustering.\n",
      "---\n",
      " implemented classification using supervised learninglike logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " built models using statistical techniques and machine learning classification models like xg boost,svm, and random forest.\n",
      "---\n",
      " improved model's accuracy by using gradient boosting technique like light gbm and gained around 82% accuracy with random forest and 77% with logistic regression.\n",
      "---\n",
      " used jupyter notebook for spark to make data manipulations.\n",
      "---\n",
      " developed etl processes for data conversions and construction of data warehouse using informatica.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements and work with other members of the it organization to deliver data visualization and reporting solutions to address those needs.\n",
      "---\n",
      " environment: tableau 10.05, aws, git, python 3.5.2 , anaconda-navigator , hadoop, nosql, random forest, mongodb, hdfs, , nltk, xml, mapreduce, informatica. data analyst karvy financial services limited july 2011 to august 2013 karvy financial services limited is a company which has been playing a very proactive role in the economic growth of india by providing loans to micro & small business segments and individuals like credit for the requirements of different sectors of economy. industries, exports, trading, agriculture, infrastructure and the individual segments. we worked on various projects which handles customer analytics, credit risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans, identify and prevent fraud detection for transactions.\n",
      "---\n",
      " compiled data from various sources public and private databases to perform complex analysis anddata manipulation for actionable results.\n",
      "---\n",
      " applied concepts of probability, distribution, and statistical inference on the given dataset to unearthinteresting findings using comparison, t-test, f-test, r-squared, p-value etc.\n",
      "---\n",
      " applied linear regression, multiple regression, ordinary least square method, mean-variance, the theory of large numbers, logistic regression, dummy variable, residuals, poisson distribution, naive bayes, fitting function etc to data with help of scikit, scipy, numpy and pandas module ofpython.\n",
      "---\n",
      " applied principal component analysis (pca) based unsupervised technique to determine unusual vpn log-on time.\n",
      "---\n",
      " performed clustering with historical, demographic and behavioral data as features to implement the personalized marketing to the customers.\n",
      "---\n",
      " also created classification model using logistic regression, random forests to classify dependent variable into two classes which are risky and okay.\n",
      "---\n",
      " used f-score, precision, recall evaluating model performance.\n",
      "---\n",
      " built user behavior models for finding activity patterns and evaluating risk scores for everytransaction using historic data to train the supervised learning models such as decision trees, random forests and svm.\n",
      "---\n",
      " real time analysis of customer financial profile and providing recommendation for financial products best suited.\n",
      "---\n",
      " performed sentimental analysis (nlp) on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by long-short term memory (lstm) cells in recurrent neural networks(rnn).\n",
      "---\n",
      " forecasted demand for loans and interest rates using time series analysis like arimax, varmax and holt-winters.\n",
      "---\n",
      " obtained better predictive performance of 81% accuracy using ensemble methods like bootstrap aggregation (bagging) and boosting (light gbm, gradient).\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " performed financial data ingestion to the spark distribution environment, using kafka.\n",
      "---\n",
      " developed visualizations and dashboards using ggplot, tableau.\n",
      "---\n",
      " prepared and presented data quality report to stakeholders to give understanding of data.\n",
      "---\n",
      " environment: tableau 10.05, git, python 3.5.2 , anaconda-navigator, hadoop, spark, kafka, nosql, random forest, mongodb, hdfs, , nlp. python developer / data analyst symbiosys technologies - visakhapatnam, andhra pradesh, in may 2009 to june 2011 genius brands international is our client and we performed exploratory data analysis on corporate purchase orders, contracts and projects data using sampling and statistical methods. identified strata, improved precision and accuracy. works with other team members, including dba's, other etl developers, technical architects, qa, and business analysts & project managers.\n",
      "---\n",
      " participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshops/meetings with various business users.\n",
      "---\n",
      " used python to place data into json files for testing django websites.\n",
      "---\n",
      " updated and manipulated content and files by using python scripts. worked on python open stack api's.\n",
      "---\n",
      " used python scripts to update content in the database and manipulate files.generated python django forms to record data of online users.\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and customized visualization tools using python, r, hadoop and mongodb.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn in python for developing various machine learning algorithms.\n",
      "---\n",
      " worked on csv, json, excel different types of files for the data cleaning and data analysis.\n",
      "---\n",
      " used r for statistical operations on the data and ggplot2 for the visualizing the data.\n",
      "---\n",
      " application of various ml algorithms and statistical modeling like decision trees, regression models, random forest , svm, clustering to identify volume using scikit-learn package in python.\n",
      "---\n",
      " performed classification using supervised algorithms like logistic regression, decision trees, knn, naive bayes.\n",
      "---\n",
      " performed data profiling to merge the data from multiple data sources.\n",
      "---\n",
      " extracted data from hdfs (hadoop distributed file system) and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " performed time series analysis using tableau.\n",
      "---\n",
      " knowledge of other relational database platforms such as oracle, db2, nosql.\n",
      "---\n",
      " managed offshore projects and coordinated work for 24 hour productivity cycle.\n",
      "---\n",
      " environment: python 2.7, django 1.4, r, oracle, github, sql server, hdfs, hive. etl developer sutherland global services - hyderabad, telangana april 2007 to april 2009 sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics. sutherland has customers across industries like financial services to healthcare. my role is to assist analytics department for the data extraction and cleaning as a data preprocessing steps to build models.\n",
      "---\n",
      " involved with business analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications.\n",
      "---\n",
      " involved in data mapping specifications to create and execute detailed system test plans. the data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n",
      "---\n",
      " managed full sdlc processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model.\n",
      "---\n",
      " developed etls to pull data from various sources and transform it for reporting applications using pl/sql.\n",
      "---\n",
      " designed ssis packages to extract, transform and load existing data into sql server, used lots of components of ssis, such as pivot transformation, fuzzy lookup, merge, merge join, data conversion, row count, sort, derived columns, conditional split, execute sql task, data flow task and execute package task.\n",
      "---\n",
      " created ssis packages that involved dealing with different source formats (flat files, excel, xml) and different destination formats.\n",
      "---\n",
      " debugged and troubleshot the etl packages by using a breakpoint, analyzing the process, catching error information by sql command in ssis\n",
      "---\n",
      " developed sql queries in sql server management studio, toad and generated complex reports forth end users.\n",
      "---\n",
      " automated and scheduled recurring reporting processes using unix shell scripting and teradatautilities such as mload, bteq, and fast load\n",
      "---\n",
      " experience with perl.\n",
      "---\n",
      " performed data analysis and data profiling using complex sql on various sources systems includingoracle and teradata.\n",
      "---\n",
      " environment: etl tools, sdlc, github, sql server, pl/sql, excel, xml, sql.\n",
      "\n",
      "---\n",
      " analyze, clean, and interpret structured and unstructured data using python\n",
      "---\n",
      " prepare data visualization\n",
      "---\n",
      " build machine learning models\n",
      "---\n",
      " designed and built complex statistical models to predict scientific outcomes using r\n",
      "---\n",
      " analyzed and interpreted large datasets with thousands of features.\n",
      "---\n",
      " develops and deploys custom tailored recommendation software using multiple filtering and machine learning techniques to identify opportunity.\n",
      "---\n",
      " provides business case and roi recommendations.\n",
      "---\n",
      " used spark api over hortonworks hadoop yarn to perform analytics on data in hive\n",
      "---\n",
      " performed big data analytics using hadoop ecosystem, and spark ecosystem.\n",
      "---\n",
      " involved in start to end process of hadoop cluster setup which includes configuring and monitoring the hadoop cluster.\n",
      "---\n",
      " involved in processing large volumes of data in hadoop infrastructure\n",
      "---\n",
      " responsible for installation and configuration of hive, pig, hbase and sqoop on the hadoop cluster.\n",
      "---\n",
      " configured various property files like core-site.xml, hdfs-site.xml, mapred-site.xml based upon the job requirement.\n",
      "---\n",
      " involved in loading data from unix file system to hdfs.\n",
      "---\n",
      " involved in data management using sql database.\n",
      "---\n",
      " implemented hadoop stack and different big data analytic tools, migration from different databases to hadoop.\n",
      "---\n",
      " monitored multiple hadoop clusters environments\n",
      "---\n",
      " monitored workload, job performance and capacity planning.\n",
      "---\n",
      " recommending hardware configuration for hadoop cluster.\n",
      "---\n",
      " installing, upgrading and managing hadoop cluster on hortonworks distribution. data scientist state farm - bloomington, il february 2016 to may 2017 collaborate with business partners to understand their requirements, and design solutions that are aligned with business objectives and in compliance with the organization's architectural standards. conduct proof of concepts on new technologies and work closely with senior architects to develop recommendations that align with state farm's it strategy. perform extensive research and maintain current understanding of how technology can enhance and offer a range of solutions for business partners.\n",
      "---\n",
      " performed big data analytics using hadoop ecosystem, and spark ecosystem.\n",
      "---\n",
      " wrote hive udfs to query data and create dash boards.\n",
      "---\n",
      " used java to write mapreduce jobs to detect sensitive information in the test environment.\n",
      "---\n",
      " used sqoop to import and export data into hadoop cluster from/to rdbms which includes mysql and oracle databases.\n",
      "---\n",
      " improved data scanning processing and created an automated system for handling large volume data scans.\n",
      "---\n",
      " ? helped building an automated tool for the detective controls team in the test environment.\n",
      "---\n",
      " ? wrote a program in spark to minimize false positives that eventually saved significant amount of man hours and money in detecting sensitive information.\n",
      "---\n",
      " developed an application to automate the checkout process for enterprise claims system that resulted in cost avoidance of ~$50k+ per year to the team.\n",
      "---\n",
      " migrated legacy application code that was written in python. senior software developer digital staffing - chicago, il february 2015 to november 2015 created user information solutions by developing, implementing, and maintaining java-based components and interfaces. defined site objectives by analyzing user requirements, envisioning system features, and functionality. set expectations and features priorities throughout the development lifecycle, determining design methodologies and tool sets. completed programming using languages and software products. designed and conducted tests to ensure smooth workflow.\n",
      "---\n",
      " saved the company $500k+ by writing in-house web reports used by 400+ hospitals, including mayo clinic and northwestern hospital.\n",
      "---\n",
      " designed rest web services using java.\n",
      "---\n",
      " ? developed the software application to download data from crm systems and integrate into existing portals.\n",
      "---\n",
      " ? completed business gathering, design, development, and coding of the application.\n",
      "---\n",
      " ? managed the full sdlc project lifecycle for the agiloft crm data api.\n",
      "---\n",
      " implemented the persistence layer using hibernate-orm\n",
      "---\n",
      " worked on spring web flow on spring mvc for building flows in the web application.\n",
      "---\n",
      " helped in a new, centralized resource portal for documentation, including business gathering and java coded business rules.\n",
      "---\n",
      " assisted with changing servers and helped to create the new tomcat 7 server.\n",
      "---\n",
      " ? configured and tested the server for speed, performance, and settings.\n",
      "---\n",
      " mentored junior-level developers, assisting them with troubleshooting and understanding company/departmental standards.\n",
      "---\n",
      " deployed applications into a custom environment for testing before placing into production.\n",
      "---\n",
      " ? performed uat, qa, stress tests, and unit tests to ensure each application was working well.\n",
      "---\n",
      " utilized source control svn daily to track all codes used throughout organizational history.\n",
      "---\n",
      " tracked projects and identified issues using jira.\n",
      "---\n",
      " created an xml file with government specifications, converted hospital data files into xml files, and uploaded onto the government website cms.gov. senior python software engineer /data scientist acerugs, inc - bensenville, il january 2009 to february 2015 provided high quality solutions to address business needs within diverse technology environments. served as the senior python software engineer on an ecommerce website to lead end-to-end development testing and integration. developed enhancements to python for functionality and operation.\n",
      "---\n",
      " developed enhancements in python to improve functionality and operation, including a web shopping cart.\n",
      "---\n",
      " streamlined the intranet inventory control system and consistently worked on enhancements to the system to meet business requirements.\n",
      "---\n",
      " automated the intranet inventory control system to be utilized solely online.\n",
      "---\n",
      " significantly reduced employee time requirement by streamlining intranet inventory system\n",
      "---\n",
      " improved customer service delivery by allowing sales associates to interact more frequently with customers.\n",
      "---\n",
      " designed and developed innovative features for location-based inventory system to improve efficiency of order processing and reduce operational costs.\n",
      "---\n",
      " created a wholesale portal with easy checkout enhanced features targeting wholesale needs and improving user experience.\n",
      "---\n",
      " using r, python for the applications thru the algorithms of natural language processing (nlp), machine learning, and deep learning for the big dataset.\n",
      "---\n",
      " created dashboards and reports using tableau\n",
      "---\n",
      " build a recommender system\n",
      "---\n",
      " worked on azure machine learning studio \n",
      "---\n",
      " master of computer science in data science in computer science university of illinois urbana champaign - champaign, il bachelor of science in computer science in computer science northeastern illinois university - chicago, il skills cassandra, impala, mapreduce, hbase, kafka\n",
      "\n",
      "---\n",
      " over 7 plus years of experience with deep/machine learning architect, cognitive services, statistics, data analysis using r/ python and ms azure machine learning, google cloud platform and aws.\n",
      "---\n",
      " over 12 plus years in architectural design, development and implementation of business applications (web and client/server) and service oriented applications(soa) using .net framework, xml, and iis. \n",
      "---\n",
      " my upwork work history includes top clients(google) ranging from start-ups to fortune 500. many of the teams are distributed across continents, so i am comfortable working both locally and in international, distributed settings. currently i am a member of upwork talent enterprise clouds in which my clients regularly access my services for innovation and to architect new and profitable solutions. authorized to work in the us for any employer work experience data scientist / ml engineer and architecture upwork - mountain view, ca february 2015 to present \n",
      "---\n",
      " research and analysis of different ml algorithms for the google r & d machine learning team.\n",
      "---\n",
      " deployed production ml models using python and flask on google cloud platform with containers as in docker and kubernetes using machine learning libraries like scikit-learn, tensorflow, etc. \n",
      "---\n",
      " implementing ml and dl components and deploying on kubeflow pipelines using python. associate director of data scientist and analytics sapientrazorfish - san francisco, ca june 2013 to october 2018 a team member of cosmos analytical and developmental group(www.cosmos.ai) and assisted in designing and development of analytical and ml models for the end clients. coordinated with end users for designing and implementation of analytics solutions as per project proposals.\n",
      "---\n",
      " develop and train machine learning models using azure ml studio/azure data factory to analyse and identify fraudulent transactions for risk analysis team. \n",
      "---\n",
      " customized and developed cltv algorithms and models for predicting customer churns using nbd and gamma distribution using python and spark with databases like sql server, mongodb and hbase. deployment of production level r models to ms azure.\n",
      "---\n",
      " implemented data stitching models using deterministic and probabilistic models for unifying cosmos customers and created customer life time graph using neo4j.\n",
      "---\n",
      " implemented feature engineering on customer transactions to find more features using transitions and clickstream data to find more insights.\n",
      "---\n",
      " configuration and processing of etl social streaming data pipelines from data lake to azure databricks and used cognitive api services in notebooks.\n",
      "---\n",
      " created natural language processing model in r using opennlp, stanford nlp, ann to classify citi credit and legal risk documents attitude by their contents.\n",
      "---\n",
      " detect and classify tumour images using\n",
      "---\n",
      "deep learning algorithms (nn, ann, cnn with backend in keras, tensorflow) in python.\n",
      "---\n",
      " prepared graphs using ggplot library and tableau\n",
      "---\n",
      "for an overview of the analytical models and results.\n",
      "---\n",
      " worked on generating reports using neo4j graph database and classification of key words into categories using a neo4j graph database and make a recommendation.\n",
      "---\n",
      " generated ad hoc reports using python and apache spark from large datasets in hadoop clusters and created dashboards, reducing effort needed to manually review documents. data scientist pimco december 2011 to december 2012 \n",
      "---\n",
      " developed predictive models using vector machines, decision tree, random forest and na\\xefve bayes, and collaborating with back-office and middle office teams for production deployment with the analytics team.\n",
      "---\n",
      " designed and developed a web based system in asp.net / python to show the broker commissions,\n",
      "---\n",
      " sales and re-conciliation reports.\n",
      "---\n",
      " development of utilities//log applications in python with numpy, pandas library.\n",
      "---\n",
      " developed application in python/.net that notifies important information and changes on how\n",
      "---\n",
      " broker and voting decisions are updated.\n",
      "---\n",
      " environment: python, shiny. lead .net/ python developer new york school construction authority, new york august 2009 to june 2011 a team member of project tracking system(pts) application that manages ny school construction projects developed using silverlight 4.0 as client and oracle 10g as database.\n",
      "---\n",
      " used ado.net entity data model and wcf ria service, web service for communication with database using devart oracle provider and used linq queries extensively\n",
      "---\n",
      " used silverlight infragistics datagrid controls and expression blend for advanced client navigation and controls.\n",
      "---\n",
      " used stimulsoft silverlight reporting controls for generating adhoc silverlight reports and custom generated user reports.\n",
      "---\n",
      " environment: vs studio 2010, wcf ria services, wwf, ms team foundation server, oracle 11g, .net 4.0, windows 7. senior .net software engineer bloomberg - lp october 2008 to march 2009 a team member of r&d desktop services .net development team. bloomberg gateway 3.0 software manages and helps to configure bloomberg terminals installed in financial clients with the backend server.\n",
      "---\n",
      " developed gateway 3.0 using bloomberg smart client api's, wcf, c# and .net 3.5 libraries that helps to configure and monitor equities, shares, stocks data.\n",
      "---\n",
      " used ibm mq series api and wcf services to communicate between smart client gateway and the backend server for audit logging for trading transaction at the client location.\n",
      "---\n",
      " developed a javascript editor using .net 3.5, winforms and bloomberg controls.\n",
      "---\n",
      " used microsoft enterprise application logging block libraries for audit logging in gateway project and used ms tfs primarily for source code management.\n",
      "---\n",
      " xaml scripts were constructed dynamically in javascript editor and send to backend servers for executing xaml scripts with wwf activities.\n",
      "---\n",
      " environment: vs studio 2008, wcf, wwf, ms team foundation server, ms sharepoint, bloomberg controls, oracle 11g, .net 3.5 windows xp. senior .net software engineer northrop - new york, ny august 2007 to august 2008 a team member of oasis .net development team developing project for processing claims for the new york city government. this is a multi-tier architecture project with oracle 11 as the back end database.\n",
      "---\n",
      " oasis claim project also has the webservices exposed for other web-based applications and development includes writing several layers like data provider, webservices, controller, factory and framework.\n",
      "---\n",
      " used cruisecontrol.net integration for visual source safe for project build, management and reporting.\n",
      "---\n",
      " environment: vs studio 2005, wpf, n-unit testing framework 1.1, ms sharepoint, infragistics controls, oracle 11g, oracle sql developer 1.1, .net 3.5 windows xp. senior software engineer verizon wireless, new jersey april 2006 to july 2007 a team member of it customer care dept working for the project acss (automated customer care support service).acss software manages billing, activation, service features, equipment order features for verizon wireless handset customers. a 3-tier architecture product developed using c# winforms, ms vc++, mfc, atl come and rogue wave libraries, java. oracle 11g is used as the database.\n",
      "---\n",
      " responsible for development, migration and maintenance of vc++, mfc / atl com, com+ and c# .net winforms code for acss project which is a 3 tier application. software engineer united parcel service, new jersey june 2002 to april 2006 june 2002- april 2006)\n",
      "---\n",
      " ? a team member of the opsv(operation portfolio system verification) group in the ams/fdc(address management group and flexible data capture) project which validates, verifies the addresses on the packages that the customers send it for delivery and it also serves for excellent facility for searching addresses in the united states.\n",
      "---\n",
      " role: software engineer \n",
      "---\n",
      " m.s computer science in computer networks monmouth university 2004 to 2006 bachelor of engineering in electronics and communication m.k university skills .net (10+ years), apache (4 years), machine learning (5 years), python. (5 years), sql (10+ years), r (5 years), azure databricks (2 years), keras (3 years), azure ml studio (3 years), hadoop (2 years), python, big data, nlp (3 years), spark (2 years), data science (7 years), java (2 years), kubernetes (2 years), neo4j (2 years), docker (2 years) links https://www.upwork.com/fl/shankarmohanakrishnan https://www.linkedin.com/in/shankarmohan2/ https://github.com/shankarpm additional information technical skills:\n",
      "---\n",
      " languages r, apache spark, c#, java, pl/sql, matlab. vc++,\n",
      "---\n",
      " scripting python, unix shell scripting, perl.\n",
      "---\n",
      " development tools rstudio, tableau, visual studio.net 2015, eclipse.\n",
      "---\n",
      " data science/big data statistical analysis, machine learning, data mining, ms azure machine learning, ggplot2, tableau, hadoop 2.x, hdfs\n",
      "---\n",
      " nosql hbase, mongodb, neo4j\n",
      "---\n",
      " operating systems windows 10.0, unix with sun solaris 8.0, hp-unix.\n",
      "---\n",
      " databases mongodb, mysql, ms sql server 2008, oracle 11g, sybase,\n",
      "---\n",
      " web technologies ajax, asp.net, java script, (iis) 7.0.\n",
      "---\n",
      " others numpy, scipy, pandas, .net 4.5, wpf, wcf, xaml, linq, ssrs\n",
      "\n",
      "---\n",
      " quantitative, sta tis tical & da ta visualiz ation technique s to complex\n",
      "---\n",
      " business problems. expe rie nced in solv ing inve stme nt management,\n",
      "---\n",
      " automo tive, e-commerce and enginee ring challe nges. open to contract &\n",
      "---\n",
      " permanent position s. work experience contract business analyst/data scientist sempra energy - los angeles, ca march 2019 to present interacted closely with sempra management to understand the business reporting\n",
      "---\n",
      " requirements of the utility's residential and commercial rebate programs.\n",
      "---\n",
      " designed complex reporting using ssrs (ms report builder, visual studio, sql management\n",
      "---\n",
      " studio) and sas.\n",
      "---\n",
      " developed custom dashboards using advanced ms excel (v-lookups, regression & pivot\n",
      "---\n",
      " tables).\n",
      "---\n",
      " wrote complex queries in sql.\n",
      "---\n",
      " created and maintained data visualizations in tableau. contract business analyst/data scientist ( python, sas, ssrs, sql & tableau) pacific investment management (pimco) - newport beach, ca june 2018 to october 2018 newport beach, ca\n",
      "---\n",
      " june 2018 to october 2018\n",
      "---\n",
      " designed pimco's next generation of book yield attribution models (prism 2.0 - pay\n",
      "---\n",
      " reinvestment securities monitor).\n",
      "---\n",
      " o prism is used by pimco account managers to demonstrate the impact of active\n",
      "---\n",
      " book yield management for insurance clients.\n",
      "---\n",
      " interpreted existing model (prism 1.0) which was written in 60+ pages of dense sql.\n",
      "---\n",
      " produced prototype of prism 2.0 in python.\n",
      "---\n",
      " interacted closely with development team using python under the agile methodology.\n",
      "---\n",
      " o interpreted existing complex models developed in sql and ssrs\n",
      "---\n",
      " the prism 2.0 prototype was 2x faster than prism 1.0 and delivered significantly higher\n",
      "---\n",
      " accuracy (lower residual values).\n",
      "---\n",
      " fielded user queries regarding first generation prism model. troubleshooted application.\n",
      "---\n",
      " designed and executed complex reporting using sas and ssrs.\n",
      "---\n",
      " built and maintained data visualizations using tableau. contract web developer (c# & full stack .net) ruben postaer & associates - santa monica, ca february 2018 to april 2018 worked as a full stack .net developer on a programming team collaborating to bring honda\n",
      "---\n",
      " motors next generation consumer website online.\n",
      "---\n",
      " o next generation website was faster, more-stable and aesthetically appealing.\n",
      "---\n",
      " developed and implemented user sign-up and validation protocol for honda racing's\n",
      "---\n",
      " ecommerce website.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " built rpa's internal employee \n",
      "---\n",
      " website\n",
      "---\n",
      " o .net, c#, ajax, java, javascript, html, css, sql and razor senior engineer/lead engineer (linux & windows) koam engineering systems - san diego, ca july 2017 to october 2017 responsible for a team of electrical and mechanical engineers developing the u.s. navy's next\n",
      "---\n",
      " generation command and control processors.\n",
      "---\n",
      " o next generation command and control processor was physically and electronically\n",
      "---\n",
      " more robust and was based on current generation hardware and software (linux).\n",
      "---\n",
      " system supports fleet anti-ballistic missile defense and threat management systems.\n",
      "---\n",
      " o cdlms, tadil, smtadil, ngc2p, link-11, link-16, link-22, jtids, solidworks, linux, windows, vme bus, tdma data scientist speedwerks precision llc - arcadia, ca january 2014 to march 2017 analyzed large oracle databases (>1 terabyte) of late-model porsche engine-management\n",
      "---\n",
      " data using sas, sql, python & r.\n",
      "---\n",
      " o developed sophisticated statistical models including least squares (linear and non- linear), principal components and machine learning (tensorflow).\n",
      "---\n",
      " o models rank-ordered key engine performance drivers and identified engine\n",
      "---\n",
      " management system improvements (including fuel efficiency, power and torque).\n",
      "---\n",
      " produced custom reports/dashboards using ssrs and data visualization software (tableau).\n",
      "---\n",
      " utilized advanced ms excel tools including optimization, regression, v-lookups and pivot\n",
      "---\n",
      " tables. restoration shop volunteer san diego air & space museum - san diego, ca january 2012 to december 2013 oversaw the restoration of historic aircraft for static display.\n",
      "---\n",
      " o major project: ground up static reconstruction of a vought f4u corsair senior research analyst (advanced excel, s & s-plus) s & s-plus - scottsdale, az august 2010 to december 2011 contributed to the development of the firm's proprietary asset allocation models based on equal risk contributions from major asset classes (equities, fixed income, commodities and\n",
      "---\n",
      " currencies).\n",
      "---\n",
      " o developed proprietary statistical models using s and s-plus.\n",
      "---\n",
      " o models improved investment performance as measured by increased relative\n",
      "---\n",
      " investment returns, reduced risk and improved sharpe ratios\n",
      "---\n",
      " provided research and analysis supporting investment strategies for koch's $7 billion defined\n",
      "---\n",
      " benefit pension plan using advanced quantitative and statistical techniques. senior vice president (c, visual basic & advanced excel) the capital group - los angeles, ca june 1996 to november 2008 created detailed quantitative and economic research on a broad range of investments.\n",
      "---\n",
      " o specialized in mortgage-backed, asset-backed and derivative securities.\n",
      "---\n",
      " managed $8 billion of fixed income assets for institutional and mutual fund clients. provided\n",
      "---\n",
      " investment recommendations for taxable fixed income portfolios totaling more than $150\n",
      "---\n",
      " billion.\n",
      "---\n",
      " coordinated risk management systems development.\n",
      "---\n",
      " o oversaw firm's implementation of futures, interest rate and credit default swaps.\n",
      "---\n",
      " o programmed cash flow models for complex securities using c, visual basic and intex.\n",
      "---\n",
      " managed cbo and cdo portfolios backed by high-yield corporate debt, abs, mbs and cmbs.\n",
      "---\n",
      " o developed tools for portfolio optimization using c and visual basic.\n",
      "---\n",
      " o optimization tools lead to cbo equity irrs in excess of 20%.\n",
      "---\n",
      " participated in firm wide asset allocation process (fixed income/equities, domestic/international/em).\n",
      "---\n",
      " consistently outperformed industry competitors in enhanced cash management.\n",
      "---\n",
      " served as chairman of the institutional fixed income management committee.\n",
      "---\n",
      " reported to cg management committee on fixed income business unit. senior associate, consultant (rats & visual basic) goldman sachs asset management - new york, ny june 1995 to march 1996 designed and programmed statistical trading models used to make absolute and relative\n",
      "---\n",
      " value decisions on adjustable rate mortgages (arms).\n",
      "---\n",
      " o programmed extensively using rats and visual basic.\n",
      "---\n",
      " models identified over and undervalued segments of the gnma arm market which increased\n",
      "---\n",
      " portfolio returns for equal risk (as measured by duration and convexity)\n",
      "---\n",
      " worked closely with dr. cliff asness (co-founder aqr hedge fund). experienced senior consultant (advanced excel, c++) arthur andersen & co., s.c - los angeles, ca january 1994 to september 1994 \n",
      "---\n",
      " designed and marketed technological services for large business litigation.\n",
      "---\n",
      " developed proprietary software applications using c++.\n",
      "---\n",
      " responsible for sybase database construction and administration.\n",
      "---\n",
      " provided in-court management and integration of large text and document databases.\n",
      "---\n",
      " prepared project proposals and made presentations to clients. senior research associate (advanced excel) economic analysis corporation - los angeles, ca february 1993 to january 1994 senior analyst, systems manager (sas) law & economics consulting group - berkeley, ca september 1989 to september 1992 work authorization\n",
      "---\n",
      " citizen of united states & canada \n",
      "---\n",
      " master of business administration in analytical finance & statistics university of chicago june 1996 master of science in electrical engineering ucla june 1994 bachelor of arts in mathematical economics uc berkeley december 1989\n",
      "\n",
      "---\n",
      " work in global risk analytics team as part of a system migration project to move ccar credit risk machine learning modelling programs and files from a sas coding environment to a python coding environment.\n",
      "---\n",
      " build and analyze sas and python programs as well as data lineage flow charts and statistical model documentation in an effort to define and document which databases are being utilized for which ccar credit risk models.\n",
      "---\n",
      " usage of sas, python, tableau, microsoft excel macros, pivot tables, vlookups, and hlookups,vertica, and hadoop hive sql. senior sas statistical credit risk modeller / digital media data scientist / project consultant td bank 2018 to 2018 \n",
      "---\n",
      " build logistic regression statistical machine learning models analyzing customer checking account attrition data.\n",
      "---\n",
      " analyze digital media marketing data.\n",
      "---\n",
      " strong usage of sas macros, sas base, sas stat, proc logistic, proc reg, proc anova, proc varclus, proc sql, sas enterprise guide, sas enterprise miner, tableau, microsoft excel macros, pivot tables, vlookups, and hlookups. senior sas project consultant wyndham worldwide - rci 2017 to 2018 \n",
      "---\n",
      " build and ftp marketing list audience files to company list management vendors using sas programming software.\n",
      "---\n",
      " prepare marketing analysis adhoc reports using sas base and sas macros.\n",
      "---\n",
      " document all processes.\n",
      "---\n",
      " strong usage of proc sql, sas enterprise guide, excel macros, pivot tables, vlookups,hlookups. senior sas quantitative data analyst / software developer / project consultant sprint corporation 2017 to 2017 \n",
      "---\n",
      " extract, transform and load (etl) millions of records of sprint assurance wireless data using sql and sas software.\n",
      "---\n",
      " analyze and track customer subscriber-based churn using sas base, sas macros, sql server, teradata, and oracle software.\n",
      "---\n",
      " usage of microsoft excel macros, pivot tables, vlookups, and hlookups. senior sas data science and machine learning project consultant knowledgent group 2017 to 2017 \n",
      "---\n",
      " use r, python, sas v9.4, matlab, tableau, sas visual analytics, sql, ms excel pivot tables and spotfire to analyze pharmaceutical real world evidence (rwe) medical outcome patient data.\n",
      "---\n",
      " utilize statistical modelling machine learning methodologies to maximize revenues for financial industry investment bank clients. senior sas data scientist / machine learning credit risk statistical modelling project consultant bank of new york mellon 2015 to 2016 develop sas statistical programs to build hundreds of econometric forecasting models and simulations for corporate treasury's investment bank portfolio. products include abs, mbs, cmbs, rmbs, munis and international. econometric time series models predict potential future\n",
      "---\n",
      " business losses in response to the federal reserve's dodd-frank ccar initiative for investment portfolio stress test modelling. heavy usage of moody's analytics data.\n",
      "---\n",
      " statistical model predictions and in-sample back-testing model validations include various financial credit risk and compliance stress-testing scenarios given by the federal reserve. these scenarios include baseline bank holding company market forecasts, adverse scenario market forecasts, and severely adverse scenario market forecasts.\n",
      "---\n",
      " sas v9.4, sas enterprise guide, sas base, sas macros, sas stat, sas ets and sas graph.\n",
      "---\n",
      " heavy usage of sas proc sql, ms excel: hlookups, vlookups, and pivot tables, ms excel vba.\n",
      "---\n",
      " lgd model documentation editor. usage of black rock aladdin portfolio management system. senior sas data scientist / machine learning marketing data analyst project consultant predictive modelling of device technology data 2015 to 2015 \n",
      "---\n",
      " use hadoop hive sql to extract and summarize device performance transactional data for predictive modelling of device technology data. develop predictive modelling machine learning equations in spss modeler. usage of tableau, visio, and microsoft excel for developing graphs, data presentation, and building report requirements.\n",
      "---\n",
      " develop graphs and charts and present to top management regarding human resource market trends. usage of oracle 11g pl/sql, toad, cognos, and endeca for report production.\n",
      "---\n",
      " use oracle 11g sql developer data miner to develop predictive attrition models and scoring equations for human resource data warehouse variable reduction and scoring.\n",
      "---\n",
      " usage of regression analysis and decision tree modelling, cluster analysis and factor analysis.\n",
      "---\n",
      " use sas enterprise guide, sas ebi, putty, korn shell on unix solaris to develop analyses and stored procedures for data and text mining,customer segmentation, and predictive churn models.\n",
      "---\n",
      " usage of ftp, sql for the creation of adhoc and dashboard reports from teradata database.\n",
      "---\n",
      " use etl to integrate millions of records from numerous databases into one main database.\n",
      "---\n",
      " use sas proc mi to develop algorithms to impute missing values for company precision project.\n",
      "---\n",
      " develop customer and prospect profiles using company cell phone data and acxiom infobase personicx cluster data. present all findings to retail clients for e-commerce direct marketing.\n",
      "---\n",
      " use sas v9.3, factor/cluster analysis, logistic/linear regression, tree models, sas enterprise miner, spss modeler, sql, r, hadoop, alteryx, ms excel pivot tables, ms access, ultra edit.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes. verizon / verizon wireless 2013 to 2014 senior sas reports developer and quantitative digital marketing data analyst project consultant jp morgan chase 2012 to 2012 \n",
      "---\n",
      " use sas enterprise guide, unix, sas ebi, sas data integration studio and tableau to develop reports, analyses, and stored procedures for jpmc financial retail/credit card digital direct marketing/e-commerce products. fulfill business requirement documentation (brds).\n",
      "---\n",
      " use etl to integrate millions of records from numerous databases into one main database.\n",
      "---\n",
      " interface with sharepoint, web portal, clear case change control software, ms excel pivot tables.\n",
      "---\n",
      " usage of ftp, ultra edit, putty, toad, db2, pl/sql,oracle, sql server and teradata.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes.\n",
      "---\n",
      " present all findings to jpm chase financial/credit card digital marketing/e-commerce team. senior sas machine learning data scientist / mobility data analyst / software developer at&t labs 2011 to 2012 \n",
      "---\n",
      " build predictive logistic regression, tree and neural network b-to-b direct mail churn models using enterprise mobility network data, corporate revenue data, and market research data.\n",
      "---\n",
      " present all findings to client. model lift gains charts consistently exceeded expectations.\n",
      "---\n",
      " use etl to integrate millions of records from numerous databases into one main database.\n",
      "---\n",
      " use sas, unix, putty, korn shell scripting, pl/sql,oracle, teradata, sql, sas enterprise miner, tibco spotfire miner, ftp, r, s+, python, awk, excel pivot tables, powerpoint, irfanview.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes. tax technology project consultant verizon / verizon wireless 2010 to 2011 \n",
      "---\n",
      " utilize cobol and sas programming for corporate tax technology department data warehouse extraction, transformation and loading (etl) of tax audit data.\n",
      "---\n",
      " convert millions of records of business and residential point of sale tax register data from mainframe files to ms sql server 2005/2008 database for tax audit purposes.\n",
      "---\n",
      " conduct system/unit testing. build excel vba macros to automate tax register data retrieval.\n",
      "---\n",
      " heavy usage of cobol, sas base, sas macros, jcl/mvs /tso/ispf, db2, sql, syncsort, ndm, ftp, ms excel, ms access, sharepoint.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes. database marketing project consultant dun & bradstreet 2009 to 2009 \n",
      "---\n",
      " develop sales/marketing direct mail crm campaign lists for b-to-b sales and e-commerce.\n",
      "---\n",
      " integrate millions of records from numerous databases into one comprehensive database.\n",
      "---\n",
      " segment marketing and sales contacts by sic codes and transaction-level point of sale data.\n",
      "---\n",
      " build ms excel macros for automation of management dashboards and adhoc reports.\n",
      "---\n",
      " extract data from corporate database. strong usage of sql server\n",
      "---\n",
      " odbc\n",
      "---\n",
      " ms access: sql, macros, queries\n",
      "---\n",
      " ms excel: macros, visual basic, vb scripting, vba, pivot tables, vlookups.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes. senior sas data scientist / machine learning statistical modeller and software developer novartis pharmaceuticals corporation 2005 to 2008 compile and analyze ims xponent prescription and sample data, medco prescription data, ims physician group practice data, sales force call data, ims plantrak pbm payer-based health\n",
      "---\n",
      " claims data, ims ddd outlet data, mckesson voucher data, impactrx speaker data, lash\n",
      "---\n",
      " specialty pharmacy transactional point of sale data\n",
      "---\n",
      " medicare part d data, dendrite, sdi, ndc, and genentech time series longitudinal patient data.\n",
      "---\n",
      " analyze market research and webmd data and present best target physicians to field sales for sales and e-commerce promotions. develop targeted direct mail and e-commerce contact lists.\n",
      "---\n",
      " utilize data mining and statistical multivariate modeling mix methodologies to evaluate company roi and promotion response for various web/e-commerce and speaker direct marketing crm campaigns targeted to specialized physicians. segment data using cluster analysis as required.\n",
      "---\n",
      " build scorecards and increase market share in the oncology, hematology, respiratory, dermatology, cardiovascular, metabolic and neuroscience therapeutic areas.\n",
      "---\n",
      " build test/control samples (a/b tests), design and build databases using patient-level transactional point of sale data, physician-level prescription data and outlet-level data.\n",
      "---\n",
      " conduct and present multivariate statistical analyses and market segmentation/optimization studies to support product marketing/sales. design sales force sizing/structuring analyses.\n",
      "---\n",
      " develop competitive intelligence market share, pricing, and forecasting models.\n",
      "---\n",
      " track patient compliance, persistence, adherence, and dosing levels. usage of icd-9 codes.\n",
      "---\n",
      " build ms excel macros to automate management dashboards and adhoc powerpoint reports.\n",
      "---\n",
      " develop, document, and train staff on computer programming algorithms and business processes.\n",
      "---\n",
      " unix, sas v9 base: sql, ods, dde, freq, means, univariate\n",
      "---\n",
      " sas stat: ttest, glm, anova, ancova, cluster analysis\n",
      "---\n",
      " sas macros, sas enterprise miner, sas connect, ms excel: macros, visual basic,vb scripting, vba, pivot tables, vlookups\n",
      "---\n",
      " ms access, powerpoint, microstrategy.\n",
      "---\n",
      " received an \"above and beyond\" recognition award for developing a new innovative methodology to create a more unbiased control group selection process for physician sampling. sas developer project consultant morgan stanley corporation 2005 to 2005 \n",
      "---\n",
      " used sas programming for company data warehouse extraction, transformation, loading (etl).\n",
      "---\n",
      " developed business processes and stored procedures.\n",
      "---\n",
      " tested algorithms to track market surveillance and support audit regulations for aml brokerage point of sale transactions. conducted system/unit testing.\n",
      "---\n",
      " usage of sas enterprise guide, sas base,sas macros,unix,udb/db2/sql,oracle, pl/sql. statistical sas programming and analysis / quantitative database marketing project consultant bookspan 2005 to 2005 \n",
      "---\n",
      " developed and presented corporate dtc high credit risk screening models & customer profiles.\n",
      "---\n",
      " credit risk models increased profits via more selective database/e-commerce marketing.\n",
      "---\n",
      " utilized sql, eda, linear and logistic discrete choice regression modeling mining in-house transaction-level point of sale credit data on ibm mainframe.\n",
      "---\n",
      " built ms excel macros to automate graphing/adhoc reporting.\n",
      "---\n",
      " sas base, sas stat, sas macros\n",
      "---\n",
      " jcl/mvs/tso/ispf sas data scientist and machine learning database marketing roi consultant westhill marketing sciences 2003 to 2004 \n",
      "---\n",
      " designed and implemented analytic and statistical multivariate modeling solutions for market research crm brand panel studies for the pharmaceutical and retail consumer goods industries.\n",
      "---\n",
      " quantified advertising dollars and web/e-commerce sales relationship using time series data. estimated roi for retail consumer goods chain (circuit city). presented all findings to client.\n",
      "---\n",
      " analyzed market research survey data to increase client market share by quantifying relationship between brand awareness and brand consideration to product purchase intent for business to business pharmaceutical (cardinal health) customers. presented all findings to client.\n",
      "---\n",
      " statistical modeling methodologies included estimating polynomial distributed lag models, ad stock models, and other non-linear regression models. strong usage of cohort data.\n",
      "---\n",
      " built ms excel macros to automate graphing/adhoc reporting.\n",
      "---\n",
      " utilized pc sas, sas base (proc freq, proc means, proc univariate), sas macros, and sas stat (proc anova, proc pls, proc reg) to automate analyses, for adhoc time series reporting, for data mining, and for statistical modeling. segmented data by media codes, as required.\n",
      "---\n",
      " spss, ms excel macros, visual basic, vb scripting, vba, vlookups\n",
      "---\n",
      " powerpoint, curve expert. vice president, project manager - marketing chase home finance 2003 to 2004 \n",
      "---\n",
      " gathered requirements and provided analytic support to top management via the publication and presentation of monthly business mis audit reports with analytic highlights on payoffs, recaptures, streamline, and home equity utilization for mortgage and home equity products.\n",
      "---\n",
      " tracked, monitored, and re-priced home equity line of credit revenue at the channel level, analyzing both customer level and transaction level point of sale data (50 mm+ records).\n",
      "---\n",
      " built and presented monthly audit reports to the mortgage retail finance department to track trailer commissions at the loan officer level for chase refinance customers.\n",
      "---\n",
      " responsible for the programming of an automated report process for loan officers to track their accounts via the usage of actuate, business objects and cognos. analyzed call center market research survey data to optimize call center performance.\n",
      "---\n",
      " built ms excel macros for automation of management dashboards and adhoc powerpoint reports.\n",
      "---\n",
      " gathered requirements and managed mortgage recapture data mining study of refi vs. purchase behavior of ex-chase home finance customers working closely with outside data vendors.\n",
      "---\n",
      " developed and presented marketing profiles of mortgage union customers to direct mail crm campaign managers for dtc target marketing purposes.\n",
      "---\n",
      " managed the analysis of mortgage/home equity share accounts and the building of a home equity line of credit utilization forecasting model. produced and presented in-depth analyses of mortgage recaptures to campaign managers using infobase and microvision data (similar to claritas prizm clusters) to segment chase mortgage customers by financial behavior categories.\n",
      "---\n",
      " developed scorecards and segmented new chase home equity customers by spp score. evaluated spp scoring model efficacy for home equity direct to consumer mailings.\n",
      "---\n",
      " gathered requirements, developed, and managed file scoring business process of internal and outside vendor discriminant analysis scoring models. discriminant analysis models consisted of a set of equations where each equation identified a financial customer type (similar to claritas prizm clusters). segmentation models increased response rates to dtc campaigns.\n",
      "---\n",
      " unix shell scripting, oracle,pl/sql, udb/db2/sql, sas base: sql, sas stat, sas macros\n",
      "---\n",
      "chaid, excel macros,vba,pivot tables,vlookups,access,powerpoint, ms project, visio.\n",
      "---\n",
      " hands-on project manager with a staff of three sas analyst/programmers. director of programming tci 2000 to 2003 \n",
      "---\n",
      " gathered requirements, designed, and deployed customer specific programming systems, business processes, and etl to support company operations in the implementation of bad pay/high credit risk scoring solutions for direct marketing/e-commerce crm clientele.\n",
      "---\n",
      " gathered requirements, developed and maintained company real-time automated client-customized e-commerce dtc credit risk order screening and list screening scoring programs.\n",
      "---\n",
      " gathered requirements and automated customized e-commerce order and list credit risk scoring solutions for direct marketing clients using perl, java, javascript , xml, html, ftp, pgp encryption, unix shell scripting, cron, autosys, python, mysql, php, and sas on linux.\n",
      "---\n",
      " wrote object-oriented python code and apis to trigger alarm once the transmitted file arrived at the company website via ftp, de-crypted the encrypted data, cleansed and address standardized the data, loaded the cleansed data from a flat file into mysql, overlaid the file\n",
      "---\n",
      " with credit risk screening data, scored the file, re-encrypted the file and transmitted the scored file via ftp to the client tcp/ip e-commerceweb address in real time.\n",
      "---\n",
      " designed and produced all in-house and third party databases including infousa us hh database - 90 mm+ records and axciom infobase data (similar to claritas prizm clusters).\n",
      "---\n",
      " loaded, audited, cleansed, built data models, and standardized raw data to ensure data quality/integrity throughout data warehouse. re-coded database production from mainframe\n",
      "---\n",
      " platform to red hat linux using mysql/ python. implemented address standardization software.\n",
      "---\n",
      " designed and enhanced pc-based file tracking systems using c and c++. developed guis using php and mysql. automated vse mainframe jobs using easytrieve, syncsort, group1, cobol, jcl/mvs/tso/ispf\n",
      "---\n",
      " sas base: sql, ods\n",
      "---\n",
      " sas stat: proc reg, proc logistic\n",
      "---\n",
      " sas macros.\n",
      "---\n",
      " as director of programming additional responsibilities included gathering requirements, hiring, hands-on training, and staff supervision. hands-on project manager and systems developer. assistant vice president, credit risk forecasting analyst citibank 2000 to 2000 \n",
      "---\n",
      " developed predictive machine learning regression model scorecards using time series credit data for delinquency, contractual and accelerated write-offs, and bankruptcy customer segments.\n",
      "---\n",
      " forecasted greatest credit risk in credit card portfolio using transaction-level point of sale data.\n",
      "---\n",
      " presented and reconciled credit risk forecasts with collections and audit departments.\n",
      "---\n",
      " built ms excel macros for automation of forecast reporting. sas base, sas stat, sas macros, unix, shell scripting, udb/db2/sql, vb scripting, vba, pivot tables, vlookups. database marketing manager unz & company 1997 to 1999 \n",
      "---\n",
      " managed company contact database for $4mm import/export/hazmat government compliance firm specializing in business to business crm direct marketing/e-marketing.\n",
      "---\n",
      " segmented marketing database using in-house transaction-level point of sale data as well as dun & bradstreet sic data for data mining/profiling of customer database. customer profiles led to more profitable list selects and more profitable marketing and sales account management.\n",
      "---\n",
      " analyzed marketing campaign results to improve direct marketing response rates.\n",
      "---\n",
      " managed list brokers to identify most lucrative b-to-b list opportunities for product marketing.\n",
      "---\n",
      " implemented market research surveys to expand company knowledge of customer base and customer supply chain/logistics. implemented third party vendor target marketing software.\n",
      "---\n",
      " member of company senior management team. developed and presented marketing analyses and adhoc reports to top management. built ms excel macros for database reporting.\n",
      "---\n",
      " usage of sql, sql server,ms access, odbc, foxpro, excel macros, visual basic, vb scripting, vba, vlookups, crystal reports, stored procedures, onyx, powerpoint. and mapinfo software. senior sas programming marketing project consultant chase manhattan bank 1997 to 1997 \n",
      "---\n",
      " integrated multiple company files from various data sources as part of post chase-chemical merger effort to consolidate home equity line/loan data into a new fully integrated database.\n",
      "---\n",
      " developed business processes and automated customized adhoc reports for marketing managers.\n",
      "---\n",
      " sas base , sas macros, syncsort, jcl/mvs/tso/ispf: ibm mainframe environment. senior marketing data scientist / machine learning sas software developer time warner inc 1991 to 1997 gathered requirements, developed, validated, maintained, enhanced, and presented market\n",
      "---\n",
      " segmentation promotion response crm customer retention and customer acquisition logistic\n",
      "---\n",
      " regression discrete choice models, chaid, neural network and discriminant analysis database\n",
      "---\n",
      " scoring models. utilized the in-house mainframe-based corporate database, us census-based\n",
      "---\n",
      " demographic data, claritas prizm cluster data and neilsen ratings data.\n",
      "---\n",
      " developed customer profiles and presented them to product managers to improve direct marketing campaign promotion response rates. worked with product managers to develop test/control market research methodologies (a/b tests) and sample sizing. corporate multivariate models\n",
      "---\n",
      " consistently increased direct marketing response rates for publishing (magazine, book, catalog) as well as entertainment products (music, video) for b-to-c and b-to-b direct marketing.\n",
      "---\n",
      " gathered requirements and created new us census-based variables for statistical modeling using factor/cluster analysis on ibm mainframe data. segmented data by media codes, as required.\n",
      "---\n",
      " developed data mining processes to improve modeling methodologies and data quality.\n",
      "---\n",
      " set up and updated department geocoding software. translated cobol record layouts to sas.\n",
      "---\n",
      " used sas base (proc tabulate, proc freq, proc means, proc univariate), sas stat (proc reg/ proc logistic/proc factor/proc discrim/proc cluster),sas macros, jcl/mvs/tso/ispf -mainframe.\n",
      "---\n",
      " utilized syncsort and easytrieve software to read in and process files with 90 million plus records. used si-chaid and cart software for exploratory data research.\n",
      "---\n",
      " as senior marketing analyst, additional responsibilities included gathering requirements, hiring, hands-on training and staff supervision. hands-on project manager and systems developer. market analyst american express trs company inc 1990 to 1990 \n",
      "---\n",
      " developed and presented direct marketing crm segmentation models and customer profiles.\n",
      "---\n",
      " utilized discrete choice logistic regression and factor analytic scoring models to raise direct marketing crm campaign promotion response rates for financial, travel, & publishing products.\n",
      "---\n",
      " utilized sas, jcl/mvs/tso/ispf on an ibm mainframe to extract demographic and company product data for data mining and to build predictive segmentation models.\n",
      "---\n",
      " sas base, sas stat (proc reg/proc logistic/proc factor/proc cluster), sas macros. at&t 1987 to 1990 statistician / data scientist consultant macro systems, inc 1986 to 1987 \n",
      "---\n",
      " designed and implemented quality control models, statistical regression machine learning models, and market research survey analysis designed to monitor and improve pell grant - u.s. department of \n",
      "---\n",
      " unemployment insurance - u.s. department of labor\n",
      "---\n",
      " child support - state of massachusetts welfare dept\n",
      "---\n",
      " and rural hospital programs evaluation -u.s. dept of health.\n",
      "---\n",
      " heavy usage of sas base, sas stat (proc anova, proc reg), sas macros on a vm/cms mainframe. used spss software to track and analyze market research survey results.\n",
      "---\n",
      " utilized dbase to build a database for the analysis of massachusetts state child support data.\n",
      "---\n",
      " co-authored and presented final report to the u.s. department of labor on the effectiveness of various state unemployment insurance programs. international economist u.s. department of commerce 1985 to 1986 prepared and presented policy papers for internal government use as part of a government/private sector team as a member of the international trade administration.\n",
      "---\n",
      " research focused on u.s. industry market access concerns in the european telecommunications, biotechnology, manufacturing, and agricultural market sectors. sas marketing data scientist and predictive machine learning modelling forecaster 1984 to 1985 \n",
      "---\n",
      " developed, maintained, and enhanced vm/cms mainframe-based econometric/statistical time series demand forecasting models to predict company revenue and volume market share.\n",
      "---\n",
      " usage of wharton econometric data and in-house market research survey data analysis with sas base, sas stat (proc anova), sas macros, sas ets (proc autoreg, proc arima), sas iml, s+ and smalltalk for data mining and to build econometric and time series statistical regression models for forecasting company market share and for developing adhoc reports.\n",
      "---\n",
      " usage of ems software to build econometric demand forecasting models and for price elasticity estimation. presented quarterly forecasts to product management.\n",
      "---\n",
      " developed pricing and competitive intelligence analyses for tariff filings and strategic planning. \n",
      "---\n",
      " master of science in agricultural economics and marketing rutgers university graduate school bachelor of arts in economics rutgers university - douglass college skills pivot tables, sas, unix, cognos, ms project, microstrategy, olap, spotfire, tableau, teradata, db2, sql server, mysql, oracle, pl/sql, sql, stored procedures, udb, vsam, cms\n",
      "\n",
      "---\n",
      " over 12 years of experience as data scientist / python developer and data analyst with technical prowess\n",
      "---\n",
      " worked on projects which involved deep learning, machine learning algorithms, natural language processing, statistical modeling, data transformation, performed sentiment analytics and handled large datasets\n",
      "---\n",
      " 4+ years experience with performing data analysis with compiling, analyzing, validating, modeling data sets and developing machine learning models including neural network models for solving the business problems\n",
      "---\n",
      " 3+ years' experience with hadoop stack, hdfs, map reduce, pig, hive, hbase, strom, apache spark and scala\n",
      "---\n",
      " 3+ years experience on building and maintaining sql scripts, indexes, and complex queries for data analysis, extraction, provided data expertise for ad-hoc support and attribute verification\n",
      "---\n",
      " worked with varieties of relational databases ( rdbms ) like sqlite, mysql, postgresql and nosql dbs\n",
      "---\n",
      " performed predictive analytics with python to predict the defaults for us mortgage loans and indian personal loans\n",
      "---\n",
      " utilized machine learning models like knn, k-means, decision trees, na\\xefve bayes, regression, xgboost, svm, random forest for estimation of parameters to predict stock movement and default on loans\n",
      "---\n",
      " experience in natural language processing including web scraping, text wrangling, parsing and sentiment analysis with application to predicting the price movement of a stock from the real time news data\n",
      "---\n",
      " performed large scale data analysis and developed statistical models for regressions, classification, clustering and time series and conducted hypothesis testing with tests like anova, t-test, f-test\n",
      "---\n",
      " hands on working knowledge of linux operating system, unix, windows os, aws and google cloud platform for machine learning applications to create and manage databases on cloud platform and analyze data sets\n",
      "---\n",
      " worked on python libraries like numpy, sklearn, matplotlib, pandas, beautifulsoup, datareader, statsmodel\n",
      "---\n",
      " utilized tensorflow and keras for implementing deep learning models like lstm, rnn to create chat bot systems\n",
      "---\n",
      " involved in various phases of software development life cycle (sdlc) such as requirements gathering, modeling, analysis, design and development with experience in agile methodologies and scrum process\n",
      "---\n",
      " involved in the process of creating use-case diagrams, activity flow diagrams, class diagrams and object diagrams in the design phase and developed the coding module\n",
      "---\n",
      " experienced in creating reports, presentations, documents, dashboards and visualizations using tableau and rshiny and presented it to senior management for review and decision making\n",
      "---\n",
      " solid knowledge of finance, risk, data and business analytics and performed as team leader for numerous projects\n",
      "---\n",
      " managed the credit risk for the bank by developing machine learning model to estimate pd and lgd\n",
      "---\n",
      " strong client facing skills- able to interact with high net worth clients and deepen relationship with them\n",
      "---\n",
      " highly motivated to discover and learn new analytical and software tools to improve the quality of work\n",
      "---\n",
      " ability to work in team environment and managed deliverables within the context of a larger projects authorized to work in the us for any employer work experience data scientist / python developer amus inc - new york, ny june 2018 to present responsibilities\n",
      "---\n",
      " scraped and cleaned tax faqs from multiple sources in python using beautifulsoup and inserted into a sql database\n",
      "---\n",
      " implemented google analytics, created dashboards, analyzed the collected data to understand the user engagement\n",
      "---\n",
      " created interactive dashboards using tableau to visualize the efficiency of the algorithm, and quarterly usage of the product\n",
      "---\n",
      " developed an alexa skill to integrate the chatbot with alexa and created lamda function using aws toolkit to process the .json data\n",
      "---\n",
      " implemented deep learning lstm using tensorflow in python to build a chat bot system\n",
      "---\n",
      " generated word2vec word embeddings for tax publications and irs tax code using tensorflow and python\n",
      "---\n",
      " calculated sentence similarity scores using word2vec embeddings and similarity measures from sklearn to handle semantic and syntactic differences\n",
      "---\n",
      " used different nlp similarity score functions (including word2vec, ngram, tfidf, topic modelling) to match an input question with our target answers\n",
      "---\n",
      " conducted a/b testing for the amus inc webpage and delivered simplified reports to senior management weekly\n",
      "---\n",
      " ensured high quality data collection and maintaining the integrity of the data. designed and developed the ui of the website using html, ajax, css and javascript\n",
      "---\n",
      " designed and developed the data management system using mysql\n",
      "---\n",
      " performed troubleshooting, fixed and deployed many python bug fixes of web application that were a main source of information for both customers\n",
      "---\n",
      " actively involved in agile methodologies and scrum process and worked closely with different stakeholders to understand their system needs\n",
      "---\n",
      " performed data migration and developed python / django based web application, postgre sqldb, and integrations with 3rd party email, messaging, storage services\n",
      "---\n",
      " python object oriented design code for manufacturing quality, monitoring, logging, and debugging code optimization\n",
      "---\n",
      " validated huge data and worked on python backend scripting\n",
      "---\n",
      " automated the developed web application/portal and developed python automation scripts using selenium ide\n",
      "---\n",
      " quantitative analysis and software development using data sets forecasting economic capital models and regulatory capital models for managing risk-based capital for the bank\n",
      "---\n",
      " analyzed and worked with all aspects of regression models (ols etc), and time series analysis\n",
      "---\n",
      " worked with credit-risk models (pd, lgd, ead) in use for retail/wholesale credit risk\n",
      "---\n",
      " redesigned market risk model originally implemented in r to use map reduce in cloudera's hadoop cluster using unsupervised learning /principal components analysis\n",
      "---\n",
      " used proc/sql to fetch tables from teradata warehouse\n",
      "---\n",
      " merged tables by using proc sort\n",
      "---\n",
      " random sampling using proc surveyselect\n",
      "---\n",
      " performed logistic regression on each variable and then delete the redundant ones\n",
      "---\n",
      " used append to generate the outcome variable table\n",
      "---\n",
      " checked outliers and missing values using proc univariate and proc freq\n",
      "---\n",
      " macros were employed for data transformation and filling up missing values\n",
      "---\n",
      " proc varclus was used to check the collinearity among explanatory variables\n",
      "---\n",
      " performed logistic regression on newly selected variables\n",
      "---\n",
      " created lift probability table and gain chart\n",
      "---\n",
      " create shared object repository, selenium library function, saved all components functions in library functions in selenium library\n",
      "---\n",
      " developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " used aws for application deployment and configuration\n",
      "---\n",
      " designed and developed the ui of the website using html, ajax, css and javascript\n",
      "---\n",
      " performed debugging and troubleshooting the web applications using subversion version control tool to coordinate team-development\n",
      "---\n",
      " created python scripts to validate based on the keyword-driven testing, test cases\n",
      "---\n",
      " developed for fully automated continuous integration system using python and bash scripting\n",
      "---\n",
      " environment: python 2.7, sas, django 1.7, css, html, jquery, pandas, postgresql, git, aws, ajax, css, javascript, hadoop python developer / data analyst hdfc bank - hyderabad, telangana december 2013 to march 2016 responsibilities\n",
      "---\n",
      " performed customer due diligence and determined the credit worthiness of the clients, approved accounts and dispensed limits up to $20k in authority and recommended higher limits to c-level management for approval\n",
      "---\n",
      " gathered loan data, designed new credit evaluation policies, created statistical data models using python, excel and sql which lowered bad debts by 5% for the personal loan segment\n",
      "---\n",
      " collaborated with cross-functional stakeholders and senior management to design credit check procedures that eliminated 15% of monthly customers at source who did not meet full criteria prior to loan underwriting process\n",
      "---\n",
      " communicated and presented default customers profiles along with reports using python and tableau, analytical results and strategic implications to senior management for strategic decision making\n",
      "---\n",
      " developed scripts in python to automate the customer query addressable system using python which decreased the time for solving the query of the customer by 45%\n",
      "---\n",
      " collaborated with other functional teams across the risk and non-risk groups to use standard methodologies and ensure a positive customer experience throughout the customer journey\n",
      "---\n",
      " monitored and resolved customer issues via phone, email, web or chat and managed customer issues from initiation till disbursement of loans which increased customer satisfaction by 15%\n",
      "---\n",
      " provided technical or analytical guidance as needed for issue management, project assessments, and reporting\n",
      "---\n",
      " environment: python (scipy, numpy, pandas, statsmodel, plotly), r, tableau, mysql, excel, google cloud platform python developer / data analyst icici securities - hyderabad, telangana september 2011 to december 2013 responsibilities\n",
      "---\n",
      " managed and supervised client portfolio by trading on equity, options and futures\n",
      "---\n",
      " developed a web scraper to collect historical financial data of technology giants from yahoo finance using datareader in python\n",
      "---\n",
      " visualized the moving averages of the stock over the years to obtain the trends and estimate the growth of the companies using seaborn in python\n",
      "---\n",
      " performed preliminary risk analysis and implemented methods like monte carlo and bootstrap to estimate the value of risk for an asset\n",
      "---\n",
      " analyzed trade racer website and customer data to identify market, product trends and profitable revenue growth opportunities using python\n",
      "---\n",
      " worked with managers and directors to design solutions and strategies enhancing trade racer platform\n",
      "---\n",
      " facilitated effective communications with equity researchers and senior management and generated trade ideas by devising derivatives strategies which boosted the customer satisfaction index by 24%\n",
      "---\n",
      " leverage information design concepts and principles to create compelling and effective charts, tables, presentations and other visuals using python and excel that convey analytical results clearly and effectively\n",
      "---\n",
      " surpassed 120% of target revenue for 3 consecutive quarters and ranked among the top 10 advisors in the western zone in terms of reactivating stopped customers\n",
      "---\n",
      " coached and mentored new trainees and consulted struggling advisors to help them meet monthly target goals\n",
      "---\n",
      " environment: python (datareader, pandas, seaborn, plotly, quandl), r, tableau, mysql, excel, yahoo finance, trade racer business data analyst bank of baroda - mumbai, maharashtra june 2009 to september 2011 responsibilities\n",
      "---\n",
      " conducted detailed industry analysis, research, drafted reports and developed analytics insights on sme industry\n",
      "---\n",
      " served as a key strategic partner to uncover underlying business sector needs and information gaps\n",
      "---\n",
      " coordinated with internal and external stakeholders to gather key compounding industry insights and proactively communicated industry news\n",
      "---\n",
      " implemented approaches like process capability analysis and root cause analysis to determine the reasons for problems in food and logistics, mining and textiles\n",
      "---\n",
      " maintained traceability among business requirements, technical requirements, design and testing\n",
      "---\n",
      " assisted to build analytic tools to manage data and streamline data analyses using r and sql server\n",
      "---\n",
      " created reporting documentation that identified metrics and data required for display as well as identification of filtering criteria and input\n",
      "---\n",
      " environment: python (datareader, numpy seaborn, plotly, pandas), r, tableau, sql server, excel python developer riconz technologies - hyderabad, telangana march 2007 to june 2009 responsibilities\n",
      "---\n",
      " gathered and analyzed the requirements and converted them into user requirement specifications and functional requirement specifications for the designers and developers to understand them as per their perspective\n",
      "---\n",
      " worked on object-oriented programming (oop) concepts using python, django and linux\n",
      "---\n",
      " developed web-based applications using python, django, xml, css, html, javascript, angular js and jquery\n",
      "---\n",
      " experience with json based rest web services and amazon web services (aws)\n",
      "---\n",
      " worked on amazon services like amazon cloud ec2\n",
      "---\n",
      " added support for amazon aws and rds to host static/media files and the database into amazon cloud\n",
      "---\n",
      " experience in writing sub queries, stored procedures, triggers, cursors, and functions on mysql and postgresql database\n",
      "---\n",
      " worked in agile and waterfall methodologies with high quality deliverables delivered on-time\n",
      "---\n",
      " experience with continuous integration and automation using jenkins\n",
      "---\n",
      " experience with unit testing/ test driven development (tdd), load testing\n",
      "---\n",
      " developed the required xml schema documents and implemented the framework for parsing xml documents\n",
      "---\n",
      " involved in unit testing and integration testing\n",
      "---\n",
      " worked on ajax framework to transform datasets and data tables into http-serializable json strings\n",
      "---\n",
      " designed interface using bootstrap framework\n",
      "---\n",
      " experience with working on multiple environments like development, testing, production. excellent analytical and problem-solving skills and ability to work on own besides being valuable and contributing team player\n",
      "---\n",
      " environment: python, django, rest web services, xml, css, http, ajax, angularjs, bootstrap, json, html, css, javascript, jquery, aws ec2, triggers, cursors, mysql and postgresql database, amazon cloud ec2, amazon web services (aws) \n",
      "---\n",
      " bachelor's skills c/c++, c++, hadoop, ms project, python, vba, visio, database, database systems, ms access, mysql, postgresql, tableau, data science, hadoop, machine learning, nlp, deep learning, neural networks, linux\n",
      "\n",
      "---\n",
      " experienced in designing star schema, snowflake schema for data warehouse, and ods architecture.\n",
      "---\n",
      " experienced in data modeling &data analysis experience using dimensional data modeling and relational data modeling, star schema/snowflake modeling, fact& dimensions tables, physical & logical data modeling.\n",
      "---\n",
      " experienced in big data analysis and developing data models using hive, pig, and map reduce, sql with strong data architecting skills designing data-centric solutions.\n",
      "---\n",
      " hands on experience with big data tools like hadoop, spark, hive, pig, impala, pyspark, sparksql.\n",
      "---\n",
      " very good knowledge and experience on aws, redshift, s3,andemr.\n",
      "---\n",
      " proficient in data analysis, mapping source and target systems for datamigration efforts and resolving issues relating to data migration.\n",
      "---\n",
      " excellent development experience sql, procedural language(pl) of databases like oracle, teradata,netezza,anddb2.\n",
      "---\n",
      " very good knowledge and working experience on big data tools like hadoop, azure data lake, aws redshift.\n",
      "---\n",
      " experienced in data scrubbing/cleansing, data quality, data mapping, data profiling, data validation in etl\n",
      "---\n",
      " experienced in creating and documenting metadata for oltp and olap when designing systems.\n",
      "---\n",
      " expertise in synthesizing machine learning, predictive analytics and big data technologies into integrated solutions.\n",
      "---\n",
      " extensive experience in development of t-sql, dts, olap, pl/sql, stored procedures, triggers, functions, packages, performance tuning and optimization for business logic implementation.\n",
      "---\n",
      " experience in using various packages in rand python like ggplot2, caret, dplyr, rweka, gmodels, rcurl, tm, c50, twitter, nlp, reshape2, rjson, dplyr, pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, beautiful soup, rpy2.\n",
      "---\n",
      " experienced using query tools like sql developer, pl/sql developer, and teradata sql assistant.\n",
      "---\n",
      " excellent knowledge of machine learning, mathematical modeling and operations research. comfortable with r, python, sas and weka, matlab, relational databases. deep understanding & exposure of big data eco-system.\n",
      "---\n",
      " expertise in designing complex mappings and have expertise in performance tuning and slowly-changing dimension tables and fact tables\n",
      "---\n",
      " extensively worked with teradata utilities bteq, fast export, and multi-load to export and load data to/from different source systems including flat files.\n",
      "---\n",
      " hands on experience in implementing lda, naive bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis.\n",
      "---\n",
      " expertise in extracting, transforming and loading data between homogeneous and heterogeneous systems like sql server, oracle, db2, ms access, excel, flat file and etc. using ssis packages.\n",
      "---\n",
      " proficient in system analysis, er/dimensional data modeling, database design and implementing rdbms specific features.\n",
      "---\n",
      " experience in unix shell scripting, perl scripting,and automation of etl processes.\n",
      "---\n",
      " extensively used etl to load data using power center / power exchange from source systems like flat files and excel files into staging tables and load the data into the target database oracle. analyzed the existing systems and made a feasibility study.\n",
      "---\n",
      " excellent understanding and working experience of industry standard methodologies like system development life cycle (sdlc), as per rational unified process (rup), agile methodologies.\n",
      "---\n",
      " proficiency in sql across a number of dialects (we commonly write mysql, postgresql, redshift, sql server, and oracle)\n",
      "---\n",
      " experienced in developing entity-relationship diagrams and modeling transactional databases and data warehouse using tools like erwin, er/studio,andpower designer and experienced with modeling using erwin in both forward and reverse engineering cases. authorized to work in the us for any employer work experience data scientist bbva compass - birmingham, al july 2017 to present architect framework bbva compass bancshares, inc - birmingham, al 2007 to present is a bank holding company headquartered in birmingham, alabama. it has been a subsidiary of the spanish multinational banco bilbao vizcaya argentaria since 2007 and operates chiefly in alabama, arizona, california, colorado, florida, new mexico, and texas.\n",
      "---\n",
      " design, develop and implement comprehensive data warehouse solution to extract, clean, transfer, load and manage quality/accuracy of data from various sources to edw enterprise data warehouse.\n",
      "---\n",
      " architect framework for data warehouse solutions to bring data from source system to edw and provide data mart solutions for order/sales operation, salesforce activity, inventory tracking, in-depth data mining and analysis for market projection etc.\n",
      "---\n",
      " utilized apache spark with python to develop and execute big data analytics and machine learning applications, executed machine learning use cases under spark ml and mllib.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as caffe, neon.\n",
      "---\n",
      " tested complex etl mappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, cassandra, mongodb, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " develop a high performance, scalable data architecture solution that incorporates a matrix of technology to relate architectural decision to business needs.\n",
      "---\n",
      " conducting strategy and architecture sessions and deliver artifacts such as mdm strategy (current state, interim state,and target state) and mdm architecture (conceptual, logical and physical) at the detail level.\n",
      "---\n",
      " design and development of dimensional data model on redshift to provide advanced selection analytics platform and developed simple to complex map reduce jobs using hive and pig.\n",
      "---\n",
      " designed and developed nlp models for sentiment analysis.\n",
      "---\n",
      " designed and provisioned the platform architecture to execute hadoop and machine learning use cases under cloud infrastructure, aws, emr, and s3.\n",
      "---\n",
      " developed and configured on informatica mdm hub supports the master data management (mdm), business intelligence (bi) and data warehousing platforms to meet business needs.\n",
      "---\n",
      " transforming staging area data into a star schema (hosted on amazon redshift) which was then used for developing embedded tableau dashboards\n",
      "---\n",
      " worked on machine learning on large size data using spark and mapreduce.\n",
      "---\n",
      " let the implementation of new statistical algorithms and operators on hadoop and sql platforms and utilized optimizations techniques, linear regressions, k-means clustering, nativebayes and other approaches.\n",
      "---\n",
      " developed spark/scala, python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " proficiency in sql across a number of dialects (we commonly write mysql, postgresql, redshift, teradata, and oracle)\n",
      "---\n",
      " responsible for full data loads from production to awsredshift staging environment and worked on migrating of edw to aws using emr and various other technologies.\n",
      "---\n",
      " worked on teradatasqlqueries, teradata indexes, utilities such as mload, tpump, fast load and fast export.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, regression models, neural networks, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " worked on data pre-processing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using python.\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau.\n",
      "---\n",
      " worked with bteq to submit sql statements, import and export data, and generate reports in teradata.\n",
      "---\n",
      " build and maintain scalable data pipelines using the hadoop ecosystem and other open sources components like hive and hbase.\n",
      "---\n",
      " created hive architecture used for real-time monitoring and hbase used for reporting and worked for map reduce and query optimization for hadoop hive and hbase architecture.\n",
      "---\n",
      " involved in teradata utilities (bteq, fast load, fast export, multiload, and tpump) in both windows and mainframe platforms.\n",
      "---\n",
      " built analytical data pipelines to port data in and out of hadoop/hdfs from structured and unstructured sources and designed and implemented system architecture for amazon ec2 based cloud-hosted solution for the client.\n",
      "---\n",
      " environment: erwin9.6.4, oracle 12c, python, pyspark, spark, spark mllib, tableau, ods, pl/sql, olap, oltp, aws, hadoop, map reduce, hdfs, python, mdm, teradata 15, hadoop, spark, cassandra, sap, ms excel, flat files, tableau, informatica, ssis, ssrs, aws ec2, aws emr, elastic search. data scientist bbva compass bancshares, inc - chicago, il august 2016 to june 2017 description:at aim specialty health (aim), it is our mission to promote appropriate, safe, and affordable health care. as the specialty benefits management partner of choice for today's leading healthcare organizations, we help improve the quality of care and reduce costs for today's most complex tests and treatments.\n",
      "---\n",
      " developed applications of machinelearning, statistical analysis,and data visualizations with challenging data processing problems in sustainability and biomedical domain.\n",
      "---\n",
      " worked on natural language processing with nltk module of python for application development for automated customer response.\n",
      "---\n",
      " used predictive modeling with tools in sas, spss, r, python.\n",
      "---\n",
      " responsible for design and development of advanced r/ python programs to prepare to transform and harmonize data sets in preparation for modeling.\n",
      "---\n",
      " identifying and executing process improvements, hands-on in various technologies such as oracle, informatica, and businessobjects.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs.\n",
      "---\n",
      " interaction with business analyst, smes,and other data architects to understand business needs and functionality for various project solutions.\n",
      "---\n",
      " created sql tables with referential integrity and developed queries using sql, sql\n",
      "---\n",
      "plus,and pl/sql.\n",
      "---\n",
      " involved with data analysis primarily identifying data sets, source data, source meta data, data definitions and data formats\n",
      "---\n",
      " wrote simple and advanced sql queries and scripts to create standard and adhoc reports for senior managers.\n",
      "---\n",
      " created pl/sql packages and database triggers and developed user procedures and prepared user manuals for the new programs.\n",
      "---\n",
      " prepare etlarchitect& design document which covers etlarchitect, ssisdesign, extraction, transformation,and loading of duck creek data into the dimensional model.\n",
      "---\n",
      " design logical & physical data model using msvisio 2003 data modeler tool.\n",
      "---\n",
      " participated in architect solution meetings & guidance in dimensional data modeling design.\n",
      "---\n",
      " applied linear regression, multiple regression, ordinary least square method, mean-variance, the theory of large numbers, logistic regression, dummy variable, residuals, poisson distribution, bayes, naive bayes, fitting function etc to data with help of scikit, scipy, numpy and pandas module of python.\n",
      "---\n",
      " applied clustering algorithms i.e. hierarchical, k-means with help of scikit and scipy.\n",
      "---\n",
      " developed visualizations and dashboards using ggplot, tableau\n",
      "---\n",
      " worked on development of data warehouse, data lake and etl systems using relational and non-relational tools like sql, no sql.\n",
      "---\n",
      " built and analyzed datasets using r, sas, matlab,and python (in decreasing order of usage).\n",
      "---\n",
      " applied linear regression in python and sas to understand the relationship between different attributes of the dataset and causal relationship between them\n",
      "---\n",
      " pipelined (ingest/clean/munge/transform) data for feature extraction toward downstream classification.\n",
      "---\n",
      " used clouderahadoopyarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " expertise in businessintelligence and data visualization using r and tableau.\n",
      "---\n",
      " validated the macro-economic data (e.g. blackrock, moody's etc.) and predictive analysis of world markets using key indicators in python and machine learning concepts like regression, bootstrap aggregation and random forest.\n",
      "---\n",
      " worked in large-scale database environments like hadoop and mapreduce, with working mechanism of hadoop clusters, nodes and hadoop distributed file system (hdfs).\n",
      "---\n",
      " interfaced with large-scaledatabase system through an etl server for data extraction and preparation.\n",
      "---\n",
      " identified patterns, data quality issues, and opportunities and leveraged insights by communicating opportunities with business partners.\n",
      "---\n",
      " environment:machine learning, aws, ms azure, cassandra, spark, hdfs, hive, pig, linux, python (scikit-learn/scipy/numpy/pandas), r, sas, spss, mysql, eclipse, pl/sql, sql connector, tableau. python developer walgreens - chicago, il march 2015 to july 2016 description:the walgreen company is an american company that operates as the second-largest pharmacy store chain in the united states behind cvs health. it specializes in filling prescriptions, health and wellness products, health information, and photo services.\n",
      "---\n",
      " designed and developed ui for creating dashboard application using angularjs, d3, c3, html, css, bootstrap, javascript and jquery.\n",
      "---\n",
      " developed and implemented python scripts to automate retrieval, parsing and reporting of configuration parameters from network devices connected to customer networks.\n",
      "---\n",
      " involved in user interface design and development using jsp, servlet, html5, css3,andjavascript.\n",
      "---\n",
      " wrote and tested python scripts to create new data files for linux server configuration using a python template tool.\n",
      "---\n",
      " modified controlling databases using sql generated via python and perl code, collected and analyzed data with python programs using sql queries from the database of data collected from the systems under tests.\n",
      "---\n",
      " developed new user interface components for different modules using kendo ui with various controls including grid controls, and chart controls etc.\n",
      "---\n",
      " involved in write application level code to interact with apis, web services using ajax, json and hence building type-ahead feature for zip code, city, and county lookup using jquery, ajax, and jquery ui.\n",
      "---\n",
      " worked on updating the existing clipboard to have the new features as per the client requirements.\n",
      "---\n",
      " skilled in using collections in python for manipulating and looping through different user-defined objects.\n",
      "---\n",
      " taken part in the entire lifecycle of the projects including design, development, and deployment, testing and implementation and support.\n",
      "---\n",
      " developed views and templates with python and django's view controller and templating language to create a user-friendly website interface.\n",
      "---\n",
      " automated different workflows, which are initiated manually with python scripts and unix shell scripting.\n",
      "---\n",
      " used python unit and functional testing modules such as unit test, unittest2, mock, and custom frameworks in-line with agile software development methodologies.\n",
      "---\n",
      " wrote and executed various mysql database queries from python using python-mysql connector and mysql db package.\n",
      "---\n",
      " created and maintained technical documentation for launching hadoop clusters and for executing hive queries and pig scripts.\n",
      "---\n",
      " developed sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in rdbms tables.\n",
      "---\n",
      " installed hadoop, map reduce, hdfs, aws and developed multiple mapreduce jobs in pig and hive for data cleaning and pre-processing.\n",
      "---\n",
      " managed datasets using panda data frames and mysql, queried mysql database queries from python using python-mysql connector and mysql db package to retrieve information.\n",
      "---\n",
      " involved in the web/application development using python, html5, css3, ajax, json,andjquery.\n",
      "---\n",
      " developed and tested many features for a dashboard using python, java, bootstrap, css, javascript, and jquery.\n",
      "---\n",
      " generated python django forms to record data of online users and used pytest for writing test cases.\n",
      "---\n",
      " implemented and modified various sql queries and functions, cursors and triggers as per the client requirements.\n",
      "---\n",
      " prototype proposal for issue tracker website using python/django connecting mysql as database.\n",
      "---\n",
      " the developed overall layout that meetscross-platform compatibility using bootstrap, media queries and angular ui bootstrap.\n",
      "---\n",
      " environment: python, html5, css3, ajax, json, jquery, mysql, numpy, sql alchemy, matplotlib, hadoop, pig scripts. python developer citi bank - irving, tx july 2013 to february 2015 description:the project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables. further, the aim was to improve bank's efficiency by reducing default rate while offering new products. moreover, i was involved in a project to identify the employees' access level, based on his/her current & historical tasks and duties..\n",
      "---\n",
      " involved in the software development lifecycle (sdlc) of tracking the requirements, gathering, analysis, detailed design, development, system testing and user acceptance testing.\n",
      "---\n",
      " developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " involved in designing user interactive web pages as the front-end part of the web application using various web technologies like html, javascript, angular js, jquery, ajax and implemented css for better appearance and feel.\n",
      "---\n",
      " knowledge of cross-browser and cross-platform development of html and css based websites in windows like ie 6, ie 7, ie 8 and ff.\n",
      "---\n",
      " interactive in providing change requests, trouble reports and requirements collection with the client.\n",
      "---\n",
      " actively involved in developing the methods for create, read, update and delete (crud) in active record.\n",
      "---\n",
      " working knowledge of various aws technologies like sqs queuing, sns notification, s3 storage, redshift, data pipeline, emr.\n",
      "---\n",
      " developed a fully automated continuous integration system using git, jenkins, mysql and custom tools developed in python and bash.\n",
      "---\n",
      " implemented multithreading module and complex networking operations like race route, smtp mail server and web server using python.\n",
      "---\n",
      " used numpy for numerical analysis for the insurance premium.\n",
      "---\n",
      " implemented and modified various sql queries and functions, cursors and triggers as per the client requirements.\n",
      "---\n",
      " managed code versioning with github, bitbucket,and deployment to staging and production servers.\n",
      "---\n",
      " implemented mvc architecture in developing the web application with the help of django framework.\n",
      "---\n",
      " used celery as task queue and rabbit mq, redis as messaging broker to execute asynchronous tasks.\n",
      "---\n",
      " designed and managed api system deployment using a fasthttp server and amazon aws architecture.\n",
      "---\n",
      " involved in code reviews using github pull requests, reducing bugs, improving code quality, and increasing knowledge sharing\n",
      "---\n",
      " install and configuring monitoring scripts for aws ec2 instances.\n",
      "---\n",
      " working under unix environment in the development of application using python and familiar with all its commands.\n",
      "---\n",
      " developed remote integration with third-party platforms by using restful web services.\n",
      "---\n",
      " updated and maintained jenkins for automatic building jobs and deployment.\n",
      "---\n",
      " improved code reuse and performance by making effective use of various design patterns and refactoring code base.\n",
      "---\n",
      " updated and maintained puppet rspec unit/system test.\n",
      "---\n",
      " worked on debugging and troubleshooting programming related issues.\n",
      "---\n",
      " worked in the mysql database on simple queries and writing stored procedures for normalization.\n",
      "---\n",
      " deployment of the web application using the linux server.\n",
      "---\n",
      " environment: python 2.7, django 1.4, html5, css, xml, mysql, javascript, backbone js, jquery, mongo db, ms sql server, javascript, git, github, aws, linux, shell scripting, ajax, java. hadoop developer sitel india pvt ltd - hyderabad, telangana august 2012 to june 2013 description: sitel group combines comprehensive customer care capabilities with unparalleled digital, training and technology expertise to help build brand loyalty and improve customer satisfaction. we partner with our clients to effectively harness our industry's explosive digital transformation to ensure an innovative, end-to-end solution to managing and enhancing the customer experience.\n",
      "---\n",
      " designed and developed multiple mapreduce jobs in java for complex analysis. importing and exporting the data using sqoop from hdfs to relational database systems and vice-versa.\n",
      "---\n",
      " integrated apache storm with kafka to perform web analytics. uploaded clickstream data from kafka to hdfs, hbase, and hive by integrating with storm.\n",
      "---\n",
      " configured flume to transport web server logs into hdfs. also used kite logging module to upload web server logs into hdfs.\n",
      "---\n",
      " developed udf functions for hive and wrote complex queries in hive for data analysis\n",
      "---\n",
      " performed installation of hadoop in fully and pseudo distributed mode for poc in early stages of the project.\n",
      "---\n",
      " analyze, develop, integrate, and then direct the operationalization of new data sources.\n",
      "---\n",
      " generating scala and java classes from the respective apis so that they can be incorporated into the overall application.\n",
      "---\n",
      " responsible for working with different teams in building hadoop infrastructure\n",
      "---\n",
      " gathered business requirements in meetings for successful implementation and poc and moving it to production and implemented poc to migrate map reduce jobs into spark rdd transformations using scala.\n",
      "---\n",
      " implemented different machine learning techniques in scala using scala machine learning library.\n",
      "---\n",
      " developed spark applications using scala for easy hadoop transitions.\n",
      "---\n",
      " successfully loaded files to hive and hdfs from oracle, netezza and sql server using sqoop\n",
      "---\n",
      " uses talend open studio to load files into hadoop hive tables and performed etl aggregations in hadoop hive.\n",
      "---\n",
      " developed simple to quebec and python mapreduce streaming jobs using python language that is implemented using hive and pig.\n",
      "---\n",
      " designing & creating etl jobs through talend to load huge volumes of data into cassandra, hadoop ecosystem, and relational databases.\n",
      "---\n",
      " worked on analyzing, writing hadoop mapreduce jobs using java api, pig, and hive.\n",
      "---\n",
      " developed some machine learning algorithms using mahout for data mining for the data stored in hdfs\n",
      "---\n",
      " used flume extensively in gathering and moving log data files from application servers to a central location in hadoop distributed file system (hdfs)\n",
      "---\n",
      " worked with oozie workflow manager to schedule hadoop jobs and high intensive jobs\n",
      "---\n",
      " responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring, and troubleshooting, manage and review data backups, manage and review hadoop log files.\n",
      "---\n",
      " extensively used hive/hql or hive queries to query data in hive tables and loaded data into hive tables.\n",
      "---\n",
      " creating udf functions in pig &hive and applying partitioning and bucketing techniques in hive for performance improvement\n",
      "---\n",
      " creating indexes and tuning the sql queries in hive and involved in database connection by using sqoop.\n",
      "---\n",
      " involved in hadoop name node metadata backups and load balancing as a part of cluster maintenance and monitoring\n",
      "---\n",
      " used file system check (fsck) to check the health of files in hdfs and used sqoop to import data from sql server to cassandra\n",
      "---\n",
      " monitored nightly jobs to export data out of hdfs to be stored offsite as part of hdfs backup\n",
      "---\n",
      " used pig for analysis of large datasets and brought data back to hbaseby pig\n",
      "---\n",
      " developed python mapper and reducer scripts and implemented them using hadoop streaming.\n",
      "---\n",
      " created schema and database objects in hive and developed unix scripts for data loading and automation\n",
      "---\n",
      " involved in the training of big data ecosystem to end-users.\n",
      "---\n",
      " environment: java1.5, j2ee, hibernate, spring, junit, weblogic html, css, javascript, jenkins, node.js, jquery, linux, cicd, spring boot, maven, log4j and junit, eclipse, rest, sql navigator. java developer napier healthcare pvt. ltd - hyderabad, telangana october 2009 to july 2011 description: napier healthcare was established in 1996 as a healthcare it products and services company. with deep domain knowledge and singular focus, we have built a robust, feature-rich suite of solutions to deliver value to the stakeholders of the healthcare industry worldwide.\n",
      "---\n",
      " development, testing, maintenance and product delivery.\n",
      "---\n",
      " developed a scalable and maintainable application using j2ee framework, hibernate, mvc model, struts, and j2ee design patterns.\n",
      "---\n",
      " prepared sow (statement of work) by communicating with agencies and organized meetings about requirements.\n",
      "---\n",
      " followed java & j2ee design patterns and the coding guidelines to design and develop the application.\n",
      "---\n",
      " extensively used jstl tags and struts tag libraries. used struts tiles as well in the presentation tier.\n",
      "---\n",
      " developing the application using struts and spring based frameworks.\n",
      "---\n",
      " actively involved in designing and implementing the application using various design patterns.\n",
      "---\n",
      " coordinating with clients and closing production issues relating to software development.\n",
      "---\n",
      " identifying and evaluate technology solutions, problem solving, and troubleshooting.\n",
      "---\n",
      " done with server-side validations using struts validation framework.\n",
      "---\n",
      " processed json response data by consuming restful web services and used an angular filter for implementing search results.\n",
      "---\n",
      " used struts-config.xml file for defining mapping definitions and action forward definitions\n",
      "---\n",
      " developed the action classes which is in between view and model layers, action form classes which is used to maintain session state of a web application, created jsps (java server pages) using struts tag libraries and configured in struts-config.xml, web.xml files.\n",
      "---\n",
      " this application is designed using mvc architecture to maintain easily.\n",
      "---\n",
      " hibernate is used for database connectivity and designed hql (hibernate query language) to create, modify and update the tables.\n",
      "---\n",
      " created new soap services using jax-ws specifications.\n",
      "---\n",
      " wrote junit test cases for testing.\n",
      "---\n",
      " environment:java, struts, hibernate, jsp, servlets, soap ui, html, css, java script, junit, apache tomcat server, ejb, netbeans \n",
      "---\n",
      " bachelor's skills python (10+ years), mysql (10+ years), oracle (10+ years), pl/sql (10+ years), sql (10+ years) additional information technical skills\n",
      "---\n",
      " python libraries beautiful soup, numpy, scipy, matplotlib, python-twitter, pandas data frame, urllib2\n",
      "---\n",
      " data analytics tools/programming python (numpy, scipy, pandas, gensim, keras), r (caret, weka, ggplot), matlab, microsoft sql server, oracle pl/sql, python.\n",
      "---\n",
      " database oracle11g, mysql 5.x, and sqlserver\n",
      "---\n",
      " version control svn, clear case, cvs\n",
      "---\n",
      " reporting tools ms office (word/excel/powerpoint/ visio/outlook), crystal reports xi, ssrs, cognos 7.0/6.0.\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " microsoft power bi, tableau, ssis, ssrs, ssas, business intelligence development studio (bids), visual studio, crystal reports, informatica 6.1.\n",
      "---\n",
      " database design tools and data modeling\n",
      "---\n",
      " ms visio, erwin 4.5/4.0, star schema/snowflake schema modeling, fact & dimensions tables, physical & logical data modeling, normalization and de-normalization techniques, kimball &inmon methodologies\n",
      "---\n",
      " ide's pycharm, emacs, eclipse, netbeans, sublime, soap ui\n",
      "---\n",
      " web/app. servers websphere application server 8.0, apache tomcat, web logic 11g/ 12c, jboss 4.x/5.x\n",
      "---\n",
      " cloudtechnologies aws, s3.\n",
      "\n",
      "---\n",
      " close to 10 years of expert involvement in it in which i have 3+ years of knowledge in data mining, machine learning and spark development with big datasets of structured and unstructured data.\n",
      "---\n",
      " data acquisition, data validation, predictive demonstrating, data visualization. capable in measurable programming languages like r and python.\n",
      "---\n",
      " proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " adept and deep understanding of statistical modeling, multivariate analysis, model testing, problem analysis, model comparison and validation.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experience in using various packages in r and libraries in python.\n",
      "---\n",
      " working knowledge in hadoop, hive and nosql databases like cassandra and hbase.\n",
      "---\n",
      " hands on experience in implementing lda, naive bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " good industry knowledge, analytical and problem-solving skills and ability to work well within a team as well as an individual.\n",
      "---\n",
      " highly creative, innovative, committed, intellectually curious, business savvy with effective communication and interpersonal skills.\n",
      "---\n",
      " analyzed trading mechanism for real-time transactions and build collateral management tools.\n",
      "---\n",
      " compiled data from various sources to perform complex analysis for actionable results.\n",
      "---\n",
      " utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " measured efficiency of hadoop/hive environment ensuring sla is met.\n",
      "---\n",
      " developed spark code using scala and spark-sql/streaming for faster processing of data.\n",
      "---\n",
      " prepared spark build from the source code and ran the pig scripts using spark rather using mr jobs for better performance.\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " analyzing the system for new enhancements/functionalities and perform impact analysis of the application for implementing etl changes.\n",
      "---\n",
      " imported data using sqoop to load data from mysql to hdfs on regular basis.\n",
      "---\n",
      " developed scripts and batch job to schedule various hadoop program. used tensorflow to train the model from insightful data and look at thousands of examples.\n",
      "---\n",
      " designing, developing and optimizing sql code (ddl / dml).\n",
      "---\n",
      " building performant, scalable etl processes to load, cleanse and validate data.\n",
      "---\n",
      " expertise in data archival and data migration, ad-hoc reporting and code utilizing sas on unix and windows environments.\n",
      "---\n",
      " tested and debugged sas programs against the test data.\n",
      "---\n",
      " processed the data in sas for the given requirement using sas programming concepts.\n",
      "---\n",
      " imported and exported data files to and from sas using proc import and proc export from excel and various delimited text-based data files such as .txt (tab delimited) and .csv (comma delimited) files into sas datasets for analysis.\n",
      "---\n",
      " expertise in producing rtf, pdf, html files using sas ods facility.\n",
      "---\n",
      " providing support for data processes. this will involve monitoring data, profiling database usage, trouble shooting, tuning and ensuring data integrity.\n",
      "---\n",
      " participating in the full software development lifecycle with requirements, solution design, development, qa implementation, and product support using scrum and other agile methodologies.\n",
      "---\n",
      " collaborate with team members and stakeholders in design and development of data environment.\n",
      "---\n",
      " learning new tools and skillsets as needs arise.\n",
      "---\n",
      " preparing associated documentation for specifications, requirements and testing.\n",
      "---\n",
      " optimizing the tensorflow model for an efficiency.\n",
      "---\n",
      " used tensorflow for text summarization.\n",
      "---\n",
      " used spark api over cloudera hadoop yarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " developed kafka producer and consumers for message handling.\n",
      "---\n",
      " responsible for analyzing multi-platform applications using python.\n",
      "---\n",
      " used storm for an automatic mechanism to analyze large amounts of non-unique data points with low latency and high throughput.\n",
      "---\n",
      " developed mapreduce jobs in python for data cleaning and data processing.\n",
      "---\n",
      " performed data profiling to learn about user behaviour and merged data from multiple data sources.\n",
      "---\n",
      " participated in all phases of data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " performed k-means clustering, multivariate analysis and support vector machines in python and r.\n",
      "---\n",
      " developed clustering algorithms and support vector machines that improved customer segmentation and market expansion.\n",
      "---\n",
      " professional tableau user (desktop, online, and server).\n",
      "---\n",
      " data story teller, mining data from different data source such as sql server, oracle, cube database, web analytics, business object and hadoop.\n",
      "---\n",
      " providing ad hoc analysis and reports to executive level management team.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, toad, business objects, power bi and smart view.\n",
      "---\n",
      " in unix development environment, for financial application reports used batch processes and models using perl and korn shell scripts with partitions and sub-partitions on oracle database.\n",
      "---\n",
      " developed analytics and strategy to integrate b2b analytics in outbound calling operations.\n",
      "---\n",
      " implemented analytics delivery on cloud-based visualization using shiny tool for business object and google analytics platform.\n",
      "---\n",
      " spoc data scientist and predictive analyst to create annual and quarterly business forecast reports.\n",
      "---\n",
      " main source of business regression report.\n",
      "---\n",
      " creating various b2b predictive and descriptive analytics using r and tableau.\n",
      "---\n",
      " creating and automating ad hoc reports.\n",
      "---\n",
      " responsible for planning & scheduling new product releases and promotional offers.\n",
      "---\n",
      " worked on nosql databases like cassandra.\n",
      "---\n",
      " experienced in agile methodologies and scrum process.\n",
      "---\n",
      " parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " worked on text analytics, naive bayes, sentiment analysis, creating word clouds and retrieving data from twitter and other social networking platforms. extensive experience and proficiency in using sas ods to create output files in a variety of formats including rtf, html and pdf.\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r and python.\n",
      "---\n",
      " worked on the project from gathering requirements to developing the entire application. worked on anaconda python environment. created, activated and programmed in anaconda environment. wrote programs for performance calculations using numpy and sqlalchemy.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " used python modules of urllib, urllib2, requests for web crawling. experience using all these ml techniques: clustering, regression, classification, graphical models.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " involved in development of web services using soap for sending and getting data from the external interface in the xml format. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " worked on development of sql and stored procedures on mysql.\n",
      "---\n",
      " analyzed the code completely and have reduced the code redundancy to the optimal level.\n",
      "---\n",
      " design and build a text classification application using different text classification models.\n",
      "---\n",
      " used jira for defect tracking and project management.\n",
      "---\n",
      " worked on writing and as well as read data from csv and excel file formats.\n",
      "---\n",
      " involved in sprint planning sessions and participated in the daily agile scrum meetings.\n",
      "---\n",
      " conducted every day scrum as part of the scrum master role.\n",
      "---\n",
      " developed the project in linux environment.\n",
      "---\n",
      " worked on resulting reports of the application.\n",
      "---\n",
      " performed qa testing on the application.\n",
      "---\n",
      " held meetings with client and worked for the entire project with limited help from the client.\n",
      "---\n",
      " using python libraries for machine learning like pandas, numpy, matplotlib, sklearn, scipy to load the dataset, summarizing the dataset, visualizing the dataset, evaluating some algorithms and making some predictions.\n",
      "---\n",
      " perform application development, maintenance and ensure adherence to process and information security control.\n",
      "---\n",
      " aws (vpc, ec2, s3, route 53).\n",
      "---\n",
      " a key part of api design and development\n",
      "---\n",
      " unit test and debugging\n",
      "---\n",
      " maintain and extend the existing python/flask rest\n",
      "---\n",
      " a strong eye on code re-usability and maintainability\n",
      "---\n",
      " strong background in object-oriented programming, design patterns, algorithms, and data structures.\n",
      "---\n",
      " used python scripts to update the content in database and manipulate files.\n",
      "---\n",
      " generated python django forms to maintain the record of online users.\n",
      "---\n",
      " used django api's to access the database.\n",
      "---\n",
      " writing unit, functional, and integration test cases for cloud computing applications on aws.\n",
      "---\n",
      " writing python scripts with cloud formation templates to automate installation of auto scaling, ec2, vpc and other services.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and aws architecture.\n",
      "---\n",
      " developed restful api's using python flask and sql alchemy data models as well as ensured code.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and amazon aws architecture.\n",
      "---\n",
      " designed and developed a horizontally scalable apis using python flask.\n",
      "---\n",
      " involved in back end development using python with framework flask.\n",
      "---\n",
      " wrote python modules to view and connect the apache cassandra instance.\n",
      "---\n",
      " created unit test/ regression test framework for working/new code.\n",
      "---\n",
      " responsible for designing, developing, testing, deploying and maintaining the web application.\n",
      "---\n",
      " wrote and executed various mysql database queries from python mysql connector and mysql db package.\n",
      "---\n",
      " involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications.\n",
      "---\n",
      " which is main source of data for customers and internal customer service team.\n",
      "---\n",
      " implemented soap/restful web services in json format.\n",
      "---\n",
      " attended many days to day meetings with developers and users and performed qa testing on the application.\n",
      "---\n",
      " environment: python, django, api, html, css, ajax, git, aws, apache http, flask, xml, ood, shell scripting, mysql, cassandra. \n",
      "---\n",
      " bachelor ece in jntuh university bachelor's skills algorithms (8 years), api (5 years), machine learning (10+ years), python. (10+ years), sql (10+ years), \n",
      "---\n",
      " expertise: scikit-learn, nltk, spacy, numpy, scipy, opencv, deep learning, nlp, rnn, cnn, tensor flow, keras, matplotlib, microsoft visual studio, microsoft office. (10+ years) additional information technical skills:\n",
      "---\n",
      " expertise: scikit-learn, nltk, spacy, numpy, scipy, opencv, deep learning, nlp, rnn, cnn, tensor flow, keras, matplotlib, microsoft visual studio, microsoft office.\n",
      "---\n",
      " machine learning algorithms: linear regression, logistic regression, decision trees, random forest, k-means clustering, support vector machines, gradient boost machines & xgboost, neural networks.\n",
      "---\n",
      " data analysis skills: data cleaning, data visualization, feature selection, pandas.\n",
      "---\n",
      " operating systems: windows, mac and linux, unix.\n",
      "---\n",
      " programming languages: python, sql, r, matlab, torch, c, c++, java ,octave, apache spark, hadoop, spark ml.\n",
      "---\n",
      " other programming knowledge and skills: elasticsearch, data scraping, restful-api using django web frame work.\n",
      "---\n",
      " tools: toad, erwin, aws, azure,d3, mule soft, alteryx, tableau, shiny, adobe analytics, anaconda\n",
      "\n",
      "---\n",
      " close to 10 years of expert involvement in it in which i have 3+ years of knowledge in data mining, machine learning and spark development with big datasets of structured and unstructured data.\n",
      "---\n",
      " data acquisition, data validation, predictive demonstrating, data visualization. capable in measurable programming languages like r and python.\n",
      "---\n",
      " proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " adept and deep understanding of statistical modeling, multivariate analysis, model testing, problem analysis, model comparison and validation.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experience in using various packages in r and libraries in python.\n",
      "---\n",
      " working knowledge in hadoop, hive and nosql databases like cassandra and hbase.\n",
      "---\n",
      " hands on experience in implementing lda, naive bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " good industry knowledge, analytical and problem-solving skills and ability to work well within a team as well as an individual.\n",
      "---\n",
      " highly creative, innovative, committed, intellectually curious, business savvy with effective communication and interpersonal skills.\n",
      "---\n",
      " analyzed trading mechanism for real-time transactions and build collateral management tools.\n",
      "---\n",
      " compiled data from various sources to perform complex analysis for actionable results.\n",
      "---\n",
      " utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " measured efficiency of hadoop/hive environment ensuring sla is met.\n",
      "---\n",
      " developed spark code using scala and spark-sql/streaming for faster processing of data.\n",
      "---\n",
      " prepared spark build from the source code and ran the pig scripts using spark rather using mr jobs for better performance.\n",
      "---\n",
      " analyzing the system for new enhancements/functionalities and perform impact analysis of the application for implementing etl changes.\n",
      "---\n",
      " imported data using sqoop to load data from mysql to hdfs on regular basis.\n",
      "---\n",
      " developed scripts and batch job to schedule various hadoop program. used tensorflow to train the model from insightful data and look at thousands of examples.\n",
      "---\n",
      " designing, developing and optimizing sql code (ddl / dml).\n",
      "---\n",
      " building performant, scalable etl processes to load, cleanse and validate data.\n",
      "---\n",
      " expertise in data archival and data migration, ad-hoc reporting and code utilizing sas on unix and windows environments.\n",
      "---\n",
      " tested and debugged sas programs against the test data.\n",
      "---\n",
      " processed the data in sas for the given requirement using sas programming concepts.\n",
      "---\n",
      " imported and exported data files to and from sas using proc import and proc export from excel and various delimited text-based data files such as .txt (tab delimited) and .csv (comma delimited) files into sas datasets for analysis.\n",
      "---\n",
      " expertise in producing rtf, pdf, html files using sas ods facility.\n",
      "---\n",
      " providing support for data processes. this will involve monitoring data, profiling database usage, trouble shooting, tuning and ensuring data integrity.\n",
      "---\n",
      " participating in the full software development lifecycle with requirements, solution design, development, qa implementation, and product support using scrum and other agile methodologies.\n",
      "---\n",
      " collaborate with team members and stakeholders in design and development of data environment.\n",
      "---\n",
      " learning new tools and skillsets as needs arise.\n",
      "---\n",
      " preparing associated documentation for specifications, requirements and testing.\n",
      "---\n",
      " optimizing the tensorflow model for an efficiency.\n",
      "---\n",
      " used tensorflow for text summarization.\n",
      "---\n",
      " used spark api over cloudera hadoop yarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " developed kafka producer and consumers for message handling.\n",
      "---\n",
      " responsible for analyzing multi-platform applications using python.\n",
      "---\n",
      " used storm for an automatic mechanism to analyze large amounts of non-unique data points with low latency and high throughput.\n",
      "---\n",
      " developed mapreduce jobs in python for data cleaning and data processing.\n",
      "---\n",
      " performed data profiling to learn about user behaviour and merged data from multiple data sources.\n",
      "---\n",
      " participated in all phases of data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " performed k-means clustering, multivariate analysis and support vector machines in python and r.\n",
      "---\n",
      " developed clustering algorithms and support vector machines that improved customer segmentation and market expansion.\n",
      "---\n",
      " professional tableau user (desktop, online, and server).\n",
      "---\n",
      " data story teller, mining data from different data source such as sql server, oracle, cube database, web analytics, business object and hadoop.\n",
      "---\n",
      " providing ad hoc analysis and reports to executive level management team.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, toad, business objects, power bi and smart view.\n",
      "---\n",
      " in unix development environment, for financial application reports used batch processes and models using perl and korn shell scripts with partitions and sub-partitions on oracle database.\n",
      "---\n",
      " developed analytics and strategy to integrate b2b analytics in outbound calling operations.\n",
      "---\n",
      " implemented analytics delivery on cloud-based visualization using shiny tool for business object and google analytics platform.\n",
      "---\n",
      " spoc data scientist and predictive analyst to create annual and quarterly business forecast reports.\n",
      "---\n",
      " main source of business regression report.\n",
      "---\n",
      " creating various b2b predictive and descriptive analytics using r and tableau.\n",
      "---\n",
      " creating and automating ad hoc reports.\n",
      "---\n",
      " responsible for planning & scheduling new product releases and promotional offers.\n",
      "---\n",
      " worked on nosql databases like cassandra.\n",
      "---\n",
      " experienced in agile methodologies and scrum process.\n",
      "---\n",
      " parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " worked on text analytics, naive bayes, sentiment analysis, creating word clouds and retrieving data from twitter and other social networking platforms. extensive experience and proficiency in using sas ods to create output files in a variety of formats including rtf, html and pdf.\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r and python.\n",
      "---\n",
      " worked on the project from gathering requirements to developing the entire application. worked on anaconda python environment. created, activated and programmed in anaconda environment. wrote programs for performance calculations using numpy and sqlalchemy.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " used python modules of urllib, urllib2, requests for web crawling. experience using all these ml techniques: clustering, regression, classification, graphical models.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " involved in development of web services using soap for sending and getting data from the external interface in the xml format. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " worked on development of sql and stored procedures on mysql.\n",
      "---\n",
      " analyzed the code completely and have reduced the code redundancy to the optimal level.\n",
      "---\n",
      " design and build a text classification application using different text classification models.\n",
      "---\n",
      " used jira for defect tracking and project management.\n",
      "---\n",
      " worked on writing and as well as read data from csv and excel file formats.\n",
      "---\n",
      " involved in sprint planning sessions and participated in the daily agile scrum meetings.\n",
      "---\n",
      " conducted every day scrum as part of the scrum master role.\n",
      "---\n",
      " developed the project in linux environment.\n",
      "---\n",
      " worked on resulting reports of the application.\n",
      "---\n",
      " performed qa testing on the application.\n",
      "---\n",
      " held meetings with client and worked for the entire project with limited help from the client.\n",
      "---\n",
      " using python libraries for machine learning like pandas, numpy, matplotlib, sklearn, scipy to load the dataset, summarizing the dataset, visualizing the dataset, evaluating some algorithms and making some predictions.\n",
      "---\n",
      " perform application development, maintenance and ensure adherence to process and information security control.\n",
      "---\n",
      " aws (vpc, ec2, s3, route 53).\n",
      "---\n",
      " a key part of api design and development\n",
      "---\n",
      " unit test and debugging\n",
      "---\n",
      " maintain and extend the existing python/flask rest\n",
      "---\n",
      " a strong eye on code re-usability and maintainability\n",
      "---\n",
      " strong background in object-oriented programming, design patterns, algorithms, and data structures.\n",
      "---\n",
      " used python scripts to update the content in database and manipulate files.\n",
      "---\n",
      " generated python django forms to maintain the record of online users.\n",
      "---\n",
      " used django api's to access the database.\n",
      "---\n",
      " writing unit, functional, and integration test cases for cloud computing applications on aws.\n",
      "---\n",
      " writing python scripts with cloud formation templates to automate installation of auto scaling, ec2, vpc and other services.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and aws architecture.\n",
      "---\n",
      " developed restful api's using python flask and sql alchemy data models as well as ensured code.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and amazon aws architecture.\n",
      "---\n",
      " designed and developed a horizontally scalable apis using python flask.\n",
      "---\n",
      " involved in back end development using python with framework flask.\n",
      "---\n",
      " wrote python modules to view and connect the apache cassandra instance.\n",
      "---\n",
      " created unit test/ regression test framework for working/new code.\n",
      "---\n",
      " responsible for designing, developing, testing, deploying and maintaining the web application.\n",
      "---\n",
      " wrote and executed various mysql database queries from python mysql connector and mysql db package.\n",
      "---\n",
      " involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications.\n",
      "---\n",
      " which is main source of data for customers and internal customer service team.\n",
      "---\n",
      " implemented soap/restful web services in json format.\n",
      "---\n",
      " attended many days to day meetings with developers and users and performed qa testing on the application.\n",
      "---\n",
      " environment: python, django, api, html, css, ajax, git, aws, apache http, flask, xml, ood, shell scripting, mysql, cassandra. \n",
      "---\n",
      " bachelor ece in jntuh university bachelor's skills algorithms (8 years), api (5 years), machine learning (10+ years), python. (10+ years), sql (10+ years), machine learning, aws, ms azure, cassandra, sas, spark, hdfs, hive, pig, linux, anaconda python , mysql, eclipse, pl/sql, sql connector, sparkml. additional information technical skills:\n",
      "---\n",
      " expertise: scikit-learn, nltk, spacy, numpy, scipy, opencv, deep learning, nlp, rnn, cnn, tensor flow, keras, matplotlib, microsoft visual studio, microsoft office.\n",
      "---\n",
      " machine learning algorithms: linear regression, logistic regression, decision trees, random forest, k-means clustering, support vector machines, gradient boost machines & xgboost, neural networks.\n",
      "---\n",
      " data analysis skills: data cleaning, data visualization, feature selection, pandas.\n",
      "---\n",
      " operating systems: windows, mac and linux, unix.\n",
      "---\n",
      " programming languages: python, sql, r, matlab, torch, c, c++, java ,octave, apache spark, hadoop, spark ml.\n",
      "---\n",
      " other programming knowledge and skills: elasticsearch, data scraping, restful-api using django web frame work.\n",
      "---\n",
      " tools: toad, erwin, aws, azure,d3, mule soft, alteryx, tableau, shiny, adobe analytics, anaconda\n",
      "\n",
      "---\n",
      " close to 10 years of expert involvement in it in which i have 3+ years of knowledge in data mining, machine learning and spark development with big datasets of structured and unstructured data.\n",
      "---\n",
      " data acquisition, data validation, predictive demonstrating, data visualization. capable in measurable programming languages like r and python.\n",
      "---\n",
      " proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " adept and deep understanding of statistical modeling, multivariate analysis, model testing, problem analysis, model comparison and validation.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experience in using various packages in r and libraries in python.\n",
      "---\n",
      " working knowledge in hadoop, hive and nosql databases like cassandra and hbase.\n",
      "---\n",
      " hands on experience in implementing lda, naive bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " good industry knowledge, analytical and problem-solving skills and ability to work well within a team as well as an individual.\n",
      "---\n",
      " highly creative, innovative, committed, intellectually curious, business savvy with effective communication and interpersonal skills.\n",
      "---\n",
      " analyzed trading mechanism for real-time transactions and build collateral management tools.\n",
      "---\n",
      " compiled data from various sources to perform complex analysis for actionable results.\n",
      "---\n",
      " utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " measured efficiency of hadoop/hive environment ensuring sla is met.\n",
      "---\n",
      " developed spark code using scala and spark-sql/streaming for faster processing of data.\n",
      "---\n",
      " prepared spark build from the source code and ran the pig scripts using spark rather using mr jobs for better performance.\n",
      "---\n",
      " analyzing the system for new enhancements/functionalities and perform impact analysis of the application for implementing etl changes.\n",
      "---\n",
      " imported data using sqoop to load data from mysql to hdfs on regular basis.\n",
      "---\n",
      " developed scripts and batch job to schedule various hadoop program. used tensorflow to train the model from insightful data and look at thousands of examples.\n",
      "---\n",
      " designing, developing and optimizing sql code (ddl / dml).\n",
      "---\n",
      " building performant, scalable etl processes to load, cleanse and validate data.\n",
      "---\n",
      " expertise in data archival and data migration, ad-hoc reporting and code utilizing sas on unix and windows environments.\n",
      "---\n",
      " tested and debugged sas programs against the test data.\n",
      "---\n",
      " processed the data in sas for the given requirement using sas programming concepts.\n",
      "---\n",
      " imported and exported data files to and from sas using proc import and proc export from excel and various delimited text-based data files such as .txt (tab delimited) and .csv (comma delimited) files into sas datasets for analysis.\n",
      "---\n",
      " expertise in producing rtf, pdf, html files using sas ods facility.\n",
      "---\n",
      " providing support for data processes. this will involve monitoring data, profiling database usage, trouble shooting, tuning and ensuring data integrity.\n",
      "---\n",
      " participating in the full software development lifecycle with requirements, solution design, development, qa implementation, and product support using scrum and other agile methodologies.\n",
      "---\n",
      " collaborate with team members and stakeholders in design and development of data environment.\n",
      "---\n",
      " learning new tools and skillsets as needs arise.\n",
      "---\n",
      " preparing associated documentation for specifications, requirements and testing.\n",
      "---\n",
      " optimizing the tensorflow model for an efficiency.\n",
      "---\n",
      " used tensorflow for text summarization.\n",
      "---\n",
      " used spark api over cloudera hadoop yarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " developed kafka producer and consumers for message handling.\n",
      "---\n",
      " responsible for analyzing multi-platform applications using python.\n",
      "---\n",
      " used storm for an automatic mechanism to analyze large amounts of non-unique data points with low latency and high throughput.\n",
      "---\n",
      " developed mapreduce jobs in python for data cleaning and data processing.\n",
      "---\n",
      " performed data profiling to learn about user behaviour and merged data from multiple data sources.\n",
      "---\n",
      " participated in all phases of data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " performed k-means clustering, multivariate analysis and support vector machines in python and r.\n",
      "---\n",
      " developed clustering algorithms and support vector machines that improved customer segmentation and market expansion.\n",
      "---\n",
      " professional tableau user (desktop, online, and server).\n",
      "---\n",
      " data story teller, mining data from different data source such as sql server, oracle, cube database, web analytics, business object and hadoop.\n",
      "---\n",
      " providing ad hoc analysis and reports to executive level management team.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, toad, business objects, power bi and smart view.\n",
      "---\n",
      " in unix development environment, for financial application reports used batch processes and models using perl and korn shell scripts with partitions and sub-partitions on oracle database.\n",
      "---\n",
      " developed analytics and strategy to integrate b2b analytics in outbound calling operations.\n",
      "---\n",
      " implemented analytics delivery on cloud-based visualization using shiny tool for business object and google analytics platform.\n",
      "---\n",
      " spoc data scientist and predictive analyst to create annual and quarterly business forecast reports.\n",
      "---\n",
      " main source of business regression report.\n",
      "---\n",
      " creating various b2b predictive and descriptive analytics using r and tableau.\n",
      "---\n",
      " creating and automating ad hoc reports.\n",
      "---\n",
      " responsible for planning & scheduling new product releases and promotional offers.\n",
      "---\n",
      " worked on nosql databases like cassandra.\n",
      "---\n",
      " experienced in agile methodologies and scrum process.\n",
      "---\n",
      " parsing data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging.\n",
      "---\n",
      " worked on text analytics, naive bayes, sentiment analysis, creating word clouds and retrieving data from twitter and other social networking platforms. extensive experience and proficiency in using sas ods to create output files in a variety of formats including rtf, html and pdf.\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r and python.\n",
      "---\n",
      " worked on the project from gathering requirements to developing the entire application. worked on anaconda python environment. created, activated and programmed in anaconda environment. wrote programs for performance calculations using numpy and sqlalchemy.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " used python modules of urllib, urllib2, requests for web crawling. experience using all these ml techniques: clustering, regression, classification, graphical models.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " involved in development of web services using soap for sending and getting data from the external interface in the xml format. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " worked on development of sql and stored procedures on mysql.\n",
      "---\n",
      " analyzed the code completely and have reduced the code redundancy to the optimal level.\n",
      "---\n",
      " design and build a text classification application using different text classification models.\n",
      "---\n",
      " used jira for defect tracking and project management.\n",
      "---\n",
      " worked on writing and as well as read data from csv and excel file formats.\n",
      "---\n",
      " involved in sprint planning sessions and participated in the daily agile scrum meetings.\n",
      "---\n",
      " conducted every day scrum as part of the scrum master role.\n",
      "---\n",
      " developed the project in linux environment.\n",
      "---\n",
      " worked on resulting reports of the application.\n",
      "---\n",
      " performed qa testing on the application.\n",
      "---\n",
      " held meetings with client and worked for the entire project with limited help from the client.\n",
      "---\n",
      " using python libraries for machine learning like pandas, numpy, matplotlib, sklearn, scipy to load the dataset, summarizing the dataset, visualizing the dataset, evaluating some algorithms and making some predictions.\n",
      "---\n",
      " perform application development, maintenance and ensure adherence to process and information security control.\n",
      "---\n",
      " aws (vpc, ec2, s3, route 53).\n",
      "---\n",
      " a key part of api design and development\n",
      "---\n",
      " unit test and debugging\n",
      "---\n",
      " maintain and extend the existing python/flask rest\n",
      "---\n",
      " a strong eye on code re-usability and maintainability\n",
      "---\n",
      " strong background in object-oriented programming, design patterns, algorithms, and data structures.\n",
      "---\n",
      " used python scripts to update the content in database and manipulate files.\n",
      "---\n",
      " generated python django forms to maintain the record of online users.\n",
      "---\n",
      " used django api's to access the database.\n",
      "---\n",
      " writing unit, functional, and integration test cases for cloud computing applications on aws.\n",
      "---\n",
      " writing python scripts with cloud formation templates to automate installation of auto scaling, ec2, vpc and other services.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and aws architecture.\n",
      "---\n",
      " developed restful api's using python flask and sql alchemy data models as well as ensured code.\n",
      "---\n",
      " designed and managed api system deployment using fast http server and amazon aws architecture.\n",
      "---\n",
      " designed and developed a horizontally scalable apis using python flask.\n",
      "---\n",
      " involved in back end development using python with framework flask.\n",
      "---\n",
      " wrote python modules to view and connect the apache cassandra instance.\n",
      "---\n",
      " created unit test/ regression test framework for working/new code.\n",
      "---\n",
      " responsible for designing, developing, testing, deploying and maintaining the web application.\n",
      "---\n",
      " wrote and executed various mysql database queries from python mysql connector and mysql db package.\n",
      "---\n",
      " involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications.\n",
      "---\n",
      " which is main source of data for customers and internal customer service team.\n",
      "---\n",
      " implemented soap/restful web services in json format.\n",
      "---\n",
      " attended many days to day meetings with developers and users and performed qa testing on the application.\n",
      "---\n",
      " environment: python, django, api, html, css, ajax, git, aws, apache http, flask, xml, ood, shell scripting, mysql, cassandra. \n",
      "---\n",
      " bachelor ece in jntuh university bachelor's skills clustering, data visualization, elasticsearch, hadoop, machine learning, \n",
      "---\n",
      " expertise: scikit-learn, nltk, spacy, numpy, scipy, opencv, deep learning, nlp, rnn, cnn, tensor flow, keras, matplotlib, microsoft visual studio, microsoft office. (8 years) additional information technical skills:\n",
      "---\n",
      " expertise: scikit-learn, nltk, spacy, numpy, scipy, opencv, deep learning, nlp, rnn, cnn, tensor flow, keras, matplotlib, microsoft visual studio, microsoft office.\n",
      "---\n",
      " machine learning algorithms: linear regression, logistic regression, decision trees, random forest, k-means clustering, support vector machines, gradient boost machines & xgboost, neural networks.\n",
      "---\n",
      " data analysis skills: data cleaning, data visualization, feature selection, pandas.\n",
      "---\n",
      " operating systems: windows, mac and linux, unix.\n",
      "---\n",
      " programming languages: python, sql, r, matlab, torch, c, c++, java ,octave, apache spark, hadoop, spark ml.\n",
      "---\n",
      " other programming knowledge and skills: elasticsearch, data scraping, restful-api using django web frame work.\n",
      "---\n",
      " tools: toad, erwin, aws, azure,d3, mule soft, alteryx, tableau, shiny, adobe analytics, anaconda\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with over 9 years of experience in data science and analytics including artificial intelligence/deep learning/machine learning, data mining and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including dataextraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, random forest, xgboost, knn, svm, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova\n",
      "---\n",
      " extensively worked on python 3.5/2.7 (numpy, pandas, matplotlib, nltk and scikit-learn)\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0jupiternotebook 4.x, r 3.0 (ggplot2, caret, dplyr) and excel[ ]\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sqlserver2008, nosql databases like mongodb3.2\n",
      "---\n",
      " developed api libraries and coded business logic using c#, xml and designed web pages using .net framework, c#, python, django, html, ajax\n",
      "---\n",
      " strong experience for over 5 years in image recognition and bigdata technologies like spark 1.6, sparksql, pyspark, hadoop 2.x, hdfs, hive 1.x\n",
      "---\n",
      " experience in visualization tools like, tableau9.x, 10.x for creating dashboards\n",
      "---\n",
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " proficient in predictive modeling, data mining methods, factor analysis, anova, hypotheticaltesting, normal distribution and other advanced statistical and econometric techniques.\n",
      "---\n",
      " developed predictive models using decision tree, randomforest, na\\xefvebayes, logisticregression, social network analysis, clusteranalysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with pythonscikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, numpy, scipy and pandas for dataanalysis.\n",
      "---\n",
      " worked with complex applications such as r, python, theano, h20, sas, matlab and spss to develop neural network, cluster analysis.\n",
      "---\n",
      " strong c#, sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing dataparsing, data ingestion, data manipulation,data architecture, data modelling and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experienced in visual basic for applications and vb programming languages c#, .net framework to work with developing applications.\n",
      "---\n",
      " worked with nosql database including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, hdfs, mapreduce, and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio ssis, ssas, ssrs.\n",
      "---\n",
      " proficient in tableau,adobe analytics and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql and python and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. work experience data scientist home depot - atlanta, ga march 2017 to present description: the home depot inc. or home depot is an american home improvement supplies retailing company that sells tools, construction products, and services. the company is headquartered at the atlanta store support center in unincorporated cobb county, georgia (with an atlanta mailing address). the mro company interline brands is also owned by the home depot with 70 distribution centers across the united states.the home depot is the largest home improvement retailer in the united states.\n",
      "---\n",
      " articulate and understand a business problem, identify challenges, formulate the machine learning problem or nlp problems and provide/prototype solutions\n",
      "---\n",
      " collect and manipulate large volumes of data\n",
      "---\n",
      " build new and improved techniques and/or solutions for data collection, management, and usage\n",
      "---\n",
      " performed data profiling to learn about behavior with various features of merchant and competitor details using python matplotlib.\n",
      "---\n",
      " using sql to extract data from edw in bigquery and customize according to project needs and built data scheduling jobs in airflowto transfer data into mysql\n",
      "---\n",
      " used numpy, scipy, pandas, nltk(natural language processing toolkit),matplotlib to build the model.\n",
      "---\n",
      " extracted data from hdfs using hive, presto and performed data analysis using spark with scala, pyspark, redshift and feature selection and created nonparametric models in spark\n",
      "---\n",
      " build several regression techniques to predict price recommendations for home depot products based on competitor data and home depot data\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frameworks.\n",
      "---\n",
      " used xgb classifier if the feature is an categorical variable and xgb regressor for continuous variables and combined it using feature union and function transformer methods of natural language processing.\n",
      "---\n",
      " used one vs rest classifier to fit each classifier against all other classifiers and used it on multiclass classification problems.\n",
      "---\n",
      " implemented application of various machine learning algorithms and statistical modeling like decision tree, text analytics, sentiment analysis, naive bayes, logistic regression and linear regression using python to determine the accuracy rate of each model.\n",
      "---\n",
      " environment: python 2.x,3.x, hive, linux, tableau desktop, microsoft excel, nlp, airflow, gcp, bigquery, kubernetes, docker, spark, pyspark, boosting algorithms etc. data scientist opera solutions, new jersey january 2016 to february 2017 description: opera solutions, llc is a technology and analytics company mainly focused on big data. the firm uses a combination of machine learning science, advanced predictive analytics, technology, large-scale data management, and human expertise. opera solutions delivers predictive analytics as a service, and offers hosted, cloud-based systems for specific business problems, e.g., predicting the behavior of individual consumers, stopping revenue leakage in hospitals, warning of threats to corporate security or brand health, etc.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features of usmle examinations of various student patterns using tableau, adobe analyticsand python matplotlib.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana etc\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " implemented statistical modeling with xgboost machine learning software package using python to determine the predicted probabilities of each model.\n",
      "---\n",
      " created master data for modelling by combining various tables and derived fields from client data and students lors, essays and various performance metrics.\n",
      "---\n",
      " formulated a basis for variable selection and gridsearch, kfold for optimal hyperparameters\n",
      "---\n",
      " utilized boosting algorithms to build a model for predictive analysis of student's behaviour who took usmle exam apply for residency.\n",
      "---\n",
      " used numpy, scipy, pandas, nltk(natural language processing toolkit),matplotlib to build the model.\n",
      "---\n",
      " formulated several graphs to show the performance of the students by demographics and their mean score in different usmle exams.\n",
      "---\n",
      " extracted data from hdfs using hive, presto and performed data analysis using spark with scala, pyspark, redshift and feature selection and created nonparametric models in spark\n",
      "---\n",
      " application of various artificial intelligence(ai)/machine learning algorithms and statistical modeling like decision trees,text analytics, image and text recognitionusing ocr tools like abbyy, natural language processing(nlp),supervised and unsupervised, regressionmodels.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frameworks.\n",
      "---\n",
      " created deep learning models using tensor flow and keras by combining all tests as a single normalized score and predict residency attainment of students.\n",
      "---\n",
      " used xgb classifier if the feature is an categorical variable and xgb regressor for continuous variables and combined it using feature union and function transformer methods of natural language processing.\n",
      "---\n",
      " used one vs rest classifier to fit each classifier against all other classifiers and used it on multiclass classification problems.\n",
      "---\n",
      " implemented application of various machine learning algorithms and statistical modeling like decision tree, text analytics, sentiment analysis, naive bayes, logistic regression and linear regression using python to determine the accuracy rate of each model.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior with cloud based products like azure ml studio and dataiku.\n",
      "---\n",
      " generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using signal hub and aws sage maker/azure data bricks.\n",
      "---\n",
      " created data layers as signals to signal hub to predict new unseen data with performance not less than the static model build using deep learning framework.\n",
      "---\n",
      " environment: python 2.x,3.x, hive, aws, linux, tableau desktop, microsoft excel, nlp, deep learning frameworks such as tensor flow, keras, boosting algorithms etc data scientist firstdata - atlanta, ga january 2015 to december 2015 description:first data corporation is a global payment processing company headquartered in atlanta, georgia, united states. the company's portfolio includes merchant transaction processing services\n",
      "---\n",
      " credit, debit, private-label, gift, payroll and other prepaid card offerings\n",
      "---\n",
      " fraud protection and authentication solutions.\n",
      "---\n",
      " provided configuration management and build support for more than 5 different applications, built and deployed to the production and lower environments.\n",
      "---\n",
      " implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using pyspark.\n",
      "---\n",
      " using air flow to keep track of job statuses in repositories like mysql and postgre databases.\n",
      "---\n",
      " explored and extracted data from source xml in hdfs, used etl for preparing data for exploratory analysis using data munging.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata, text mining and building models using topic analysis, sentiment analysis for both semi-structured and unstructured data.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " used r and python for exploratory data analysis, a/b testing, hql, vql, data lake, aws redshift, oozie, pyspark, anova test and hypothesis test to compare and identify the effectiveness of creative campaigns.\n",
      "---\n",
      " computing a/b testing frameworks, clickstream and time spent databases using airflow\n",
      "---\n",
      " created clusters to control and test groups and conducted group campaigns using text analytics.\n",
      "---\n",
      " created positive and negative clusters from merchant's transaction using sentiment analysisto test the authenticity of transactions and resolve any chargebacks.\n",
      "---\n",
      " analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n",
      "---\n",
      " created and developed classes and web page elements using c# and ajax. jsp was used for validating client side responses and connected c# to database to retrieve sql data\n",
      "---\n",
      " developed linux shell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using pl/sql\n",
      "---\n",
      " created various types of data visualizations using r, c#, python and tableau/spotfire also connected pipeline pilot with spotfire to create more interactive business driven layouts.\n",
      "---\n",
      " used python, r, sql, tensorflow to create statistical algorithms involving multivariate regression, linearregression, logisticregression, pca, image recognition,random forest models, decision trees, support vector machine for estimating the risks of welfare dependency.\n",
      "---\n",
      " identified and targeted welfare high-risk groups with machinelearning/deep learningalgorithms.\n",
      "---\n",
      " conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity-relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " created multiple custom sqlqueries in teradatasqlworkbench to prepare the right data sets for tableau dashboards\n",
      "---\n",
      " perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using sas programming.\n",
      "---\n",
      " environment: r 3.x, hdfs, c#,hadoop 2.3, pig, hive, linux, r-studio, tableau 10, sql server, ms excel, pyspark. data scientist bank of america - wilmington, de august 2012 to december 2014 description: bank of america is a multinational banking and financial services corporation. it is ranked 2nd on the list of largest banks in the united states by assets. as of 2016, bank of america was the 26th largest company in the united states by total revenue.\n",
      "---\n",
      " participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n",
      "---\n",
      " collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n",
      "---\n",
      " redefined many attributes and relationships and cleansed unwanted tables/columns using sql queries.\n",
      "---\n",
      " utilized spark sql api in pyspark to extract and load data and perform sql queries and also used c# connector to perform sql queries by creating and connecting to sql engine.\n",
      "---\n",
      " performed data imputation using scikit-learn package in python.\n",
      "---\n",
      " performed data processing using python libraries like numpy and pandas.\n",
      "---\n",
      " worked with data analysis using ggplot2 library in r to do data visualizations for better understanding of customers' behaviors.\n",
      "---\n",
      " implemented statistical modeling with xgboost machine learning software package using r to determine the predicted probabilities of each model.\n",
      "---\n",
      " delivered the results with operation team for better decisions.\n",
      "---\n",
      " environment: python, r, sql, tableau, spark, machine learning software package, recommendation systems. python developer cenvien technologies - hyderabad, telangana january 2011 to july 2012 description: cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products. it is highly qualified and strongly dedicated developing team that produces unique solutions.\n",
      "---\n",
      " developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " implemented the presentation layer with html, css and javascript.\n",
      "---\n",
      " involved in writing stored procedures using oracle.\n",
      "---\n",
      " optimized the database queries to improve the performance.\n",
      "---\n",
      " designed and developed data management system using oracle.\n",
      "---\n",
      " environment:mysql, oracle, html5, css3, javascript, shell, linux & windows, django, python programmer analyst pennar industries limited - hyderabad, telangana march 2009 to december 2010 description:as a backend developer of web applications and data science infrastructure. the main area of focus is to come up with comprehensive solutions that need massive capacity and throughput.\n",
      "---\n",
      " effectively communicated with the stakeholders to gather requirements for different projects\n",
      "---\n",
      " used mysql db package and python-mysql connector for writing and executing several mysql database queries from python.\n",
      "---\n",
      " implemented client/server applications using c++, c#, jsp and sql\n",
      "---\n",
      " created functions, triggers, views and stored procedures using my sql.\n",
      "---\n",
      " worked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " involved in the code review meetings.\n",
      "---\n",
      " environment: python, mysql, c#. \n",
      "---\n",
      " bachelor of computer science in tools and technologies informatica power centre skills android. (less than 1 year), application development (less than 1 year), bi (less than 1 year), data warehouse (less than 1 year), linux (6 years)\n",
      "\n",
      "---\n",
      "al services/smartlounge dublin, ca www.linkedin.com/in/alberttaijinlee\n",
      "---\n",
      " www.kaggle.com/atlee731 authorized to work in the us for any employer work experience freelance python developer / data scientist dublin, ca february 2017 to present working on a variety of projects, both on my own and through freelance websites. founder/teacher leen green math - dublin, ca 2012 to present established and taught an innovative, successful in-class math enrichment program for high achieving students\n",
      "---\n",
      " + provides differentiated instruction without imposing more work or time demands upon teachers or funding\n",
      "---\n",
      " + every grade level from k-4 has participated in the program at least one year\n",
      "---\n",
      " + heavily involved in the community, including chairperson for school site council founder/tutor solvur, supreme \n",
      "---\n",
      "al services/smartlounge 2002 to 2017 personally tutored over 1000 high school and college students in math and other\n",
      "---\n",
      " sciences, typically raising grades by one to two levels.\n",
      "---\n",
      " + grew business annually exclusively through referrals\n",
      "---\n",
      " + recruited and managed team of high quality tutors, many with phds and teaching\n",
      "---\n",
      " awards to their credit\n",
      "---\n",
      " + launched on-demand, online platform to satisfy global need for low-cost,\n",
      "---\n",
      " high-accessibility, and high-quality academic help school site council chairperson john green elementary 2014 to 2016 - organizer of math kangaroo contest\n",
      "---\n",
      " - also member of pfc instructor stanford online high school - palo alto, ca 2004 to 2006 also at diablo valley college (pleasant hill, ca),\n",
      "---\n",
      " city college of san francisco, quarry lane school (dublin, ca):\n",
      "---\n",
      " 2004 - 2006 and 2015 - 2017:\n",
      "---\n",
      " + taught high school math and college level physics, both in a classroom and in an online setting. \n",
      "---\n",
      " management of technology certificate haas school of business, uc-berkeley - berkeley, ca may 2004 m.s. in mechanical engineering university of california - berkeley, ca 1998 b.s. in engineering and applied science california institute of technology - pasadena, ca 1996 skills python, sql, teaching, data analysis links http://www.linkedin.com/in/alberttaijinlee\n",
      "\n",
      "---\n",
      " around 9+ years of it experience includes in data science (machine learning, deep learning, nlp/ text mining), data/business analytics, data visualization, data operations, and bi \n",
      "---\n",
      " a deep understanding of statistical modelling, multivariate analysis, bigdata analytics and standard procedures highly efficient in dimensionality reduction methods such as pca (principal component analysis), factor analysis etc. implemented bootstrapping methods such as random forests (classification), k-means clustering, knn, na\\xefve bayes, svm, decision tree, bfs, linear and logistic regression methods \n",
      "---\n",
      " experience in text understanding, classification, pattern recognition, recommendation systems, targeting systems and ranking systems using python \n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova, crosstabs, t tests and correlation techniques \n",
      "---\n",
      " worked with applications like r, spss and python to develop predictive models \n",
      "---\n",
      " experience with natural language processing (nlp) \n",
      "---\n",
      " extensively worked on python 3.5/2.7 (numpy, pandas, matplotlib, nltk and scikit-learn) \n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupyter notebook 4.x, r 3.0(ggplot2) and excel \n",
      "---\n",
      " worked on tableau to create dashboards and visualizations \n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python, power bi and tableau \n",
      "---\n",
      " hands on experience in business understanding, data understanding, and preparation of large databases \n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing datamining and reporting solutions that scales across massive volume of structured and unstructured data \n",
      "---\n",
      " documenting new data to help source to target mapping. also updating the documentation for existing data assisting with data profiling to maintain data sanitation, validation \n",
      "---\n",
      " identifies what data is available and relevant, including internal and external data sources, leveraging new data collection processes such geo-location information \n",
      "---\n",
      " good understanding on data preparation techniques \n",
      "---\n",
      " experienced working on large volume of data using base sas programming \n",
      "---\n",
      " proficiency in application of statistical prediction modeling, machine learning classification techniques and econometric forecasting techniques \n",
      "---\n",
      " proficiency in various type of optimization, market mix modeling, segmentation, time series, price promo models etc.\n",
      "---\n",
      " experience in the application of neural network, support vector machines (svm), and random forest \n",
      "---\n",
      " creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available \n",
      "---\n",
      " identifies/creates the appropriate algorithm to discover patterns, validate their findings using an experimental and iterative approach \n",
      "---\n",
      " applies advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems \n",
      "---\n",
      " closely works with product managers, service development managers, and product development team in productizing the algorithms developed \n",
      "---\n",
      " experience in designing star schema, snowflake schema for data warehouse, ods architecture \n",
      "---\n",
      " experience in designing stunning visualizations using tableau and power bi software and publishing and presenting dashboards, storyline on web and desktop platforms \n",
      "---\n",
      " experience in working with relational databases (teradata, oracle) with advanced sql programming skills \n",
      "---\n",
      " in-depth knowledge of statistical procedures that are applied in supervised / unsupervised problems \n",
      "---\n",
      " proficiency in sas (base sas, enterprise guide, enterprise miner) \n",
      "---\n",
      " familiar with graphical models and deep learning models, including deep learning frameworks such as tensorflow \n",
      "---\n",
      " experienced in working with advanced analytical teams to design, build, validate and refresh data models that enable the next generation of sophisticated solutions for global clients \n",
      "---\n",
      " excellent communication skills (verbal and written) to communicate with clients and team, prepare deliver effective presentations.\n",
      "---\n",
      " strong experience in software development life cycle (sdlc) including requirements analysis, design specification and testing as per cycle in both waterfall and agile methodologies.\n",
      "---\n",
      " strong experience in interacting with stakeholders/customers, gathering requirements through interviews, workshops, and existing system documentation or procedures, defining business processes, identifying and analyzing risks using appropriate templates and analysis tools.\n",
      "---\n",
      " mapping and tracing data from system to system in order to establish data hierarchy and lineage.\n",
      "---\n",
      " used data lineage and reverse engineering as a way to track back errors in data till the data source. authorized to work in the us for any employer work experience data scientist cvs health - scottsdale, az august 2018 to present cvs health is an american pharmacy and healthcare company with nearly 10,000 stores in its network. its health-focused business includes pharmacy services, retail, in-store health clinics, and its own digital innovation lab aimed at creating smart devices and apps to improve healthcare. involved in multiple projects with the primary objective of providing better services to the customers. performed behavioral analysis, personalization and customer targeting. data scientist sherwin williams - minneapolis, mn january 2017 to july 2018 minneapolis, mn jan 2017 - jul 2018\n",
      "---\n",
      " data scientist\n",
      "---\n",
      " the sherwin-williams company is an american fortune 500 company. it is a global leader in the manufacture, development, distribution, and sale of paints, coatings and related products to professional, industrial, commercial, and retail customers. the primary objective of the project is to perform predictive analytics using the production and client data to estimate the production quantities.\n",
      "---\n",
      " used the classification machine learning algorithms na\\xefve bayes, linear regression, logistic regression, svm, neural networks and used clustering algorithm k means.\n",
      "---\n",
      " analyzed business requirements and developed the applications, models, used appropriate algorithms for arriving at the required insights.\n",
      "---\n",
      " established partnerships with product and engineering teams and work closely with other teams.\n",
      "---\n",
      " work collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science.\n",
      "---\n",
      " used unsupervised (k-means, dbscan) and supervised learning techniques (regression, classification) for feature engineering and did principal component analysis for dimensionality reduction of features.\n",
      "---\n",
      " used spark-streaming apis to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from kafka in near real time.\n",
      "---\n",
      " worked on data cleaning, data preparation and feature engineering with python 3.x.\n",
      "---\n",
      " used ml toolkit - h2o for model fitting and to discover patterns in data.\n",
      "---\n",
      " used h20 for data visualization and for different machine learning algorithms.\n",
      "---\n",
      " used tensorflow library for development and evaluation of deep learning models.\n",
      "---\n",
      " performed text classification task using nltk package and implemented various natural language processing techniques.\n",
      "---\n",
      " used tableau for data visualization to create reports, dashboards for insights and business process improvement.\n",
      "---\n",
      " extensively used python's multiple data science packages like pandas, numpy, matplotlib, scipy, scikit-learn and nltk.\n",
      "---\n",
      " used tensorflow python api's to perform tensorflow graphs.\n",
      "---\n",
      " worked on spark python modules for machine learning and predictive analytics in spark on aws.\n",
      "---\n",
      " worked on end to end pipe line in spark.\n",
      "---\n",
      " explored and analyzed the customer specific features by using spark sql.\n",
      "---\n",
      " performed data imputation using scikit-learn package in python.\n",
      "---\n",
      " created the dashboards and reports in tableau for visualizing the data in required format.\n",
      "---\n",
      " collaborated with team members and translated functional requirements to technical requirements for development.\n",
      "---\n",
      " conducted code review for the fit gap done by the team members.\n",
      "---\n",
      " created hive scripts to create external, internal data tables on hive. worked on creating datasets to load data into hive.\n",
      "---\n",
      " environment: spark, apache spark, hive, machine learning, python, numpy, nltk, pandas, scipy, sql, tableau, hdfs, tableau, dynamodb, mongo db, sql server, and etl. data scientist tracfone - miami, fl march 2015 to december 2016 tracfone wireless, inc. is a prepaid mobile virtual network operator in the united states, puerto rico, and the us virgin islands. the objective of the project is to analyze the customer data, work on churn and build recommendation engines and automated customer scoring systems.\n",
      "---\n",
      " enhancing data collection procedures to include information that is relevant for building analytic systems processing, cleansing, and verifying the integrity of data used for analysis \n",
      "---\n",
      " doing ad-hoc analysis and presenting results in a clear manner \n",
      "---\n",
      " constant tracking of model performance \n",
      "---\n",
      " excellent understanding of machine learning techniques and algorithms, such as logistic regression, svm, random forests, deep learning etc.\n",
      "---\n",
      " worked with data governance, data quality, data lineage, data architect to design various models.\n",
      "---\n",
      " independently coded new programs and designed tables to load and test the program effectively for the given poc's.\n",
      "---\n",
      " extending company's data with third party sources of information when needed.\n",
      "---\n",
      " designed data models and data flow diagrams using erwin and ms visio.\n",
      "---\n",
      " developed implemented & maintained the conceptual, logical & physical data models using erwin for forward/reverse engineered databases.\n",
      "---\n",
      " experience with common data science toolkits, such as r, python, spark, etc.\n",
      "---\n",
      " good applied statistics skills, such as statistical sampling, testing, regression, etc.\n",
      "---\n",
      " build analytic models using a variety of techniques such as logistic regression, risk scorecards and pattern recognition technologies.\n",
      "---\n",
      " analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data, create variables, build models and test those models.\n",
      "---\n",
      " work with technical and development teams to deploy models. build model performance reports and \n",
      "---\n",
      " modeling technical documentation to support each of the models for the product line.\n",
      "---\n",
      " developed new reports in sas using sas ods, proc report, proc tabulate, and proc sql.\n",
      "---\n",
      " imported data from relational database into sas files per detailed specifications.\n",
      "---\n",
      " responsible for the development and maintenance of sas information maps for analytics and business forecasting team.\n",
      "---\n",
      " performed exploratory data analysis and data visualizations using r, and tableau.\n",
      "---\n",
      " perform a proper eda, univariate and bi-variate analysis to understand the intrinsic effect/combined.\n",
      "---\n",
      " established data architecture strategy, best practices, standards, and roadmaps.\n",
      "---\n",
      " lead the development and presentation of a data analytics data-hub prototype with the help of the other members of the emerging solutions team.\n",
      "---\n",
      " experience in sas programming for auditing data, developing data, performing data validation qa and improve efficiency of sas programs.\n",
      "---\n",
      " involved in analysis of business requirement, design and development of high level and low-level designs, unit and integration testing.\n",
      "---\n",
      " worked with several r packages including knitr, dplyr, sparkr, causalinfer, spacetime.\n",
      "---\n",
      " interacted with the other departments to understand and identify data needs and requirements.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop, nosql, teradata, oltp, random forest, olap, hdfs, ods data science analyst kindred health - louisville, ky april 2013 to february 2015 kindred healthcare incorporated is a healthcare services company that operates hospitals, nursing centers, and contract rehabilitation services across the united states. the project involved in predictive analysis and trend analysis using the patient historical data.\n",
      "---\n",
      " extracted, transformed and loaded data from given source to analysis.\n",
      "---\n",
      " hands-on implementation of r, python, hadoop, tableau and sas to extract and import data.\n",
      "---\n",
      " had a pleasure working experience on spark (spark streaming, spark sql), scala and kafka. also converting hive/sql queries into spark transformations using spark rdds and scala.\n",
      "---\n",
      " used kafka to load data in to hdfs and move data into nosql databases.\n",
      "---\n",
      " worked on spark sql and data frames for faster execution of hive queries using spark sql context.\n",
      "---\n",
      " gained extreme knowledge on map reduce using python, hive queries using oozie workflows.\n",
      "---\n",
      " performed data cleaning process applied backward - forward filling methods on dataset for handling missing values.\n",
      "---\n",
      " design built and deployed a set of python modelling apis for customer analytics, which integrate multiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs.\n",
      "---\n",
      " segmented the customers based on demographics using k-means clustering.\n",
      "---\n",
      " explored different regression and ensemble models in machine learning to perform forecasting.\n",
      "---\n",
      " used classification techniques including random forest and logistic regression to quantify the likelihood of each user referring.\n",
      "---\n",
      " performed boosting method on predicted model for the improve efficiency of the model.\n",
      "---\n",
      " environment: r/r studio, informatica, sql/plsql, oracle 10g, ms-office, teradata. data analyst suntrust bank - atlanta, ga october 2011 to march 2013 worked for credit card management team and involved in calculating the credit card performance by dividing the data into different datamart's and performing analysis on the data and credit risk calculation for the customers and lending money (mortgaging).\n",
      "---\n",
      " the team of data analysts focused on providing analytics insights and decision support tools for executives for accurate decision making.\n",
      "---\n",
      " applied highly advanced data access routines to extract data from source systems for monitoring operations compliance to banking laws, rules and regulations using visual basic apps (vba), sql server ssis, sas and sql.\n",
      "---\n",
      " identified, measured and recommended improvement strategies for kpis across all business areas.\n",
      "---\n",
      " assisted in defining, implementing, and utilizing business metrics calculations and methodologies.\n",
      "---\n",
      " designed and provided complex excel reports including summaries, charts, and graphs to interpret findings to team and stakeholders.\n",
      "---\n",
      " assisted the team for standardization of reports using sas macros and sql.\n",
      "---\n",
      " responsible for creation of credit data related warehouse to help with risk assessment for commercial loans.\n",
      "---\n",
      " performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis.\n",
      "---\n",
      " created macros and used existing macros to develop sas programs for data analysis.\n",
      "---\n",
      " created and manipulated various management reports in ms excel for sales metrics using vlookup and pivot tables.\n",
      "---\n",
      " developed transformation logic for bi tools (informatica) for data transformation into various layers in data warehouse.\n",
      "---\n",
      " utilized sql to develop stored procedures, views to create result sets to meet varying reporting requirements.\n",
      "---\n",
      " used advanced excel formulas (lookup functions, pivot table, if statements etc.) for analyzing data.\n",
      "---\n",
      " identified process improvements that significantly reduce workloads or improve quality.\n",
      "---\n",
      " worked for bi analytics team to conduct a/b testing, data extraction and exploratory analysis.\n",
      "---\n",
      " generated dashboards and presented the analysis to researchers explaining insights on the data.\n",
      "---\n",
      " environment: excel 2010, r, informatica power center 9.0, ms sql server 200. data analyst/ python developer teleparadigm networks private limited may 2009 to september 2011 teleparadigm networks delivers software and it services to clientele across the globe under service models. have a wide client base in healthcare and telecom industries.\n",
      "---\n",
      " brought in and implemented updated analytical methods such as regression modelling, classification tree, statistical tests and data visualization techniques with python.\n",
      "---\n",
      " analyzed customer help data, contact volumes, and other operational data in mysql to provide insights that enable improvements to help content and customer experience.\n",
      "---\n",
      " maintained and updated existing automated solutions.\n",
      "---\n",
      " improved data collection and distribution processes by using pandas and numpy packages in python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics.\n",
      "---\n",
      " analysed historical demand, filter out outliers/exceptions, identify the most appropriate statistical forecasting algorithm, develop base plan, understand variance, propose improvement opportunities, and incorporate demand signal into forecast and executed data visualization by using plotly package in python.\n",
      "---\n",
      " interacted with qa to develop test plans from high-level design documentation.\n",
      "---\n",
      " environment: mysql, statistical modelling, python libraries, pandas and numpy packages \n",
      "---\n",
      " bachelor's in feature engineering and selection stanford ner tagger skills apache hadoop hdfs (5 years), hadoop distributed file system (5 years), hdfs (5 years), python (7 years), sql (6 years)\n",
      "\n",
      "---\n",
      " 7+ years of data science experience in architecting and building comprehensive analytical solutions in marketing, sales and operations functions across technology, banking, manufacturing, healthcare and retail industries.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python.\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data.\n",
      "---\n",
      " expert knowledge in supervised and unsupervised learning algorithms such as ensemble methods (random forests), logistic regression, regularized linear regression, svms, deep neural networks, extreme gradient boosting, decision trees, kmeans, gaussian mixture models, hierarchical models, and time series models (arima, garch, varch etc.)\n",
      "---\n",
      " expertise writing production quality code in sql, r, python and spark. hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments.\n",
      "---\n",
      " familiar with predictive models using classification algorithms like knn, naive base, regression and decision trees.\n",
      "---\n",
      " familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks, and ensemble methods like bagging, boosting and random forest to improve the efficiency of the predictive model.\n",
      "---\n",
      " worked on text mining and sentimental analysis for extracting the unstructured data from various social media platforms like facebook, twitter and reddit.\n",
      "---\n",
      " ability to translate analytic ideas into r/ python production quality scripts\n",
      "---\n",
      " strong background & hands-on in data science, big data, data structures, statistics, algorithms like regression, classification etc.\n",
      "---\n",
      " strong background & hands-on of supervised learning (decision trees, random forest, logistic regression, svms, gbm, etc) and unsupervised learning (k-means, knn).\n",
      "---\n",
      " strong background & hands-on of deep learning using tensforflow, keras, theano, h2o.\n",
      "---\n",
      " sound understanding of deep learning using cnn, rnn, ann, reinforcement learning, transfer learning.\n",
      "---\n",
      " strong background & hands-on in natural language processing and text analytics.\n",
      "---\n",
      " experience and passion for solving analytical problems involving big data sets using quantitative approaches to generate insights from data.\n",
      "---\n",
      " identify, analyze, and interpret trends or patterns in complex data sets. strong in predictive and prescriptive analytics approaches and experienced in operating tools like r and in programming using python.\n",
      "---\n",
      " has working experience in data science, machine learning implementation in cloud platforms like google cloud platform, aws, azure, bluemix.\n",
      "---\n",
      " familiar with predictive models using different cloud based machine learning tools like microsoft azure ml.\n",
      "---\n",
      " oversee all activities related to data cleansing, data quality and data consolidation using industry standards and processes.\n",
      "---\n",
      " perform exploratory data analysis\n",
      "---\n",
      " generate and test working hypothesis\n",
      "---\n",
      " and, prepare and analyze historical data and identify patterns\n",
      "---\n",
      " work closely with cross functional teams to encourage best practices for experimental design and data analysis.\n",
      "---\n",
      " proficient with python 3.x including numpy, scikit-learn, pandas, matplotlib and seaborn.\n",
      "---\n",
      " extensive experience in rdbms such as sql server 2012, mysql 5.x.\n",
      "---\n",
      " experienced in non-relational database such as mongodb 3.x.\n",
      "---\n",
      " experienced in hadoop 2.x ecosystem and apache spark 2.x framework such as hive, pig, pyspark.\n",
      "---\n",
      " proficient at data visualization tools such as tableau, power bi, python matplotlib and seaborn.\n",
      "---\n",
      " experienced in amazon web services (aws) and microsoft azure, such as aws ec2, s3, rd3, azure hdinsight, machine learning studio, azure data lake.\n",
      "---\n",
      " apply various data modeling techniques to model user behavior and identify actionable levers for retaining and growing users.\n",
      "---\n",
      " define key metrics, conduct a/b testing, and oversee statistical measurement of new algorithms and approaches.\n",
      "---\n",
      " expert at distilling questions, wrangling data, and driving decisions with data analytics.\n",
      "---\n",
      " strong knowledge of relational databases and ability to write sql code at an expert level.\n",
      "---\n",
      " aptitude with numbers, intellectual curiosity about metrics and measuring impact.\n",
      "---\n",
      " present models, findings and insights to senior management to catalyze business decisions.\n",
      "---\n",
      " ability to solve complex problems by applying analytical techniques and predictive models to massive data sets, and translate business needs into mathematical abstractions for algorithms to solve.\n",
      "---\n",
      " research and prototype models and pipelines, and work with engineers to put them into production at scale.\n",
      "---\n",
      " raw data analysis such as assessing quality, cleansing, structuring for downstream processing.\n",
      "---\n",
      " use mathematical, statistical, and programmatic knowledge to spec out, design, and build first-class predictive models about customer behavior.\n",
      "---\n",
      " design and prototype of accurate and scalable prediction algorithms.\n",
      "---\n",
      " collaboration with engineering team to bring analytical prototypes to production.\n",
      "---\n",
      " creative and pragmatic quantitatively minded individual with a passion for understanding location and human behavior.\n",
      "---\n",
      " ability to identify issues quickly and rapidly determine root cause and effective resolution approach.\n",
      "---\n",
      " very solid data analysis skills including application of analytical techniques such as statistical and machine learning.\n",
      "---\n",
      " fundamental coding skills enabling the building of analytical pipelines and the development of prototypes for core company products and systems.\n",
      "---\n",
      " excellent verbal and written communication skills, ability to communicate technical topics to non-technical individuals.\n",
      "---\n",
      " ability to manage own time and work effectively with others on projects. work experience data scientist cuna mutual group - madison, wi april 2017 to present this project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims.\n",
      "---\n",
      " claims severity prediction in real-time\n",
      "---\n",
      " built classification models to predict the fraudulent claims by severity in real-time reducing the time for execution from 6 hours to 4 seconds. implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products. forwarded the high-risk claims for further investigation.\n",
      "---\n",
      " text analytics for fraud prediction\n",
      "---\n",
      " executed topic modelling for finding different topics based on the notes made corresponding to the claimant's claim. attributed the resulting topics to classify into fraud and not fraud categories.\n",
      "---\n",
      " risk assessment prediction\n",
      "---\n",
      " incorporated models built in python and r into the business processes using clustering techniques to assess the risk involved with a customer. the models built are used in assessing the premiums amount required to be paid by the customer.\n",
      "---\n",
      " customer churn/attrition prediction\n",
      "---\n",
      " developed models that predict whether a customer's propensity to churn leveraging the information related to insurance policies, demographics, claims, related to the customer, payment frequency, home ownership status, household tenure etc.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " analyze data and performed data preparation by applying historical model on the data set in azure ml.\n",
      "---\n",
      " perform data cleaning process applied backward - forward filling methods on dataset for handling missing value\n",
      "---\n",
      " perform data transformation method for rescaling and normalizing variables.\n",
      "---\n",
      " develop a predictive model and validate knn model for predict the feature label.\n",
      "---\n",
      " plan, develop, and apply leading-edge analytic and quantitative tools and modeling techniques to help clients gain insights and improve decision-making.\n",
      "---\n",
      " leverage the most appropriate algorithms and be prepared to justify your decisions.\n",
      "---\n",
      " work closely with key stakeholders in product, finance and operations to form deep understanding of growth and marketplace dynamics, including product and pricing patterns, outlier detection, forecasting, and imputation.\n",
      "---\n",
      " collaborate with product and engineering to integrate various sources of data.\n",
      "---\n",
      " apply strict sampling, statistical inference, and survey techniques to derive insights from small samples of data.\n",
      "---\n",
      " utilize sqoop to ingest real-time data. used analytics libraries sci-kit learn, mllib and mlxtend.\n",
      "---\n",
      " extensively use python's multiple data science packages like pandas, numpy, matplotlib, seaborn, scipy, scikit-learn and nltk.\n",
      "---\n",
      " performed exploratory data analysis, trying to find trends and clusters.\n",
      "---\n",
      " develop rigorous data science models to aggregate inconsistent real-time signals into strong predictors of market trends.\n",
      "---\n",
      " automate and own the end-to-end process of modeling and data visualization.\n",
      "---\n",
      " collaborate with data engineers and software developers to develop experiments and deploy solutions to production.\n",
      "---\n",
      " work on data that was a combination of unstructured and structured data from multiple sources and automate the cleaning using python scripts.\n",
      "---\n",
      " extensively perform large data read/writes to and from csv and excel files using pandas.\n",
      "---\n",
      " tasked with maintaining rdd's using sparksql.\n",
      "---\n",
      " communicate and coordinate with other departments to collection business requirement.\n",
      "---\n",
      " tackle highly imbalanced fraud dataset using under sampling with ensemble methods, oversampling and cost sensitive algorithms.\n",
      "---\n",
      " improved fraud prediction performance by using random forest and gradient boosting for feature selection with python scikit-learn.\n",
      "---\n",
      " implemented machine learning model (logistic regression, xgboost) with python scikit- learn.\n",
      "---\n",
      " optimize algorithm with stochastic gradient descent algorithm fine-tuned the algorithm parameter with manual tuning and automated tuning such as bayesian optimization.\n",
      "---\n",
      " write research reports describing the experiment conducted, results, and findings and also make strategic recommendations to technology, product, and senior management. built executive banco santander, s.a 2000 to present doing business as santander group, is a spanish banking group. as its name suggests, the company originated in santander, spain. the group has expanded since 2000 through a number of acquisitions, with operations across europe, south america, north america and asia. santander has been ranked as 37th in the forbes global 2000 list of the world's biggest public companies. santander is spain's largest bank.\n",
      "---\n",
      " key projects\n",
      "---\n",
      " credit history predictive modeling:\n",
      "---\n",
      " analyzed and predicted the customer's credit history and past bill payments based on the credit card offers, to create a predictive model, and send offers to customers on the base of model and past data. a system was successfully created on the past data of credit history and payments activity of customers\n",
      "---\n",
      " ran model against the historical data and get predicted label if customers were eligible for credit card offer, and on that basis send them an offer. as a result, customers actually got an offer they liked and increased the number of offer acceptance, which lead to profit for the bank.\n",
      "---\n",
      " forecasting loan balance:\n",
      "---\n",
      " forecasted bank-wide loan balances under normal and stressed macroeconomic scenarios using r. performed variable reduction using the stepwise, lasso, and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques.\n",
      "---\n",
      " top down models (commercial real estate):\n",
      "---\n",
      " automated the scraping and cleaning of data from various data sources in r and python. developed bank's loss forecasting process using relevant forecasting and regression algorithms in r. the projected losses under stress conditions helped bank reserve enough funds per dfast policies.\n",
      "---\n",
      " loan payment default prediction:\n",
      "---\n",
      " built classification models using several features related to customer demographics, macroeconomic dynamics, historic payment behavior, type and size of loan, credit scores and loan to value ratios and with accuracy of 95% accuracy the model predicted the likelihood of default under various stressed conditions.\n",
      "---\n",
      " marketing campaign measurement:\n",
      "---\n",
      " built executive dashboards in tableau that measured changes in customer behavior post campaign launch\n",
      "---\n",
      " the roi measurements helped to strategically select the effective campaigns.\n",
      "---\n",
      " credit risk scorecards:\n",
      "---\n",
      " built credit risk scorecards and marketing response models using sql and sas. evangelized the complex technical analysis into easily digestible reports for top executives in the company. developed several interactive dashboards in tableau to visualize nearly 5 terabytes of credit data by designing a scalable data cube structure.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " gathered, analyzed, documented and translated application requirements into data models, supported standardization of documentation and the adoption of standards and practices related to data and applications.\n",
      "---\n",
      " queried and aggregated data from amazon redshift to get the sample dataset.\n",
      "---\n",
      " identified patterns, data quality issues, and leveraged insights by communicating with bi team.\n",
      "---\n",
      " in preprocessing phase, used pandas to remove or replace all the missing data, and feature engineering to eliminate unrelated features.\n",
      "---\n",
      " balanced the dataset with over-sampling the minority label class and under-sampling the majority label class.\n",
      "---\n",
      " in data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data.\n",
      "---\n",
      " tested classification algorithms such as logistic regression, gradient boosting and random forest using pandas and scikit-learn and evaluated the performance.\n",
      "---\n",
      " implemented, tuned and tested the model on aws ec2 with the best algorithm and parameters.\n",
      "---\n",
      " set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data.\n",
      "---\n",
      " deployed the model on aws lambda, collaborated with develop team to build the business solutions.\n",
      "---\n",
      " collected the feedback after deployment retrained the model to improve the performance.\n",
      "---\n",
      " discovered flaws in the methodology being used to calculate weather peril zone relativities\n",
      "---\n",
      " designed and implemented a 3d algorithm based on k-means clustering and monte carlo methods.\n",
      "---\n",
      " observed groups of customers being neglected by the pricing algorithm\n",
      "---\n",
      " used hierarchical clustering to improve customer segmentation and increase profits by 6%.\n",
      "---\n",
      " designed, developed and maintained daily and monthly summary, trending and benchmark reports in tableau desktop.\n",
      "---\n",
      " environment: aws ec2, s3, redshift, lambda, linux, python (scikit-learn/numpy/pandas/matplotlib), machine learning (logistic regression/gradient boosting/random forest), tableau. data scientist santander - boston, ma march 2016 to april 2017 data scientist banco santander, s.a - austin, tx march 2014 to december 2015 whole foods market inc. is an american supermarket chain that specializes in selling organic foods products without artificial additive products for growing foods, colors, flavors, sweeteners, and hydrogenated fats. it has 473 stores in north america and the united kingdom.\n",
      "---\n",
      " key projects\n",
      "---\n",
      " customer purchase propensity modelling:\n",
      "---\n",
      " built machine learning based regression models using scikit-learn python frameworks to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in, revenue, historic purchases, frequency and regency behaviours. these predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients.\n",
      "---\n",
      " coupon recommender system:\n",
      "---\n",
      " developed a personalized coupon recommender system using recommender algorithms (collaborative filtering, low rank matrix factorization) that recommended best offers to a user based on similar user profiles. the recommendations enabled users to engage better and helped improving the overall customer retention rates.\n",
      "---\n",
      " outlier /anomalous pattern detection:\n",
      "---\n",
      " created interactive dashboard suite that illustrated outlier characteristics across several sales-related dimensions and overall impact of outlier imputation in r (shiny). used iterative outlier detection and imputation algorithm using multiple density-based clustering techniques (dbscan, kernel density estimation).\n",
      "---\n",
      " cross sell and upsell opportunity analysis:\n",
      "---\n",
      " implemented market basket algorithms from transactional data, which helped identify coupons used/purchased together frequently. discovering frequent coupon sets helped unearth cross sell and up selling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams.\n",
      "---\n",
      " forecast process innovations:\n",
      "---\n",
      " forecast sales and improved accuracy by 10-20% by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates. increased accuracy helped business plan better with respect to budgeting and sales and operations planning.\n",
      "---\n",
      " price elasticity analysis:\n",
      "---\n",
      " measured the price elasticity for products that experienced price cuts and promotions using regression methods\n",
      "---\n",
      " based on the elasticity, whole foods made selective and cautious price cuts for certain categories.\n",
      "---\n",
      " customer churn prediction:\n",
      "---\n",
      " predicted the likelihood of customer churn based on customer attributes like customer size, rfm loyalty metrics, revenue, type of industry, competitor products and growth rates etc. the models deployed in production environment helped detect churn in advance and aided sales/marketing teams plan for various retention strategies in advance like price discounts, custom licensing plans etc.\n",
      "---\n",
      " working as project technical lead, paired with business lead in scoping, researching, and assessing project feasibility, outcomes, and product deliverables on knowledge graph project (hadoop ecosystem, spark, elasticsearch, janusgraph, etc.).\n",
      "---\n",
      " working with business lead, successfully launched end-to-end knowledge graph mvp in 7 months' time whereas previous attempts at knowledge graph project had languished for 3-4 years.\n",
      "---\n",
      " using nlp and ml built and maintained a variety of cloud-based natural language web scrapers and parsers to augment existing data sources: python (numpy, scipy, spacy, etc.), spark, tensorflow, jupyter notebooks.\n",
      "---\n",
      " mentored junior teammates, shared knowledge of nlp and information retrieval best practices, and performed code reviews on a weekly basis.\n",
      "---\n",
      " prototyped, conducted, and reported on data science experiments (to technical and non-technical audiences) using supervised, semi-supervised and unsupervised learning techniques: anomaly detection, named entity recognition, ontology creation, etc.\n",
      "---\n",
      " using python and nltk, aggregated natural language data from online documents and developed pipelines to ingest scraped data into relational databases.\n",
      "---\n",
      " performed ad-hoc data science analyses for departments across the business using r, python, natural language toolkit (nltk), machine learning. analyst/data scientist banco santander, s.a - hyderabad, telangana april 2012 to march 2014 bharti axa general insurance company ltd is a joint venture between bharti enterprises, a leading indian business group and axa, a world leader in financial protection. bharti axas offers insurance coverage across various categories - motor, health , travel , home, student travel and more. the objective was to load data, analyze, and provide monthly reports for the predictions on a claim's potential of a third-party recovery. tableau and ssrs were used to build claim and recovery reports.\n",
      "---\n",
      " assembled a predictive modeling module by using supervised learning for subrogation claim prediction to identify which claims would be classified as having subrogation potential.\n",
      "---\n",
      " implemented models such as logistic regression and na\\xefve bayes, in python using scikit-learn, to predict the claim potential outcome.\n",
      "---\n",
      " dimensionality reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results.\n",
      "---\n",
      " gathered requirements and business rules from business users to implement predictive modeling.\n",
      "---\n",
      " designed and developed etl packages using ssis to create data warehouses from different tables and file sources like flat and excel files, with different methods in ssis such as derived columns, aggregations, merge joins, count, conditional split and more to transform the data.\n",
      "---\n",
      " designed reporting solutions for different stakeholders from mock-up till deployment in different areas such as potential subrogation claims, monthly revenue from subrogation & transactions.\n",
      "---\n",
      " performed data visualization and designed dashboards with tableau, and provided complex reports, including charts, summaries, and graphs to interpret the findings for adjustors to view various claim information.\n",
      "---\n",
      " optimized queries in t-sql by removing unnecessary columns and redundant data, normalized tables, established joins and indices\n",
      "---\n",
      " developed complex sql queries, stored procedures, views, functions and reports that meet customer requirements.\n",
      "---\n",
      " worked on development of customer support and complains registration system. this is a customer feedback and complains management system.\n",
      "---\n",
      " design, develop, test, deploy and maintain the website.\n",
      "---\n",
      " coding and execution of scripts in python/unix/vb.\n",
      "---\n",
      " development of application using java and python.\n",
      "---\n",
      " recording of scripts (web, web services html) using vugen and soapui and script validation through co correlations, parameterizations and other methods. scripting- web and web services.\n",
      "---\n",
      " data set up using sql/oracle/teradata.\n",
      "---\n",
      " resolving complexity in the scripts of the website due to the complex logic and correlations.\n",
      "---\n",
      " script validation sometimes becomes challenging as it demanded many web based logic rather than correlation and parameterization.\n",
      "---\n",
      " running load/endurance tests using vugen, alm and controller, server monitoring, analysis using dynatrace, unix putty, sql logs and other tools and reporting the performance. analyzing errors and exceptions using putty logs (unix), etc.\n",
      "---\n",
      " testing in citrix protocol with scripts and scenario.\n",
      "---\n",
      " execution of batch jobs in control m, perfmon and other tools.\n",
      "---\n",
      " scripting and validation of scripts through correlation, parameterization and web based logic testing (smoke test, load test, endurance) using controller for a duration further analysis, checking response times, cpu utilizations, memory leaks of servers and other performance characteristics of the website through capturing perfmon logs and creating papal reports and creating test reports.\n",
      "---\n",
      " designed and developed data management system using mysql.\n",
      "---\n",
      " rewrite existing python/django/java module to deliver certain format of data.\n",
      "---\n",
      " used django database api's to access database objects.\n",
      "---\n",
      " wrote python scripts to parse xml documents and load the data in database.\n",
      "---\n",
      " generated property list for every application dynamically using python.\n",
      "---\n",
      " responsible for search engine optimization (seo) to improve the visibility of the website.\n",
      "---\n",
      " handled all the client side validation using javascript.\n",
      "---\n",
      " creating unit test/regression test framework for working/new code.\n",
      "---\n",
      " using subversion version control tool to coordinate team-development.\n",
      "---\n",
      " responsible for debugging and troubleshooting the web application.\n",
      "---\n",
      " environment: python, putty, sql, teradata, soapui, controlm, perfmon, mysql, linux, html, xhtml, css, ajax, javascript, apache web server. \n",
      "---\n",
      " bachelor of technology in computer science & engineering sreenidhi institute of science & technology - hyderabad, telangana skills algorithms (10+ years), bi (10+ years), business intelligence (10+ years), linux (10+ years), logistic regression (10+ years) additional information technical skills\n",
      "---\n",
      " languages java 8, python, r\n",
      "---\n",
      " python and r\n",
      "---\n",
      " numpy, scipy, pandas, scikit-learn, matplotlib, seaborn, ggplot2, caret, dplyr, purrr, readxl, tidyr, rweka, gmodels, rcurl, c50, twitter, nlp, reshape2, rjson, plyr, beautiful soup, rpy2\n",
      "---\n",
      " algorithms\n",
      "---\n",
      " kernel density estimation and non-parametric bayes classifier, k-means, linear regression, neighbors (nearest, farthest, range, k, classification), non-negative matrix factorization, dimensionality reduction, decision tree, gaussian processes, logistic regression,\n",
      "---\n",
      " na\\xefve bayes, random forest, ridge regression, matrix factorization/svd\n",
      "---\n",
      " nlp/machine learning/deep learning\n",
      "---\n",
      " lda (latent dirichlet allocation), nltk, apache opennlp, stanford nlp, sentiment analysis, svms, ann, rnn, cnn, tensorflow, mxnet, caffe, h2o, keras, pytorch, theano, azure ml\n",
      "---\n",
      " cloud google cloud platform, aws, azure, bluemix\n",
      "---\n",
      " web technologies jdbc, html5, dhtml and xml, css3, web services, wsdl\n",
      "---\n",
      " data modelling tools erwin r 9.6, 9.5, 9.1, 8.x, rational rose, er/studio, ms visio, sap power designer\n",
      "---\n",
      " big data technologies hadoop, hive, hdfs, mapreduce, pig, kafka\n",
      "---\n",
      " databases sql, hive, impala, pig, spark sql, databases sql-server, my sql, ms access, hdfs, hbase, teradata, netezza, mongodb, cassandra.\n",
      "---\n",
      " reporting tools\n",
      "---\n",
      " ms office (word/excel/power point/ visio), tableau, crystal reports xi, business intelligence, ssrs, business objects 5.x/ 6.x, cognos7.0/6.0.\n",
      "---\n",
      " etl tools informatica power centre, ssis.\n",
      "---\n",
      " version control tools svm, github\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " tableau, tableau server, tableau reader, sap business objects, obiee, qlikview, sap business intelligence, amazon redshift, or azure data warehouse\n",
      "---\n",
      " operating system windows, linux, unix, macintosh hd, red hat\n",
      "\n",
      "---\n",
      " perform exploratory analysis, hypothesis testing, cluster analysis, correlation, anova, roc curve and build models in supervised and unsupervised machine learning algorithms, text analytics & time series forecasting\n",
      "---\n",
      " prepared the model data and built machine learning algorithms using python pandas, scikit learn, numpy, keras etc. libraries using anaconda jupyter & programming linear, logistic regressions, knn, k-means clustering, sentiment/text analytics, nlp, na\\xefve bayes, time series forecasting using lm, glm, arima, apriori, forecast.\n",
      "---\n",
      " extracting data from big data hadoop data lake, excel, analyzing, cleaning, sorting, merging reporting and creating dashboards using base sas, sas macros, sql, hive, sas va, sas, and excel.\n",
      "---\n",
      " developing mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws. implemented a python-based distributed random forest via python streaming.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " currently working on building clustering and predictive models using mllib to predict fault code occurrences using spark and mllib.\n",
      "---\n",
      " conducting studies, rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data.\n",
      "---\n",
      " demonstrated experience in design and implementation of statistical models, predictive models, enterprise data model, metadata solution and data life cycle management in both rdbms, big data environments.\n",
      "---\n",
      " stored and retrieved data from data-warehouses using amazon redshift and designed and implemented system architecture for amazon ec2 based cloud-hosted solution for client.\n",
      "---\n",
      " developed simple to complex map reduce jobs using hive and pig and developed multiple map reduce jobs in java for data cleaning and preprocessing.\n",
      "---\n",
      " analyzing large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n",
      "---\n",
      " developed map reduce/spark python modules for machine learning & predictive analytics in hadoop on aws.\n",
      "---\n",
      " worked with various teradata15 tools and utilities like teradata viewpoint, multi load, arc, teradata administrator, bteq and other teradata utilities.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, spark streaming, caffe, tensorflow, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " developed linux shell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " working on information extraction from different kinds of text documents using nlp, text mining and regular expressions.\n",
      "---\n",
      " developed map reduce/spark python modules for machine learning & predictive analytics in hadoop on aws. implemented a python-based distributed random forest via python streaming.\n",
      "---\n",
      " perform exploratory analysis, hypothesis testing, cluster analysis, correlation, anova, roc curve and build models in supervised and unsupervised machine learning algorithms, text analytics & time series forecasting\n",
      "---\n",
      " prepared the model data and built machine learning algorithms using python pandas, scikit learn, numpy, keras etc. libraries using anaconda jupyter & programming linear, logistic regressions, knn, k-means clustering, sentiment/text analytics, nlp, na\\xefve bayes, time series forecasting using lm, glm, arima, apriori, forecast.\n",
      "---\n",
      " extracting data from big data hadoop data lake, excel, analyzing, cleaning, sorting, merging reporting and creating dashboards using base sas, sas macros, sql, hive, sas va, sas, and excel.\n",
      "---\n",
      " developing mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws. implemented a python-based distributed random forest via python streaming.\n",
      "---\n",
      " used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " currently working on building clustering and predictive models using mllib to predict fault code occurrences using spark and mllib.\n",
      "---\n",
      " conducting studies, rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data.\n",
      "---\n",
      " demonstrated experience in design and implementation of statistical models, predictive models, enterprise data model, metadata solution and data life cycle management in both rdbms, big data environments.\n",
      "---\n",
      " stored and retrieved data from data-warehouses using amazon redshift and designed and implemented system architecture for amazon ec2 based cloud-hosted solution for client.\n",
      "---\n",
      " developed simple to complex map reduce jobs using hive and pig and developed multiple mapreduce jobs in java for data cleaning and preprocessing.\n",
      "---\n",
      " analyzing large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n",
      "---\n",
      " developed mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws.\n",
      "---\n",
      " worked with various teradata15 tools and utilities like teradata viewpoint, multi load, arc, teradata administrator, bteq and other teradata utilities.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, spark streaming, caffe, tensorflow, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " developed linux shell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " working on information extraction from different kinds of text documents using nlp, text mining and regular expressions.\n",
      "---\n",
      " worked extensively on tableau desktop, apply filters, drill downs, and generate data visualizations, interactive dash boards, that can interact with views of data and worked on several options like query, display, analyze, sort, group, drill down, organize, summarize and generate charts, monitor and measure goals, identify patterns\n",
      "---\n",
      " implemented end-to-end systems for data analytics, data automation and integrated with custom visualization tools using r, hadoop and mongodb, cassandra.\n",
      "---\n",
      " building predictive models using tools such as sas, r with very granular data stored in big data platform.\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r and used spark for test data analytics using mllib and analyzed the performance to identify bottlenecks.\n",
      "---\n",
      " involved working with machine learning algorithms such as decision trees, random forest, gradient boosting, support vector machines, k mean clustering, na\\xefve bayes, bayesian belief networks and artificial neural networks.\n",
      "---\n",
      " developed predictive models, machine learning (supervised and non-supervised) using r for machine motor.\n",
      "---\n",
      " creating various b2b predictive and descriptive analytics using r and tableau and performed data cleaning and data preparation tasks to convert data into a meaningful data set using r.\n",
      "---\n",
      " used r to verify the results of mahout on small data sets. developed missing but important features of ml algorithms to the mahout.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc\n",
      "---\n",
      " created partitioned and bucketed tables in hive. involved in creating hive internal and external tables, loading with data and writing hive queries which involves multiple join scenarios.\n",
      "---\n",
      " performed k-means clustering, multivariate analysis and support vector machines in r.\n",
      "---\n",
      " create analytical models using analytics algorithms like regression, decision trees, clustering, text mining etc. and leveraging tools like r, tableau etc. to deliver actionable insights and recommendations.\n",
      "---\n",
      " developed multiple spark jobs using scala for data cleaning and preprocessing\n",
      "---\n",
      " designed the schema, configured and deployed aws redshift for optimal storage and fast retrieval of data.\n",
      "---\n",
      " used external loaders like multi load, t pump and fast load to load data into teradata14.1database.\n",
      "---\n",
      " involved in troubleshooting and quality control of data transformations and loading during migration from oracle systems into netezza edw.\n",
      "---\n",
      " used s3 bucket to store the jar's, input datasets and used dynamo db to store the processed output from the input data set.\n",
      "---\n",
      " worked on classification/scripting of multiple attribute models by applying text-mining, nlp, svm and regular expressions given product features like title, description etc. & predicting product attribute values using python/r\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in r.\n",
      "---\n",
      " used spark for test data analytics using mllib and analyzed the performance to identify bottlenecks and used supervised learning techniques such as classifiers and neural networks to identify patters in these data sets\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop. tableau workbooks from multiple data sources using data blending.\n",
      "---\n",
      " developing new data warehousing system based on spark 2.x and spark streaming, utilizing scala and java 8\n",
      "---\n",
      " strong knowledge on concepts of datamodeling star schema/snowflake modeling, fact& dimensions tables and logical&physical data modeling.\n",
      "---\n",
      " environment: r3.x, erwin 9.5.2, mdm, qlikview, mllib, pl/sql, tableau, teradata 14.1, json, hadoop (hdfs), mapreduce, sql server, mllib, scala nlp, ssms, erp, crm, netezza, pandas, sas, spss, java, ide, cassandra, sql, pl/sql, aws, ssrs, informatica, pig, spark, azure, r studio, mongodb, mahout, java, hive, aws redshift. sql developer against ods western digital - burlington, nj april 2012 to december 2014 burlington, nj april 2012 to december 2014\n",
      "---\n",
      " design database, data models, etl processes, data warehouse applications and business intelligence (bi) reports through the use of best practices and tools, including erwin, sql, ssis, ssrs and olap, oltp.\n",
      "---\n",
      " transformed logical data model to physical data model ensuring the primary key and foreign key relationships in pdm, consistency of definitions of data attributes and primary index considerations.\n",
      "---\n",
      " validated the data of reports by writing sql queries in pl/sql developer against ods.\n",
      "---\n",
      " developed mapreduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the edw.\n",
      "---\n",
      " analyzed trading mechanism for real-time transactions and build collateral management tools.\n",
      "---\n",
      " compiled data from various sources to perform complex analysis for actionable results.\n",
      "---\n",
      " utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " measured efficiency of hadoop/hive environment ensuring sla is met.\n",
      "---\n",
      " developed spark code using scala and spark-sql/streaming for faster processing of data.\n",
      "---\n",
      " prepared spark build from the source code and ran the pig scripts using spark rather using mr jobs for better performance.\n",
      "---\n",
      " analyzing the system for new enhancements/functionalities and perform impact analysis of the application for implementing etl changes.\n",
      "---\n",
      " imported data using sqoop to load data from mysql to hdfs on regular basis.\n",
      "---\n",
      " developed scripts and batch job to schedule various hadoop program. used tensorflow to train the model from insightful data and look at thousands of examples.\n",
      "---\n",
      " designing, developing and optimizing sql code (ddl / dml)\n",
      "---\n",
      " building performant, scalable etl processes to load, cleanse and validate data.\n",
      "---\n",
      " expertise in data archival and data migration, ad-hoc reporting and code utilizing sas on unix and windows environments.\n",
      "---\n",
      " tested and debugged sas programs against the test data.\n",
      "---\n",
      " processed the data in sas for the given requirement using sas programming concepts.\n",
      "---\n",
      " imported and exported data files to and from sas using proc import and proc export from excel and various delimited text-based data files such as .txt (tab delimited) and .csv (comma delimited) files into sas datasets for analysis.\n",
      "---\n",
      " expertise in producing rtf, pdf, html files using sas ods facility.\n",
      "---\n",
      " providing support for data processes. this will involve monitoring data, profiling database usage, trouble shooting, tuning and ensuring data integrity.\n",
      "---\n",
      " participating in the full software development lifecycle with requirements, solution design, development, qa implementation, and product support using scrum and other agile methodologies.\n",
      "---\n",
      " collaborate with team members and stakeholders in design and development of data environment.\n",
      "---\n",
      " learning new tools and skillsets as needs arise.\n",
      "---\n",
      " preparing associated documentation for specifications, requirements and testing.\n",
      "---\n",
      " optimizing the tensorflow model for an efficiency.\n",
      "---\n",
      " used tensorflow for text summarization.\n",
      "---\n",
      " used spark api over cloudera hadoop yarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " developed kafka producer and consumers for message handling.\n",
      "---\n",
      " responsible for analyzing multi-platform applications using python.\n",
      "---\n",
      " used storm for an automatic mechanism to analyze large amounts of non-unique data points with low latency and high throughput.\n",
      "---\n",
      " developed mapreduce jobs in python for data cleaning and data processing.\n",
      "---\n",
      " worked on the project from gathering requirements to developing the entire application. worked on anaconda python environment. created, activated and programmed in anaconda environment. wrote programs for performance calculations using numpy and sqlalchemy.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " used python modules of urllib, urllib2, requests for web crawling. experience using all these ml techniques: clustering, regression, classification, graphical models.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " involved in development of web services using soap for sending and getting data from the external interface in the xml format. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " worked on development of sql and stored procedures on mysql.\n",
      "---\n",
      " analyzed the code completely and have reduced the code redundancy to the optimal level.\n",
      "---\n",
      " design and build a text classification application using different text classification models.\n",
      "---\n",
      " used jira for defect tracking and project management.\n",
      "---\n",
      " worked on writing and as well as read data from csv and excel file formats.\n",
      "---\n",
      " involved in sprint planning sessions and participated in the daily agile scrum meetings.\n",
      "---\n",
      " conducted every day scrum as part of the scrum master role.\n",
      "---\n",
      " developed the project in linux environment.\n",
      "---\n",
      " worked on resulting reports of the application.\n",
      "---\n",
      " performed qa testing on the application.\n",
      "---\n",
      " held meetings with client and worked for the entire project with limited help from the client.\n",
      "---\n",
      " environment: python, anaconda, sypder (ide), windows 7, teradata, requests, urllib, urllib2, beautiful soup, tableau, python libraries such as numpy, sql alchemy, mysqldb. skills sql (10+ years), apache hadoop mapreduce (8 years), mapreduce (8 years), mapreduce (8 years), pl/sql (8 years) additional information skills\n",
      "---\n",
      " apache hadoop mapreduce (7 years), mapreduce (7 years), olap (7 years), online analytical processing (7 years), pl/sql (7 years)\n",
      "---\n",
      " technical skills:\n",
      "---\n",
      " data analytics tools/programming: python (numpy, scipy, pandas, gensim, keras), r ( caret, weka, ggplot), matlab, microsoft sql server, oracle plsql, python, sql, pl/sql, t-sql, unix shell scripting, java, sas.\n",
      "---\n",
      " big data techs: hadoop, hive, hdfs, mapreduce, pig, kafka, hbase, cassandra, mongodb.\n",
      "---\n",
      " analysis and modeling tools: erwin, sybase power designer, oracle designer, bpwin, rational rose, er/studio, toad, ms visio\n",
      "---\n",
      " etl tools: informatica power center, data stage 7.5, ab initio, talend\n",
      "---\n",
      " olap tools: ms sql analysis manager, db2 olap, cognospowerplay\n",
      "---\n",
      " languages: sql, pl/sql, t-sql, xml, html, unix shell scripting, c, c++, awk\n",
      "---\n",
      " databases: oracle12c/11g/10g/9i/8i/8.0/7.x, teradata14.0, db2 udb 8.1, ms sqlserver 2012/2008/2005, netezaa and sybase ase 12.5.3/15, informix 9, hbase, mongodb, cassandra, amazon redshift.\n",
      "---\n",
      " operating systems: windows 2007/8, unix (sun-solaris, hp-ux), windows nt/xp/vista, msdos\n",
      "---\n",
      " project execution methodologies: ralph kimball and bill inmon data warehousing methodology, rational unified process (rup), rapid application development (rad), joint application development (jad)\n",
      "---\n",
      " reporting tools: business objectsxir2/6.5/5.0/5.1, cognos impromptu 7.0/6.0/5.0, informatica analytics delivery platform, microstrategy, tableau.\n",
      "---\n",
      " tools: ms-office suite (word, excel, ms project and outlook), vss\n",
      "---\n",
      " others: spark mllib, scala nlp, mariadb, azure, sas, ide, microsoft azure, aws,\n",
      "---\n",
      " data scientist/machine learning engineer - macy's inc\n",
      "---\n",
      " duluth, ga\n",
      "---\n",
      " close to eight years of expert involvement in it in which i have 3+ years of knowledge in data mining, machine learning and spark development with big datasets of structured and unstructured data.\n",
      "---\n",
      " data acquisition, data validation, predictive demonstrating, data visualization. capable in measurable programming languages like r and python.\n",
      "---\n",
      " proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " adept and deep understanding of statistical modeling, multivariate analysis, model testing, problem analysis, model comparison and validation.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " experience in using various packages in r and libraries in python.\n",
      "---\n",
      " working knowledge in hadoop, hive and nosql databases like cassandra and hbase.\n",
      "---\n",
      " hands on experience in implementing lda, naive bayes and skilled in random forests, decision trees, linear and logistic regression, svm, clustering, neural networks, principle component analysis and good knowledge on recommender systems.\n",
      "---\n",
      " good industry knowledge, analytical and problem-solving skills and ability to work well within a team as well as an individual.\n",
      "---\n",
      " highly creative, innovative, committed, intellectually curious, business savvy with effective communication and interpersonal skills.\n",
      "---\n",
      " i can be able to quickly adapt the new work pace and learning\n",
      "\n",
      "---\n",
      " professional qualified data scientist/data analyst with over 9 years of experience in data science and analytics including artificial intelligence/deep learning/machine learning, data mining and statistical analysis\n",
      "---\n",
      " involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data, created er diagrams and schema.\n",
      "---\n",
      " experienced with machine learning algorithm such as logistic regression, random forest, xgboost, knn, svm, neural network, linear regression, lasso regression and k-means\n",
      "---\n",
      " implemented bagging and boosting to enhance the model performance.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/b test, experiment design, hypothesis test, anova\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0 jupiter notebook 4.x, r 3.0 (ggplot2, caret, dplyr) and excel [ ]\n",
      "---\n",
      " solid ability to write and optimize diverse sql queries, working knowledge of rdbms like sql server 2008, nosql databases like mongodb 3.2\n",
      "---\n",
      " developed api libraries and coded business logic using c#, xml and designed web pages using .net framework, c#, python, django, html, ajax\n",
      "---\n",
      " strong experience for over 5 years in image recognition and big data technologies like spark 1.6, sparksql, pyspark, hadoop 2.x, hdfs, hive 1.x\n",
      "---\n",
      " experience in visualization tools like, tableau 9.x, 10.x for creating dashboards\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " excellent understanding agile and scrum development methodology\n",
      "---\n",
      " used the version control tools like git 2.x and build tools like apache maven/ant\n",
      "---\n",
      " passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making\n",
      "---\n",
      " ability to maintain a fun, casual, professional and productive team atmosphere\n",
      "---\n",
      " experienced the full software life cycle in sdlc, agile, devops and scrum methodologies including creating requirements, test plans.\n",
      "---\n",
      " skilled in advanced regression modeling, correlation, multivariate analysis, model building, business intelligence tools and application of statistical concepts.\n",
      "---\n",
      " proficient in predictive modeling, data mining methods, factor analysis, anova, hypothetical testing, normal distribution and other advanced statistical and econometric techniques.\n",
      "---\n",
      " developed predictive models using decision tree, random forest, na\\xefve bayes, logistic regression, social network analysis, cluster analysis, and neural networks.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " experienced in python to manipulate data for data loading and extraction and worked with python libraries like matplotlib, numpy, scipy and pandas for data analysis.\n",
      "---\n",
      " worked with complex applications such as r, python, theano, h20, sas, matlab and spss to develop neural network, cluster analysis.\n",
      "---\n",
      " expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data ingestion, data manipulation,data architecture, data modelling and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, remap, merge, subset, reindex, melt and reshape.\n",
      "---\n",
      " strong c#, sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " extensively worked on python 3.5/2.7 (numpy, pandas, matplotlib, nltk and scikit-learn)\n",
      "---\n",
      " experienced in visual basic for applications and vb programming languages c#, .net framework to work with developing applications.\n",
      "---\n",
      " worked with nosql database including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, hdfs, mapreduce, and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio ssis, ssas, ssrs.\n",
      "---\n",
      " proficient in tableau,adobe analytics and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql and python and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner.\n",
      "---\n",
      " tools and technologies:\n",
      "---\n",
      " languages java 8, python, r, c#, powershell\n",
      "---\n",
      " packages\n",
      "---\n",
      " ggplot2, caret, dplyr, rweka, gmodels, edward, rcurl, tm, c50, twitter, nlp, reshape2, rjson, plyr, pandas, numpy, tensorflow, seaborn, scipy, matplot lib, scikit-learn, beautiful soup, rpy2.\n",
      "---\n",
      " web technologies jdbc, html5, dhtml and xml, css3, web services, wsdl, django, aws\n",
      "---\n",
      " data modelling tools erwin r 9.6, 9.5, 9.1, 8.x, rational rose, er/studio, ms visio, sap power designer.\n",
      "---\n",
      " big data technologies hadoop, hive, hdfs, presto, mapreduce, pig, kafka, oozie.\n",
      "---\n",
      " databases\n",
      "---\n",
      " sql, hive, impala, pig, spark sql, hql, vql, databases sql-server, my sql, ms sql,ms access, hdfs, hbase, teradata, netezza, mongodb, cassandra.\n",
      "---\n",
      " reporting tools\n",
      "---\n",
      " ms office (word/excel/power point/ visio), tableau, spotfire, crystal reports xi, business intelligence, ssrs, business objects 5.x/ 6.x, cognos7.0/6.0.\n",
      "---\n",
      " etl tools informatica power centre, ssis.\n",
      "---\n",
      " version control tools svm, github.\n",
      "---\n",
      " project execution work experience data scientist / big data cigna health insurance - bloomfield, ct august 2017 to present description: the cigna information management & analytics (cima) unit offers solutions that provide actionable insights to internal and external business partners and customers that help reduce health costs, improve outcomes, provide financial security and measure and forecast business performance\n",
      "---\n",
      " several visualizations (density plots, forest plots, leverage plots, network plots, covariant adjustment plots etc.) were made using packages such as ggplot2, ggmcmc.\n",
      "---\n",
      " successfully delivered multiple nlp projects like building a chatbot that assists a customer to trouble shoot claim issues and recommend actions.further the bot could handle questions asked in natural language related to common issues with the customer e.g. when is my premium due, what is my plan deductible, what is my copay for a sick reject.\n",
      "---\n",
      " extracted data from multiple sources like medicare, medicaid, aca claims.\n",
      "---\n",
      " performed data pre-processing like data cleaning, text preprocessing, noise removal, lexicon normalization and object standardization.\n",
      "---\n",
      " perform featuring engineering like word embedding using word2vec models.\n",
      "---\n",
      " build seq2seqmodels using structured data & word embedding. seq2seq model take an input and returns as desired output for e.g. it can take a question as an input and returns an answer. the benefit is it can take any arbitrary length question and returns and answers in natural language. it uses a recurrent neural network (lstm/memory network) at the back-end.\n",
      "---\n",
      " performing map reduce jobs in hadoop and implemented spark analysis using python for performing machine learning & predictive analytics on aws platform.\n",
      "---\n",
      " analyzed administrative claims data - medicare and aca marketplace - to answer health services research questions on costs, utilization or outcomes, using advanced statistical and econometric methods.\n",
      "---\n",
      " used tensorflow packages to train machine learning models.\n",
      "---\n",
      " developed oozie workflows to ingest/parse the raw data, populate staging tables and store the refined data in partitioned tables in the hive.\n",
      "---\n",
      " hand-on experience with data ingestion into big data platform from disparate data sources using sqoop, hive, pig, flume and spark.\n",
      "---\n",
      " created an end-to-end data analytical solutions and models by manipulating large data sets and integrating diverse data sources.\n",
      "---\n",
      " worked with team of developers to design, develop and implement bi solutions in tableau to measure point of sale kpis at micro and macro level.\n",
      "---\n",
      " environment: tableau, python, pycharm, statistics, machine learning, tensorflow, alteryx, hadoop, hive, pig, no sql, pl/sql, excel, aws data scientist opera solutions, new jersey january 2016 to july 2017 description: opera solutions, llc is a technology and analytics company mainly focused on big data. the firm uses a combination of machine learning science, advanced predictive analytics, technology, large-scale data management, and human expertise. opera solutions delivers predictive analytics as a service, and offers hosted, cloud-based systems for specific business problems, e.g., predicting the behavior of individual consumers, stopping revenue leakage in hospitals, warning of threats to corporate security or brand health, etc.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features of usmle examinations of various student patterns using tableau, adobe analytics and python matplotlib.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana etc\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " implemented statistical modeling with xgboost machine learning software package using python to determine the predicted probabilities of each model.\n",
      "---\n",
      " created master data for modelling by combining various tables and derived fields from client data and students lors, essays and various performance metrics.\n",
      "---\n",
      " formulated a basis for variable selection and gridsearch, kfold for optimal hyperparameters\n",
      "---\n",
      " utilized boosting algorithms to build a model for predictive analysis of student's behaviour who took usmle exam apply for residency.\n",
      "---\n",
      " used numpy, scipy, pandas, nltk(natural language processing toolkit),matplotlib to build the model.\n",
      "---\n",
      " formulated several graphs to show the performance of the students by demographics and their mean score in different usmle exams.\n",
      "---\n",
      " extracted data from hdfs using hive, presto and performed data analysis using spark with scala, pyspark, redshift and feature selection and created nonparametric models in spark\n",
      "---\n",
      " application of various artificial intelligence(ai)/machine learning algorithms and statistical modeling like decision trees,text analytics, image and text recognition using ocr tools like abbyy, natural language processing(nlp), supervised and unsupervised, regression models.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frameworks.\n",
      "---\n",
      " created deep learning models using tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students.\n",
      "---\n",
      " used xgb classifier if the feature is an categorical variable and xgb regressor for continuous variables and combined it using featureunion and functiontransfomer methods of natural language processing.\n",
      "---\n",
      " used onevsrest classifier to fit each classifier against all other classifiers and used it on multiclass classification problems.\n",
      "---\n",
      " implemented application of various machine learning algorithms and statistical modeling like decision tree, text analytics, sentiment analysis, naive bayes, logistic regression and linear regression using python to determine the accuracy rate of each model.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior with cloud based products like azure ml studio and dataiku.\n",
      "---\n",
      " generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using signal hub and aws sagemaker/azure databricks.\n",
      "---\n",
      " created data layers as signals to signal hub to predict new unseen data with performance not less than the static model build using deep learning framework.\n",
      "---\n",
      " environment: python 2.x,3.x, hive, aws, linux, tableau desktop, microsoft excel, nlp, deep learning frameworks such as tensorflow, keras, boosting algorithms etc data scientist mercedes benz financial services, michigan july 2014 to december 2015 description: mercedes-benz financial services is a leading, captive financial services provider and the global financial services company of daimler ag. doing business as mercedes-benz financial services and daimler truck financial, we provide financing for automotive and commercial vehicle dealers and their retail consumers in the united states, canada, mexico, brazil and argentina.\n",
      "---\n",
      " performed data profiling to learn about behavior with various features such as traffic pattern, location, time, date and time etc using adode analytics.\n",
      "---\n",
      " application of various artificial intelligence(ai)/machine learning algorithms and statistical modeling like decision trees,text analytics, natural language processing(nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " utilized spark, snowflake, presto, scala, hadoop, hql, vql, oozie, pyspark, data lake, tensorflow, hbase, cassandra, athena, redshift, mongodb, kafka, kinesis, spark streaming, edward, cuda, mllib, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " created and connected sql engine through c# to connect database, developed api libraries and business logic using c#, xml and python\n",
      "---\n",
      " exploring dag's, their dependencies and logs using airflow pipelines for automation\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as caffe, neon etc\n",
      "---\n",
      " developed spark/scala, python,r for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources. used clustering technique k-means to identify outliers and to classify unlabeled data.\n",
      "---\n",
      " utilized aws lambda in created user-friendly interface for quick view of reports by using c#, jsp, xml and developed expandable menu that show drilldown data on graph click\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana etc\n",
      "---\n",
      " categorised comments into positive and negative clusters from different social networking sites using sentiment analysis and text analytics and have done image recognition\n",
      "---\n",
      " tracking operations using sensors until certain criteria is met using airflow technology.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata using utilities like tpump, fexp, mload, bteq, fload etc\n",
      "---\n",
      " analyze traffic patterns by calculating autocorrelation with different time lags.\n",
      "---\n",
      " ensured that the model has low false positive rate and text classification and sentiment analysis for unstructured and semi-structured data.\n",
      "---\n",
      " addressed overfitting by implementing of the algorithm regularization methods like l2 and l1.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n",
      "---\n",
      " performed multinomial logistic regression, random forest, decision tree, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve data from oracle database and used etl for data transformation.\n",
      "---\n",
      " used mllib, spark's machine learning library to build and evaluate different models and used aws rekognition for image analysis.\n",
      "---\n",
      " implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using sap predictive analytics.\n",
      "---\n",
      " developed mapreduce pipeline for feature extraction using hive and pig.\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau/spotfire.\n",
      "---\n",
      " communicated the results with operations team for taking best decisions.\n",
      "---\n",
      " collected data needs and requirements by interacting with the other departments.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, c#, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2012, microsoft excel, matlab, spark sql, pyspark. data scientist first data - atlanta, ga january 2013 to june 2014 description: first data corporation is a global payment processing company headquartered in atlanta, georgia, united states. the company's portfolio includes merchant transaction processing services\n",
      "---\n",
      " credit, debit, private-label, gift, payroll and other prepaid card offerings\n",
      "---\n",
      " fraud protection and authentication solutions.\n",
      "---\n",
      " provided configuration management and build support for more than 5 different applications, built and deployed to the production and lower environments.\n",
      "---\n",
      " implemented public segmentation using unsupervised machine learning algorithms by implementing k-means algorithm using pyspark.\n",
      "---\n",
      " using airflow to keep track of job statuses in repositories like mysql and postgre databases.\n",
      "---\n",
      " explored and extracted data from source xml in hdfs, used etl for preparing data for exploratory analysis using data munging.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata, text mining and building models using topic analysis, sentiment analysis for both semi-structured and unstructured data.\n",
      "---\n",
      " handled importing data from various data sources, performed transformations using hive, map reduce, and loaded data into hdfs\n",
      "---\n",
      " used r and python for exploratory data analysis, a/b testing, hql, vql, data lake, aws redshift, oozie, pyspark, anova test and hypothesis test to compare and identify the effectiveness of creative campaigns.\n",
      "---\n",
      " computing a/b testing frameworks, clickstream and time spent databases using airflow\n",
      "---\n",
      " created clusters to control and test groups and conducted group campaigns using text analytics.\n",
      "---\n",
      " created positive and negative clusters from merchant's transaction using sentiment analysis to test the authenticity of transactions and resolve any chargebacks.\n",
      "---\n",
      " analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data.\n",
      "---\n",
      " created and developed classes and web page elements using c# and ajax. jsp was used for validating client side responses and connected c# to database to retrieve sql data\n",
      "---\n",
      " developed linuxshell scripts by using nzsql/nzload utilities to load data from flat files to netezza database.\n",
      "---\n",
      " developed triggers, stored procedures, functions and packages using cursors and ref cursor concepts associated with the project using pl/sql\n",
      "---\n",
      " created various types of data visualizations using r, c#, python and tableau/spotfire also connected pipeline pilot with spotfire to create more interactive business driven layouts.\n",
      "---\n",
      " used python, r, sql, tensorflow to create statistical algorithms involving multivariate regression, linear regression, logistic regression, pca, image recognition, random forest models, decision trees, support vector machine for estimating the risks of welfare dependency.\n",
      "---\n",
      " identified and targeted welfare high-risk groups with machine learning/deep learning algorithms.\n",
      "---\n",
      " conducted campaigns and run real-time trials to determine what works fast and track the impact of different initiatives.\n",
      "---\n",
      " developed tableau visualizations and dashboards using tableau desktop.\n",
      "---\n",
      " used graphical entity-relationship diagramming to create new database design via easy to use, graphical interface.\n",
      "---\n",
      " created multiple custom sql queries in teradata sql workbench to prepare the right data sets for tableau dashboards\n",
      "---\n",
      " perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using sas programming.\n",
      "---\n",
      " environment: r 3.x, hdfs, c#,hadoop 2.3, pig, hive, linux, r-studio, tableau 10, sql server, ms excel, pypark. data scientist tripadvisor - new york, ny november 2011 to december 2012 description: tripadvisor, inc. is an american travel website company providing reviews of travel-related content. it also includes interactive travel forums. tripadvisor was an early adopter of user-generated content. the website services are free to users, who provide most of the content, and the website is supported by an advertising business model.\n",
      "---\n",
      " involved in design, development and support phases of software development life cycle (sdlc)\n",
      "---\n",
      " performed data etl by collecting, exporting, merging and massaging data from multiple sources and platforms including ssrs/ssis (sql server integration services) in sql server.\n",
      "---\n",
      " programming experience with .net framework, c#, visual studio 2005/2008 to build web based, client/server architecture and to produce reports with c# and jsp.\n",
      "---\n",
      " worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from mongodb through mongdb connector for hadoop.\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark.\n",
      "---\n",
      " performed partitional clustering into 100 by k-means clustering using scikit-learn package in python where similar hotels for a search are grouped together and image recognition.\n",
      "---\n",
      " used python to perform anova test to analyze the differences among hotel clusters.\n",
      "---\n",
      " implemented application of various machine learning algorithms and statistical modeling like decision tree, text analytics, sentiment analysis, naive bayes, logistic regression and linear regression using python to determine the accuracy rate of each model.\n",
      "---\n",
      " determined the most accurately prediction model based on the accuracy rate.\n",
      "---\n",
      " used text-mining process of reviews to determine customers' concentrations.\n",
      "---\n",
      " delivered analysis support to hotel recommendation and providing an online a/b test.\n",
      "---\n",
      " designed tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards.\n",
      "---\n",
      " developed hybrid model to improve the accuracy rate.\n",
      "---\n",
      " environment: python, pyspark, c#, tableau, mongodb, hadoop, sql server, sdlc, etl, ssis, recommendation systems, machine learning algorithms, text-mining process, a/b test data scientist bank of america - wilmington, de october 2010 to october 2011 description: bank of america is a multinational banking and financial services corporation. it is ranked 2nd on the list of largest banks in the united states by assets. as of 2016, bank of america was the 26th largest company in the united states by total revenue.\n",
      "---\n",
      " participated in all phases of research including data collection, data cleaning, data mining, developing models and visualizations.\n",
      "---\n",
      " collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements.\n",
      "---\n",
      " redefined many attributes and relationships and cleansed unwanted tables/columns using sql queries.\n",
      "---\n",
      " utilized spark sql api in pyspark to extract and load data and perform sql queries and also used c# connector to perform sql queries by creating and connecting to sql engine.\n",
      "---\n",
      " performed data imputation using scikit-learn package in python.\n",
      "---\n",
      " performed data processing using python libraries like numpy and pandas.\n",
      "---\n",
      " worked with data analysis using ggplot2 library in r to do data visualizations for better understanding of customers' behaviors.\n",
      "---\n",
      " implemented statistical modeling with xgboost machine learning software package using r to determine the predicted probabilities of each model.\n",
      "---\n",
      " delivered the results with operation team for better decisions.\n",
      "---\n",
      " environment: python, r, sql, tableau, spark, machine learning software package, recommendation systems. python developer cenvien technologies - hyderabad, telangana june 2009 to august 2010 description: cenvien technologies gather the requirements by listening and understanding to the client's business requirement to deliver quality products. it is highly qualified and strongly dedicated developing team that produces unique solutions.\n",
      "---\n",
      " developed entire frontend and backend modules using python on django web framework.\n",
      "---\n",
      " implemented the presentation layer with html, css and javascript.\n",
      "---\n",
      " involved in writing stored procedures using oracle.\n",
      "---\n",
      " optimized the database queries to improve the performance.\n",
      "---\n",
      " designed and developed data management system using oracle.\n",
      "---\n",
      " environment: mysql, oracle, html5, css3, javascript, shell, linux & windows, django, python programmer analyst pennar industries limited - hyderabad, telangana april 2008 to may 2009 description: as a backend developer of web applications and data science infrastructure. the main area of focus is to come up with comprehensive solutions that need massive capacity and throughput.\n",
      "---\n",
      " effectively communicated with the stakeholders to gather requirements for different projects\n",
      "---\n",
      " used mysql db package and python-mysql connector for writing and executing several mysql database queries from python.\n",
      "---\n",
      " implemented client/server applications using c++, c#, jsp and sql\n",
      "---\n",
      " created functions, triggers, views and stored procedures using my sql.\n",
      "---\n",
      " worked closely with back-end developer to find ways to push the limits of existing web technology.\n",
      "---\n",
      " involved in the code review meetings.\n",
      "---\n",
      " environment: python, mysql, c#. \n",
      "---\n",
      " bachelor of computer science in computer science jntu anantapur - anantapur, andhra pradesh skills application development, tableau (8 years), linux (5 years), sap (1 year), bi (1 year), python, r, hadoop, machine learning, spark additional information methodologies\n",
      "---\n",
      " ralph kimball and bill inmon data warehousing methodology, rational unified process (rup), rapid application development (rad), joint application development (jad).\n",
      "---\n",
      " bi tools\n",
      "---\n",
      " tableau, tableau server, tableau reader, sap business objects, obiee, qlikview, sap business intelligence, amazon redshift, or azure data warehouse\n",
      "---\n",
      " operating system windows, linux, unix, macintosh hd, red hat,android.\n",
      "\n",
      "---\n",
      " major responsibility for model quality of product that represents 70% of all company revenues ($30-$100m range). instituted processes, repeatable analyses and quality control to mitigate unforeseen changes from external data sources.\n",
      "---\n",
      " mentored and coached 2 new team members to scale our operations and maintain distribution of shared knowledge. senior manager / senior staff engineer app annie ltd - beijing 2013 to 2015 tasked with building and leading team around our flagship product. held director-level responsibilities that included developing and maintaining the technical architecture, coaching and directing team members, collaborating with directors and tech leads, and overseeing project management. reported to evp of engineering.\n",
      "---\n",
      " delivered new, game-changing paid products for the company. built and led team signified as the development team solely responsible for all revenue-generating products at app annie.\n",
      "---\n",
      " created initial set of development processes and best practices that paved the way for scaling of development efforts (including introduction of new qa team, integration of pm tools, proposals for internal guidelines).\n",
      "---\n",
      " recruited and hired over 25 individuals, maintaining an above-average retention rate. mentored 3 team members who subsequently became successful tech leaders.\n",
      "---\n",
      " took on responsibility of managing data science team for 1 year. software engineer app annie ltd - beijing 2011 to 2012 contributed to a wide range of activities within a fast-paced, challenging start-up period. worked closely with other team\n",
      "---\n",
      " members in developing our core platform, implementing our first set of integrations, managing our production environment and hiring our initial set of developers.\n",
      "---\n",
      " laid the groundwork for the operations in a joint effort with 2 other founding engineers, our ceo and a half-time pm as the company grew from 5 to 100 team members.\n",
      "---\n",
      " implemented two proof-of-concepts that became their own product. one an extension of our data collector that\n",
      "---\n",
      " rolled into our free product suite and the other an add-on component to our paid offering.\n",
      "---\n",
      " took over server maintenance. extended our automation and monitoring capabilities to support our growth.\n",
      "---\n",
      " managed and mentored new hires with an emphasis on making them productive as soon as possible. software engineer exoweb ltd - beijing 2010 to 2011 served as a individual contributor for the project of a large customer that brought in 95% of the company's total revenues.\n",
      "---\n",
      " backend development for high-traffic website with an 8 years old legacy codebase. system architect / researcher / java .net python developer hungarian academy of sciences - budapest 2006 to 2010 participated in 3 long-term projects at the data mining and web search group, institute for computer science & automation\n",
      "---\n",
      " developed multi-platform, customizable, web-based search application. proposed and implemented integration ar- chitecture, designed pluggable web framework, consolidated previously separate applications, and transformed past results into a marketable product targeting enterprise customers.\n",
      "---\n",
      " designed and developed front-end of client-server data mining application for one of the largest insurance companies in hungary. designed interoperability api and java swing gui with strong emphasis on usability.\n",
      "---\n",
      " participated in research projects on data mining including but not limited to high-dimensional modeling, web search, svm, mcmc\n",
      "---\n",
      " onboarded and mentored a new team member. consultant various clients - budapest 1998 to 2010 completed multiple projects for client companies over a 12-year period, including:\n",
      "---\n",
      " case management system for an independent court bailiff's office (led the entire project using agile methods)\n",
      "---\n",
      " inventory application for a costume wholesaler\n",
      "---\n",
      " small crm application for a pr company (created a lean collaboration system)\n",
      "---\n",
      " bpm system for a longevity insurance company\n",
      "---\n",
      " inventory manager for a restaurant client desktop support engineer pannon gsm, est media - budapest 2002 to 2006 worked on 2 separate projects for the 2nd largest mobile provider in hungary (pannon gsm) and a smaller media company with 200 employees (est media), respectively. \n",
      "---\n",
      " incomplete (phd) in implementing data mining algorithms on modern gpus phd school of computer science, eo?tvo?s lo?ra?nd university - budapest, hungary 2008 to 2010 msc in software engineering with specialization in systems development budapest university of technology and economics - budapest, hu 2000 to 2007 skills b2b software (7 years), software development (10+ years), python (8 years), data analysis (6 years), data mining (6 years), manager (3 years) publications kdd cup 2009 @ budapest: feature partitioning and boosting http://dl.acm.org/citation.cfm?id=3000364.3000370 2009-07 multiple authors:\n",
      "---\n",
      " we describe the method used in our final submission to kdd cup 2009 as well as a selection of promising directions that are generally believed to work well but did not justify our expectations. our final method consists of a combination of a logitboost and an adtree classifier with a feature selection method that, as shaped by the experiments we have conducted, have turned out to be very different from those described in some well-cited surveys. some methods that failed include distance, information and dependence measures for feature selection as well as combination of classifiers over a partitioned feature set. as another main lesson learned, alternating decision trees and logitboost outperformed most classifiers for most feature subsets of the kdd cup 2009 data. additional information technical skills\n",
      "---\n",
      " languages: python, shell, r, emacs lisp, java\n",
      "---\n",
      " paradigms: functional, declarative, object-oriented\n",
      "---\n",
      " software engineering: large scale scrum, ci, rad\n",
      "---\n",
      " stats: eda, dataviz, ml, doe, quality control\n",
      "---\n",
      " databases: postgresql, physical database design\n",
      "---\n",
      " architecture: distributed systems, protocol design\n",
      "\n",
      "---\n",
      " over 7 years\\x92 hand on experienced in the areas of machine learning, deep learning and statistical modeling, big data analytics. including prediction modeling, time series forecasting, financial fraud detection, opencv /image processing, object detection, semantic segmentation, deep speech recognition, and nlp\n",
      "---\n",
      " expertise in multiple deep learning platforms (tensorflow, keras, intel neon/bigdl, etc.) with diversified use cases\n",
      "---\n",
      " experienced in spark, relational databases, graph databases, nosql databases, and related ml use cases. authorized to work in the us for any employer work experience senior data scientist consultant verizon wireless - irving, tx august 2018 to present \n",
      "---\n",
      " data science projects for wise of customer team at verizon wireless. enhance online-sale performance and customer experiences based on text mining, recommendation systems and machine learning modeling. \n",
      "---\n",
      " nlp and text mining for voc(voice of customers-online chats): topic extraction with n-grams by lda, nmf, lsi/svd, and lda2vec\n",
      "---\n",
      " dashboard visualization using pyldavis, t-sne, d3/js, etc. sentiment and semantic analysis. language modeling using spacy, gensim/word2vec, etc. \n",
      "---\n",
      " improve customer experiences and online purchase/visiting rates through integrations of recommendation system, prediction modeling, hypothesis tests, and a/b tests, etc. perform under-sampling/super-sampling for unbalanced datasets. fine tune the hyperparameters for major algorithms and evaluate the models.\n",
      "---\n",
      " data science automated workflow on aws/docker, build flask application to aws elastic beanstalk, setup connection to the databases, build web interface to indicate desired database table and filtering keywords to extract data automatically, and to input modeling parameters in order to train the model on click, then return the modeling results/visualizations as dashboard to the webpage directly. senior data scientist - consultant dell emc - round rock, tx november 2017 to present \n",
      "---\n",
      " artificial intelligence deep learning project: research on deep learning platforms with variant use cases, evaluating different deep learning platforms, infrastructure with spark and gpu parallel systems.\n",
      "---\n",
      " testing wide variety of use cases: lung nodule classification, object detection by ssd and yolo, financial time series forecasting, deep speech recognition, etc..\n",
      "---\n",
      " testing spark bigdl/intel neon/tensorflow/cntk/keras, performance evaluations on backends of cpu/mkl/gpu. focusing on tensorflow and bigdl on spark/hadoop.\n",
      "---\n",
      " presented the project use cases results to both external customers and dell internal stake holders.\n",
      "---\n",
      " big data analytics with hadoop, hiveql, spark rdd, and spark sql.\n",
      "---\n",
      " intel deep learning training and nvidia deep learning training. senior data scientist consultant optum/united health group - hopkins, mn march 2018 to august 2018 \n",
      "---\n",
      " data science innovative project for united health group big data apps team: research on machine \n",
      "---\n",
      " learning and deep learning algorithms to optimize business performance of health care/insurance.\n",
      "---\n",
      " apply machine learning and deep learning algorithms to match members/claims, using jellyfish, difflib, pandas, pyspark, and scikit_learn\n",
      "---\n",
      " nlp, simhash, fuzzy matching algorithms\n",
      "---\n",
      " optimized the cross-table computations by pandas functions and spark.\n",
      "---\n",
      " performed clustering analysis to recognize patterns and identify outliers and anomaly for provider medical claims, using dbscan, level set tree, hidden markov model, and self-organizing maps. \n",
      "---\n",
      " time series analysis to forecast number of health claims and total claim amount per period. using keras, tensorflow, and arima modeling for time series forecasting\n",
      "---\n",
      " machine learning, decision tree and recommendation modeling on graph networks.\n",
      "---\n",
      " data transformation using hive spark sql and pandas, feature engineering based on pandas and \n",
      "---\n",
      " spark rdd. python developer/data scientist zero one consulting - katy, tx july 2015 to october 2017 \n",
      "---\n",
      " machine learning projects based on python, r, sql, spark mllib and sas. performed data exploratory, data visualizations, feature selections, and model validations. improved the model performance on test data, for example: in a customer segmentation/campaign and credit risk prediction project, the metric scoring was lifted to 0.83 for final model from 0.57 for legacy model.\n",
      "---\n",
      " applications of machine learning algorithms, including knn, nb, ann, random forest and boosted tree, svm, sgd (stochastic gradient descendent regression), pca, singular value decomposition, nltk/textblob, neural network, spark mllib and deep learning using tensorflow/keras, and cntk.\n",
      "---\n",
      " performed time series forecasting in r and python keras, natural language processing, statistical analysis, generated reports, listings and graphs.\n",
      "---\n",
      " big data analytics with hadoop, hiveql, spark rdd, and spark sql.\n",
      "---\n",
      " tested python/sas on aws cloud service and cntk modeling on ms-azure cloud service. geophysical data scientist sclumberger geosolution - houston, tx december 2012 to december 2016 \n",
      "---\n",
      "built prediction models of major subsurface properties for underground image, geologic interpretation and drilling decisions. utilized advanced methods of big data analytics, machine learning, artificial intelligence, wave equation modeling, and statistical analysis. provided exclusive summary on oil/gas seismic data and well profiles, conduct predictive analyses and data mining to support interpretation and operations.\n",
      "---\n",
      " cross-correlation based data analysis method through python and r on multi-offset-well to help predict the models and pore-pressure ahead a little for real time drilling. big data modeling with incorporation of seismic, rock physics, statistical analysis, well logs and geological information into the 'beyond image'.\n",
      "---\n",
      " using python and java, developing, operationalizing, and productionizing machine learning models to make significant impact on the geological pattern identification and subsurface model prediction. analyzing seismic and log data with sub-group analysis (classification-clustering, hybrid approach by pca and som) and model prediction methods (regression, random forest, boosted tree, standard neural network etc.).\n",
      "---\n",
      " use sas statistical regression method to simulate the anisotropic trend.\n",
      "---\n",
      " tested the migrated data processing system on google cloud with velocity model updating tasks.\n",
      "---\n",
      " etl to convert unstructured data to structured data and import the data to hadoop hdfs. utilized mapr as a low-risk big data solution to build a digital oilfield. efficiently integrated and analyzed the data to increase drilling performance and interpretation quality. analyzed sensors and well log data in hdfs with hiveql and prepare for prediction learning models.\n",
      "---\n",
      "constantly monitored the data and models to identify the scope of improvement in the processing and business. manipulated and prepared the data for data visualization and report generation. performed data analysis, statistical analysis, generated reports, listings and graphs.\n",
      "---\n",
      " co-leader of mathematics community 2015, schlumberger eureca. data scientist/advanced software engineer tgs nopec r&d - houston, tx december 2011 to december 2012 \n",
      "---\n",
      " using c/c++ programming language, develop and support modules of grid based model operations and acquisition geometry simulations.\n",
      "---\n",
      " support script based workflows, including i/o of large datasets, calling geophysical modules, and communications of parallel processes.\n",
      "---\n",
      " seismic attributes clustering using unsupervised neural network to help better utilizing big volume of geophysical data for geological feature recognition and oil production risk control.\n",
      "---\n",
      " acquisition geometry simulation using matlab/unix scripts. seismic data processor cggveritas - houston, tx july 2008 to december 2011 \n",
      "---\n",
      " seismic data processing & interpretation\n",
      "---\n",
      " generated high quality subsurface images as a part of the team, with strong problem-solving ability, efficiency communication and corporation.\n",
      "---\n",
      " load acquisition data for processing with oracle database management.\n",
      "---\n",
      " perform data analysis from different domains using both geophysical and statistical methods. detect noisy/bad traces based on anomalous analysis. predict and subtract noise models and surface related multiple models based on pattern reorganization, convolution methods and amplitude ratio identification.\n",
      "---\n",
      " clean/archive data sets on schedule. processed seismic surveys of terabyte-datasets. financial analyst tpr investments, l.p september 2006 to july 2008 \n",
      "---\n",
      " financial and business analytics service using sas and c. generate prediction and regression model using statistical supervised learning methods.\n",
      "---\n",
      " collaborated with the analyst group to analyze the portfolio and evaluate the models. estimate value-at-risk using monte carlo simulation.\n",
      "---\n",
      " performed data analysis, statistical analysis, generated reports, listings and graphs.\n",
      "---\n",
      " sas financial time series autocorrelation analysis and regression using sas/sgplot, proc autoreg, with investigations into the model parameters aic, aicc, mape, r-square, etc.\n",
      "---\n",
      " trained the stock market prediction models and select the model with data mining methods.\n",
      "---\n",
      " selected projects\n",
      "---\n",
      " unsupervised neural network in geologic feature recognition project\n",
      "---\n",
      " -clustering the seismic attributes into meaningful geological categories robustly.\n",
      "---\n",
      " -combination of principal component (pca) and self organizing map (som).\n",
      "---\n",
      " artificial intelligence deep learning project\n",
      "---\n",
      " -lung nodule classification, imagenet/googlenet, object detection by ssd and yolo, deep speech.\n",
      "---\n",
      " -intel neon/tensorflow/keras, performance evaluations.\n",
      "---\n",
      " job salary prediction project\n",
      "---\n",
      " -process the text file with natural language processing tools-nltk, word2vec, and efficiently predict the salary of job information posted online.\n",
      "---\n",
      " - nltk, tensorflow nlp, opennlp/java for feature preparation\n",
      "---\n",
      " regression model selected from random forest, logistic regression and stochastic gradient descent(sgd)\n",
      "---\n",
      " deep learning application in health care\n",
      "---\n",
      " - lung nodule classification by 3d cnn, improved the map from 65% (base line) to 76%.\n",
      "---\n",
      " - spark bigdl/tensorflow/intel neon/keras.\n",
      "---\n",
      " cusomer marketing campaign and risk management project\n",
      "---\n",
      " - predict the charge-off event based on collected custom data for risk management in credit industry.\n",
      "---\n",
      " - compared xgboost, svm, multilayered perceptron (mlp), and random forest. \n",
      "---\n",
      " p h.d of mathematics in teaching assistant rice university - houston, tx may 2006 ms in applied mathematics huazhong university of science& technology - wuhan, cn june 1998 skills python, java, r, sas, sql, hadoop, hive, spark, tensorflow, bigdl, machine learning, deep learning (7 years), python, hadoop, machine learning, r, spark links https://github.com/yuewu000 certifications/licenses sas certified base programmer for sas 9 present sas certified advanced programmer for sas 9 present\n",
      "\n",
      "---\n",
      " i'm a python/bash/linux scientific software hacker and enjoy getting my hands dirty with large amounts of data. i have worked in the public and private sectors in areas ranging from computational chemistry to algorithmic trading to exercise physiology (to name a few).\n",
      "---\n",
      " my \n",
      "---\n",
      " includes liberal arts, geological sciences, chemistry, and geographical information systems (gis). i'm also a published author in fields including applied mathematics, chemical physics, computational drug design, and biophysics.\n",
      "---\n",
      " i've run my own business and worked for a startup or two as well as larger organizations - as long as the work is engaging, i'm happy. if you think i'll make a good addition to your team, don't hesitate to contact me. authorized to work in the us for any employer work experience data scientist bsx athletics - austin, tx december 2015 to june 2016 all-around data scientist, researcher, software developer, and part-time exercise physiologist. i specialized in gluing things together: data, schedules, platforms, and software. i developed, maintained, and implemented software solutions for aggregating, analyzing, and visualizing large amounts of physiological time-series data - and i oversaw the data collection process, too.\n",
      "---\n",
      " i also conducted physiology testing protocols and managed, assimilated, visualized, and interpreted data from those tests. i got my hands dirty in all parts of physiology testing: if you came into our facilities for testing, you know how hands-on i am!\n",
      "---\n",
      " i assisted our marketing and sales teams with data visualization and help explain technical narratives for larger audiences, via channels such as blog posts. i wrote software for our customer service team to ease parts of their workload and streamline server-side data reprocessing workflow.\n",
      "---\n",
      " finally, i supported optical engineering and hardware/firmware development teams when needed. python developer texas water development board - austin, tx june 2015 to december 2015 worked on scientific applications (standalone and web-based). maintained and extended a gui-based application ( python) used for extracting sedimentation rates in reservoirs and lakes monitored by the twdb. assisted in adding functionality and maintaining web-based ( python, flask, celery, ansible, redis, jinja2) applications for real-time monitoring, aggregation, and dissemination of water-related data for the state of texas (water data for texas). gis technician city - austin, tx november 2014 to june 2015 performed routine and advanced (scripting in python/arcpy, spatial analysis, geoprocessing) gis tasks with data relating to watersheds in the austin metropolitan area. worked in desktop and arcsde environments. modeled geochemical evolution of groundwater using phreeqc in a collaboration with the water quality protection division. managing principal bicycle trading, llc - austin, tx march 2011 to july 2014 designed and built all infrastructure supporting automated algorithmic trading of futures, equities, and foreign exchange instruments in the u.s. and europe.\n",
      "---\n",
      " curated extremely large data sets used for time-series analysis of futures and equities products and developed predictive models and wrote proprietary pricing algorithms. administered and\n",
      "---\n",
      " optimized mysql databases. traded own funds, net return 39.80%. software developer enthought, inc. - austin, tx february 2013 to july 2013 developed scientific software for applications in the oil and gas industry. \n",
      "---\n",
      " specialized in integrating proprietary workflows and algorithms with schlumberger's techlog platform.\n",
      "---\n",
      " implemented algorithms for augmenting functionality of techlog to include calculating rock physics properties. research scientist rgm advisors, llc - austin, tx december 2005 to march 2010 led research efforts for expanding the futures asset class in the u.s. and europe. \n",
      "---\n",
      " responsible for building and maintaining models trading 75 symbols on 12 exchanges.\n",
      "---\n",
      " conceived of and implemented novel signal convolution formul\\xe6 that enhanced profitability of equities, futures, and foreign exchange trading by $10mm/year.\n",
      "---\n",
      " directly responsible for $15mm/year in net trading profits. contractor targacept, inc. - winston-salem, nc february 2000 to november 2005 increased computational accuracy and decreased lead discovery time for neuronal nicotine receptor drug targets implicated in alzheimer's disease and schizophrenia.\n",
      "---\n",
      " enhanced computational drug design platform to include quantum mechanical effects via the car-parrinello formalism.\n",
      "---\n",
      " personally constructed and copyrighted a putative crystal structure for human ?7 homopentameric neuronal nicotine receptor, presently licensed and in use. lecturer university of texas - austin, tx august 2004 to january 2005 taught undergraduate course in general chemistry. assistant professor university of colorado - denver, co august 2002 to november 2003 researched drug discovery methods and structure-function relationships in motor proteins. \n",
      "---\n",
      " awarded $2mm nist advanced technology project grant in collaboration with targacept, inc. and princeton university (award number 00-00-5629: new software tool for improving drug discovery and development).\n",
      "---\n",
      " taught undergraduate and graduate courses in general chemistry, computational chemistry, and protein chemistry. postdoctoral fellow and lecturer princeton university - princeton, nj february 2000 to august 2002 worked for roberto car, and designed car-parrinello molecular dynamics simulations to better understand how motor proteins convert chemical energy to mechanical force.\n",
      "---\n",
      " published research findings from investigations of motor protein function that yielded the first evidence of the specific pathway of hydrolysis of atp in myosin.\n",
      "---\n",
      " team-taught (with andy bocarsly and bob cava) undergraduate courses in general chemistry and materials science. postdoctoral fellow and lecturer university of california - san francisco, ca february 1999 to january 2000 worked for peter kollman, and used classical molecular dynamics tools (amber) to simulate the interaction of ligands with motor proteins. collaborated with roger cooke (ucsf) and ed pate (wsu).\n",
      "---\n",
      " taught graduate laboratory course in physical chemistry. \n",
      "---\n",
      " certificate in gis and cartography austin community college district - austin, tx 2014 to 2014 phd in chemistry university of texas - austin, tx 1995 to 1999 ma in geological sciences university of texas - austin, tx 1993 to 1995 ba in international relations lehigh university - bethlehem, pa 1987 to 1991 skills python (5 years), linux (10+ years), machine learning, data mining, signal processing, algorithmic trading, unix administration, scientific programming, high-performance computing, numerical analysis, applied mathematics, arcgis (3 years), research (10+ years), statistical analysis (10+ years) links https://www.linkedin.com/pub/todd-minehardt/b8/330/164 https://github.com/toddjm awards welch teaching excellence award 1995 dreyfus foundation fellowship 1996 dreyfus foundation fellowship 1997 university of texas continuing fellowship 1998 university of california president's postdoctoral fellowship 1999 princeton university council on science and technology postdoctoral fellowship 2000 certifications/licenses geoscientist-in-training december 2014 to december 2016 license git-98. geoscientist-in-training. publications enhanced matrix spectroscopy: the preconditioned green-function block lanczos algorithm http://pre.aps.org/abstract/pre/v56/i4/p4837_1 1997-10 quasi-classical dynamics of benzene overtone relaxation on an ab initio force field: 30-mode models of energy flow and survival probability for ch(v=2) http://www.sciencedirect.com/science/article/pii/s0009261498009944 1998-10-16 quasiclassical dynamics of benzene overtone relaxation on an ab initio force field. i. energy flow and survival probabilities in planar benzene for ch(v = 2,3) http://jcp.aip.org/resource/1/jcpsa6/v109/i19/p8330_s1?isauthorized=no 1998-11-15 quantum dynamics of overtone relaxation in 30-mode benzene: a time-dependent local mode analysis for ch(? = 2) http://jcp.aip.org/resource/1/jcpsa6/v110/i7/p3326_s1?isauthorized=no 1999-02-15 quasi-classical and quantum dynamics of benzene overtone relaxation: early time (t?240 fs) intramolecular vibrational energy redistribution for ch(v=2) in a 15-mode model http://www.sciencedirect.com/science/article/pii/s0009261499001402 1999-04-09 energy partitioning and normal mode analysis of ivr in 30-mode benzene: overtone relaxation for ch(v=2) http://www.sciencedirect.com/science/article/pii/s0009261499002547 1999-04-16 quantum dynamics of intramolecular vibrational energy redistribution for initally excited cc ring modes in 30-mode benzene http://www.sciencedirect.com/science/article/pii/s0009261499009914 1999-10-29 molecular dynamics study of the energetic, mechanistic, and structural implications of a closed phosphate tube in ncd http://www.sciencedirect.com/science/article/pii/s0006349501760924 2001-03 a classical and ab initio study of the interaction of the myosin triphosphate binding domain with atp http://www.sciencedirect.com/science/article/pii/s0006349502754295 2002-02 closing of the nucleotide pocket of kinesin-family motors upon binding to microtubules http://valelab.ucsf.edu/publications/2003nabersci.pdf 2003-05-02 protonation-induced stereoisomerism in nicotine: conformational studies using classical (amber) and ab initio (car\\x96parrinello) molecular dynamics http://link.springer.com/article/10.1007%2fs10822-005-0096-7 2005-01 the open nucleotide pocket of the profilin/actin x-ray structure is unstable and closes in the absence of profilin http://www.sciencedirect.com/science/article/pii/s0006349506724274 2006-04-01\n",
      "\n",
      "---\n",
      " working on contract for cisco cx innovation group to improve the customer experience using the power of data. \n",
      "---\n",
      " designed, implemented and fine-tuned deep bidirectional transformer networks (bert) and xlnet\n",
      "---\n",
      "for the task of classifying sentiments of customer feedback's and net promoter score data.\n",
      "---\n",
      " successfully improved the performance by 3 % on the sentiment classification task by using data augmentation.\n",
      "---\n",
      " experimented with different nlp techniques (word embeddings, lstm seq2seq paraphrase models ) to increase the training dataset in order to improve the\n",
      "---\n",
      "performance of the sentiment classification task on the test data.\n",
      "---\n",
      " helped in designing the guidelines for data labeling for internal cisco data.\n",
      "---\n",
      " set up evaluation environments for speech-to-text libraries (ibm, google, kaldi) for evaluating their performance on customer feedback recordings.\n",
      "---\n",
      " researching on question answering dnn frameworks (seq2seq, transformer attention-based models). python developer trainee integra technologies llc june 2018 to december 2018 \n",
      "---\n",
      " worked as a volunteer to enhance and maintain a complex python framework.\n",
      "---\n",
      " crafted web solutions to resolve technical problems to satisfy customer business needs.\n",
      "---\n",
      " assisted in architecture design and deployment tasks in back-end web development. research assistant rochester institute of technology - rochester, ny january 2017 to december 2017 \n",
      "---\n",
      " assisted in designing and implementing data-driven models at the computational biomedicine lab at rit to solve health domain problems.\n",
      "---\n",
      " suggested and demonstrated state of the art deep learning ideas to solve the problem of classifying unstructured electrocardiogram signals.\n",
      "---\n",
      " designed and improved the performance of a hybrid deep learning architecture comprising of convolutional neural networks and long short-term memory model by 10%. the improved model showed better performance than previous traditional machine learning models.\n",
      "---\n",
      " worked on feature engineering and addressing issues related to highly unbalanced (non-uniformly distributed) datasets.\n",
      "---\n",
      " performed tuning of deep learning models on aws ec2 cloud instances. software developer intern hcl infosystems ltd may 2013 to july 2013 \n",
      "---\n",
      " built an online management system on asp.net framework for arcedu campus.\n",
      "---\n",
      " assisted in integrating the web application with mysql database.\n",
      "---\n",
      " worked in a team of 20 to design and manage web based forms. \n",
      "---\n",
      " masters in computer science rochester institute of technology december 2017 bachelors in computer science and engineering sir padampat singhania university may 2014 skills python (scipy, numpy, keras, pandas, scikit-learn, matplotlib) (3 years), matlab (2 years), java (2 years), apache spark (less than 1 year), keras (1 year), tensorflow (1 year), r (1 year), aws ec2 (less than 1 year), python, hadoop (less than 1 year), aws, mysql, git links https://github.com/niharvanjararit certifications/licenses introduction to big data with apache spark july 2015 to present deep learning specialization july 2018 to present https://www.coursera.org/account/accomplishments/specialization/certificate/byqny5ned2vg. additional information projects\n",
      "---\n",
      " sentiment classification on text \n",
      "---\n",
      " ( python jupyter notebook)\n",
      "---\n",
      " feb 2018 - march 2018\n",
      "---\n",
      " implemented a model which takes a sentence as an input and classifies a sentence as an emotion belonging to one of the 5 classes.\n",
      "---\n",
      " used pretrained glove 50-dimensional vector word embeddings to train an lstm. the model classifies emotions with high accuracy (85%).\n",
      "---\n",
      " image detection and classification of mathematical expressions (crohme dataset) \n",
      "---\n",
      " python \n",
      "---\n",
      " jan2017 \\x96 may 2017 \n",
      "---\n",
      " developed a machine learning pipeline for labeling around 100,000 online handwritten mathematical expression images.\n",
      "---\n",
      " performed segmentation, classification and parsing of images in order to label every expression in the image accurately. \n",
      "---\n",
      " yavis assistant \n",
      "---\n",
      " ibm watson bluemix\n",
      "---\n",
      " python \n",
      "---\n",
      " jan 2017 \\x96 may 2017\n",
      "---\n",
      " created an nlp based ai assistant python application using ibm watson and google calendar rest api\\x92s. \n",
      "---\n",
      " the application can add and retrieve calendar entries through natural language (text).\n",
      "---\n",
      " demand prediction for bakery products ( python)\n",
      "---\n",
      " classification of color fundus diabetic retinopathy images matlab\n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " designed image processing techniques and ml techniques (pca) to extract features from retinal images.\n",
      "---\n",
      " the processed features were then trained on an svm model to accurately detect different stages of diabetic retinopathy.\n",
      "---\n",
      " analysis of intrusion detection system data \n",
      "---\n",
      " matlab, python, java\n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " developed data-driven models (svm, random forest, na\\xefve bayes and neural network) for creating anomaly and misuse based intrusion detection system based on kdd cup data. \n",
      "---\n",
      " the predictive models can distinguish between bad (intrusions) and good connections.\n",
      "---\n",
      " demand prediction of bakery goods \n",
      "---\n",
      " python \n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " designed predictive models (linear regression) on sales data for maximizing sales and minimizing returns for bakery goods. \n",
      "---\n",
      " data integration was performed to join multiple tables.\n",
      "---\n",
      " classification of diabetes data \n",
      "---\n",
      " weka \n",
      "---\n",
      " january 2016 \\x96 may 2016\n",
      "---\n",
      " performed data cleaning and preparation on the uci diabetes dataset having 100,000 instances and 55 attributes. \n",
      "---\n",
      " performed feature extraction and dimensionality reduction and tested various classification models.\n",
      "\n",
      "---\n",
      " developing machine learning models to classify customer's intent and provide real-time recommendation to customer care agents on chat.\n",
      "---\n",
      " designed metrics and developed processes to evaluate the performance of the recommendation system and to get continuous data-driven\n",
      "---\n",
      " insights for areas of improvements.\n",
      "---\n",
      " setup annotation schemes and use active learning to prioritize data to be labeled for significant system improvement.\n",
      "---\n",
      " the improved dialogue-based recommendation system lead to 17% lift in sales for our telecommunication clients.\n",
      "---\n",
      " worked with clustering, classification (random forest, svm, rnn and more), word embedding's, topic modeling and more. software developer ( python, java, aws, git) niki.ai - bengaluru, karnataka october 2015 to july 2016 \n",
      "---\n",
      " conceptualised and implemented the nlp backend for users to shop using the chatbot in simple human understandable messages.\n",
      "---\n",
      " improved understanding capability of the chatbot by around 30% by standardizing entity extraction across the application.\n",
      "---\n",
      " developed lightweight restful api's to interact with the merchants and users to resolve user queries from android application. summer intern ( python) the university of sheffield - sheffield june 2015 to september 2015 \n",
      "---\n",
      " developed an algorithm to summarize comments on online newspapers. used lda to cluster user comments according to topics\n",
      "---\n",
      " ranked comments in each cluster using pagerank. top ranked comment from each cluster were combined to form a summary.\n",
      "---\n",
      " summaries formed were significantly better than baseline system at significance level of 0.05. research assistant ( python, terrier search engine) da-iict - gandhinagar, gujarat january 2014 to april 2014 \n",
      "---\n",
      " developed an algorithm for a legal recommendation system to find and summarize the past legal cases similar to the searched case.\n",
      "---\n",
      " expanded queries using blind and actual relevance feedback for efficient retrieval of documents. \n",
      "---\n",
      " master's in data science illinois institute of technology - chicago, il march 2016 to december 2017 master's in advanced computer science university of sheffield - sheffield september 2014 to september 2015 bachelor's in information and communication technology dhirubhai ambani institute of information and communication technology - gandhinagar, gujarat july 2010 to may 2014 skills git, hadoop, hive, python, keras, matplotlib, numpy, pandas, tensorflow, machine learning, hadoop, nlp, mysql, postgresql, java, sql, nltk, keras, scikit-learn, gensim, aws, stanford corenlp, hypothesis testing, spark, r links https://www.linkedin.com/in/ishita-agarwal-ds https://github.com/github-ish\n",
      "\n",
      "---\n",
      " implemented full lifecycle in data modeler/ data analyst, data warehouses and datamart's with star schemas, snowflake schemas, and scd& dimensional modeling erwin\n",
      "---\n",
      " gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis.\n",
      "---\n",
      " update python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " analyze data from aws redshift database in python using psycopg2, pandas and numpy module\n",
      "---\n",
      " use aws python sdk to download data from dynamodb.\n",
      "---\n",
      " use python mysql client to download data from aws rds mysql database. wrote sql queries to analyze transactional and customer data from mysql database.\n",
      "---\n",
      " use spark-context to analyze dataset and explore data using lambda function.\n",
      "---\n",
      " develop mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws.\n",
      "---\n",
      " implement end-to-end systems for data analytics, data automation and integrated with custom visualization tools using r, python and hadoop\n",
      "---\n",
      " create data quality scripts using sql and hive to validate successful data load and quality of the data.\n",
      "---\n",
      " create several types of data visualizations using python and tableau.\n",
      "---\n",
      " perform data analysis by using hive to retrieve the data from the hadoop cluster, sql to retrieve data from oracle database.\n",
      "---\n",
      " wrangle data, worked on large datasets (acquired data and cleaned the data), analyzed trends by making visualizations using mat plot lib and python.\n",
      "---\n",
      " oversees the mapping of the integrating process for data into power bi tool and reporting platforms from the data warehouse.\n",
      "---\n",
      " create stored procedures in various complexities for ssrs report datasets and power bi dashboards.\n",
      "---\n",
      " created sub-reports, drilldown-reports, summary reports, and parameterized reports in ssrs\n",
      "---\n",
      " develop data mapping documentation to establish relationships between source and target tables including transformation processes using sql.\n",
      "---\n",
      " tableau is used for analyzing the data to show the trends, variations and density of the data in form of graphs and charts. tableau was connected to files, relational and big data sources to acquire and process data.\n",
      "---\n",
      " build and maintain sql scripts, indexes, and complex queries for data analysis and extraction.\n",
      "---\n",
      " environment: python, dynamodb, mysql, sql, aws rds, aws python sdk, spark, lambda function, map-reduce, jupyter notebook, powerbi, ssrs, tableau data scientist morgan stanley - baltimore, md january 2017 to september 2018 \n",
      "---\n",
      " created 3nf business area data modeling with de-normalized physical implementation data and information requirements analysis using erwin tool.\n",
      "---\n",
      " participated in all phases of data mining\n",
      "---\n",
      " data collection, data cleaning, developing models, validation, visualization and performed gap analysis.\n",
      "---\n",
      " built analytical data pipelines to port data in and out of hadoop/hdfs from structured and unstructured sources and designed and implemented system architecture for amazon ec2 based cloud-hosted solution for client.\n",
      "---\n",
      " performed data analysis by retrieving the data from the hadoop cluster.\n",
      "---\n",
      " created logical and physical data models with star and snowflake schema techniques using erwin in datawarehouse as well as in data mart.\n",
      "---\n",
      " involved in data analysis, data validation, data cleansing, data verification and identifying data mismatch.\n",
      "---\n",
      " resolved the data related issues such as: assessing data quality, data consolidation, evaluating existing data sources.\n",
      "---\n",
      " participated in data modeling discussion and provided inputs on both logical and physical data modelling.\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " collaborated with data engineers and operation team to implement the etl process, wrote and optimized sql queries to perform data extraction to fit the analytical requirements.\n",
      "---\n",
      " involved in creating various reports like drill through, drill down and ad-hoc according to the user requirement using sql server reporting services (ssrs).\n",
      "---\n",
      " tools: hadoop, map-reduce, tableau, python \n",
      "---\n",
      " master's skills mapreduce, clustering, hadoop, data analysis, mysql, sql, git, hadoop, json, mapreduce, python, ggplot2, matplotlib, numpy, pandas, shiny, anova, boosting, decision trees, dimensionality reduction\n",
      "\n",
      "---\n",
      " software professional and data scientist with over 3 years strong experience in performing development and basic data science tasks such as data exploration, cleaning and transforming data, automate the data ingestion of common formats, applying optimization algorithms to analyze data and perform data modeling, performance evaluation and application of machine-learning techniques, clustering and classification techniques within the information technology industry.\n",
      "---\n",
      " possess strong knowledge of computer science concepts, such as python, java, r, big data (hadoop, spark), hive, pig, scala, git, matlab, various data modeling techniques (linear/logistic regression, clustering) training and evaluating models. proficient in python (pandas, scikit-learn), excel, sql, nlp, elk stack and machine learning techniques (knn, svm, dts, rf).\n",
      "---\n",
      " knowledgeable in deep learning (tensorflow, keras) and amazon ec2, etc. work experience python developer/data scientist infomerica inc october 2017 to present \n",
      "---\n",
      " responsible for python development and basic data science tasks, such as data exploration, cleaning and transforming data, automate the data ingestion of common formats, applying optimization algorithms to analyze data and output well formed json to an ingestion service and usage of variety of data visualization tools to visualize and report on results within the information technology department of a $25m organization in the advertising & marketing industry.\n",
      "---\n",
      " processing, cleansing, and verifying the integrity of data used for analysis before ingestion.\n",
      "---\n",
      " develop python code to automate the ingestion of common formats such as json, csv by using logstash from elastic search to kibana dashboard to be viewed by clients.\n",
      "---\n",
      " develop dashboards using advanced visualization techniques and data capabilities such as embedding analytics in automobile and business process tools or designing dashboards tailored to strategic goals.\n",
      "---\n",
      " generate reports and analytics using python and the pandas framework of automobile datasets as per client requirements.\n",
      "---\n",
      " building machine learning models from development through testing and validation to 10+ million records in production\n",
      "---\n",
      " environment: python, elastic search, logstash, kibana, json, csv, pandas, numpy, machine learning models, matplotlib, seaborn, scikit-learn, rest api\n",
      "---\n",
      " achievements:\n",
      "---\n",
      " successfully developed and implemented a python script to automatically generate visualization reports for every dataset. this automated process has resulted in a significant time savings of two hour per week. python/odoo developer computech corporation june 2017 to september 2017 \n",
      "---\n",
      " responsible for python development and basic odoo erp tasks, such as architect, design, develop and testing new modules within the odoo framework, maintained employees security management databases, created a work flow using technologies such as git/ssh to develop multi -programmer, integrate odoo with other web applications and edit existing odoo modules within the information technology department of a $50m organization in the computer hardware & software industry.\n",
      "---\n",
      " developing etl workflows for structured and unstructured sets of large data related to employees for ingesting into odoo.\n",
      "---\n",
      " building machine learning models from development through testing and validation to a significant number of employees in production\n",
      "---\n",
      " environment: python 2.7, mysql, postgresql, odoo 10 framework, html5, css, json, agile, vmware player, javascript, web services, sdlc\n",
      "---\n",
      " achievements:\n",
      "---\n",
      " successfully developed odoo modules in python for hr security access. this enhanced the security authentication of employees in odoo erp which resulted in significant time savings of one hour per day. python developer softtech systems january 2017 to may 2017 \n",
      "---\n",
      " the brief outline of the project was to understand the sentiments of people for predicting stock market.\n",
      "---\n",
      " worked on two different algorithms namely svm and tensor flow to understand the tweets of people and generate a prediction of stock market based on sentiment of tweets.\n",
      "---\n",
      " environment: python, twitter api, keras, tensor flow programmer/designer/product manager - senior design softtech systems july 2016 to december 2016 online intrusion alert aggregation with data stream modelling jul 2016 - dec 2016\n",
      "---\n",
      " programmer/designer/product manager - senior design project to provide a platform for network security administrators for detecting intrusions.\n",
      "---\n",
      " involved in design, development and idea of the web interactive interface to reach multiple platform users\n",
      "---\n",
      " environment: java 1.7, awt, swing, mysql programmer/designer softtech systems september 2015 to june 2016 - a project that uses nasa api dataset to predict the mars images and estimate the time complexity of machine learning libraries.\n",
      "---\n",
      " responsible for implementing kmeans and svm libraries in anaconda python for training and predicting the images.\n",
      "---\n",
      " environment: python, apache spark, scala, svm, kmeans \n",
      "---\n",
      " technology jawaharlal nehru technological university - hyderabad, telangana august 2011 to may 2015\n",
      "\n",
      "---\n",
      " played a key role to solve complex problems, pivotal to nationwide's business and drive customer insights from petabytes of data.\n",
      "---\n",
      " extracting the data from different sources.\n",
      "---\n",
      " build and train scalable models using multiple regression and enabling re-use for future project\n",
      "---\n",
      " analyzed large, complex data sets by developing advanced statistical and machine learning models based on business initiatives.\n",
      "---\n",
      " extensively used random forest for regression.\n",
      "---\n",
      " building models according to the training data.\n",
      "---\n",
      " applying statistical analysis on the given training data. data scientist nationwide insurance - columbus, oh november 2016 to november 2017 project overview: pim (product investment manager) is an application used in express. our goal is analyzing customers to shop across multiple channels in real time and make difference in their purchase decisions.\n",
      "---\n",
      " extracting the data from different db's\n",
      "---\n",
      " experience building highly scalable solutions in data storage, real-time analysis and reporting for multi-terabyte data sets\n",
      "---\n",
      " synthesize data from multiple sources for a clear understanding of opportunities.\n",
      "---\n",
      " segmenting the data based on their z-scores.\n",
      "---\n",
      " build the database, design data flows, automate stored procedures of scripts for global retail-trading firm where \n",
      "---\n",
      " plotting graphs for different scenarios and analyzing the mean square error.\n",
      "---\n",
      " conducted predictive model analysis and portfolio back testing based on etl process in ms database system. python developer cme - bengaluru, karnataka july 2012 to december 2015 overview: minerva is a search engine application for the next generation data management so that it can be used for wide variety of cross-platforms within the organization. we were mainly into the backend development of the application.\n",
      "---\n",
      " designed the application using python, html, css, ajax, json and jquery. worked on backend of the application.\n",
      "---\n",
      " responsible for setting up rest api and integrating with application using django\n",
      "---\n",
      " involved in writing sql queries implementing functions, triggers, cursors, object types, sequences, indexes etc.\n",
      "---\n",
      " created and managed all of hosted or local repositories through source tree's simple interface of git client, collaborated with git command lines and stash.\n",
      "---\n",
      " utilized continuous integration and automated deployments with jenkins.\n",
      "\n",
      "---\n",
      " software professional and data scientist with over 2 years strong experience in performing development and basic data science tasks such as data exploration, cleaning and transforming data, automate the data ingestion of common formats, applying optimization algorithms to analyze data and perform data modeling, performance evaluation and application of machine-learning techniques, clustering and classification techniques within the information technology industry.\n",
      "---\n",
      " possess strong knowledge of computer science concepts, such as python, java, r, big data (hadoop, spark), hive, pig, scala, git, matlab, various data modeling techniques (linear/logistic regression, clustering) training and evaluating models. proficient in python(pandas, scikit-learn), excel, sql, nlp, elk stack and machine learning techniques(knn, svm, dts, rf).\n",
      "---\n",
      " fluent in english, hindi and telugu, verbal and written communication.also knowledgeable in deep learning(tensorflow, keras) and amazon ec2, etc. authorized to work in the us for any employer work experience python developer/data scientist infomerica inc october 2017 to present \n",
      "---\n",
      " responsible for python development and basic data science tasks, such as data exploration, cleaning and transforming data, automate the data ingestion of common formats, applying optimization algorithms to analyze data and output well formed json to an ingestion service and usage of variety of data visualisation tools to visualise and report on results within the information technology department of a $25m organization in the advertising & marketing industry.\n",
      "---\n",
      " processing, cleansing, and verifying the integrity of data used for analysis before ingestion.\n",
      "---\n",
      " develop python code to automate the ingestion of common formats such as json, csv by using logstash from elasticsearch to kibana dashboard to be viewed by clients.\n",
      "---\n",
      " data mining, optimization algorithms to analyze data on top of hadoop/hive\n",
      "---\n",
      " develop dashboards using advanced visualization techniques and data capabilities such as embedding analytics in automobile and business process tools or designing dashboards tailored to strategic goals.\n",
      "---\n",
      " generate reports and analytics using python and the pandas framework of automobile datasets as per client requirements.\n",
      "---\n",
      " building machine learning models from development through testing and validation to 10+ million records in production\n",
      "---\n",
      " environment: python, elasticsearch, logstash, kibana, json, csv, pandas, numpy, machine learning models, matplotlib, seaborn, scikit-learn, rest api\n",
      "---\n",
      " achievements:\n",
      "---\n",
      " successfully developed and implemented a python script to automatically generate visualization reports for every dataset.this automated process has resulted in a significant time savings of two hour per week. python/odoo developer computech corporation june 2017 to september 2017 \n",
      "---\n",
      " responsible for python development and basic odoo erp tasks, such as architect, design, develop and testing new modules within the odoo framework, maintained employees security management databases, created a work flow using technologies such as git/ssh to develop multi -programmer, integrate odoo with other web applications and edit existing odoo modules within the information technology department of a $50m organization in the computer hardware & software industry.\n",
      "---\n",
      " developing etl workflows for structured and unstructured sets of large data related to employees for ingesting into odoo.\n",
      "---\n",
      " building machine learning models from development through testing and validation to a significant number of employees in production\n",
      "---\n",
      " environment: python 2.7, mysql, postgresql, odoo 10 framework, html5, css, json, agile, vmware player, javascript, web services, sdlc\n",
      "---\n",
      " achievements:\n",
      "---\n",
      " successfully developed odoo modules in python for hr security access.this enhanced the security authentication of employees in odoo erp which resulted in significant time savings of one hour per day. stock market prediction january 2017 to may 2017 \n",
      "---\n",
      " the brief outline of the project was to understand the sentiments of people for predicting stock market.\n",
      "---\n",
      " worked on two different algorithms namely svm and tensorflow to understand the tweets of people and generate a prediction of stock market based on sentiment of tweets.\n",
      "---\n",
      " environment: python, twitter api, keras, tensorflow programmer/designer/product manager online intrusion alert aggregation july 2016 to december 2016 - senior design project to provide a platform for network security administrators for detecting intrusions.\n",
      "---\n",
      " involved in design, development and idea of the web interactive interface to reach multiple platform users\n",
      "---\n",
      " environment: java 1.7, awt, swing, mysql graduate teaching assistant suny polytechnic institute september 2015 to december 2016 responsible for python development and basic tasks such as assisting with computer science courses( c, python and java programming), helped create curriculum, composed exams and term paper assignments, led weekly discussion sections, graded all written work and determined final grades. within the computer science department of a $10m organization in the colleges & universities industry. programmer/designer nasa september 2015 to june 2016 - a project that uses nasa api dataset to predict the mars images and estimate the time complexity of machine learning libraries.\n",
      "---\n",
      " responsible for implementing kmeansand svm librariesin anaconda pythonfortraining and predicting theimages\n",
      "---\n",
      " environment: python, apache spark, scala, svm, kmeans \n",
      "---\n",
      " masters in computer science in computer science state university of new york - utica, ny august 2015 to may 2017 technology jawaharlal nehru technological university - hyderabad, telangana august 2011 to may 2015 links https://github.com/ghattim https://www.linkedin.com/in/mahesh-ghatti\n",
      "\n",
      "---\n",
      " + extensive experience in text analytics, developing different statistical machine learning, datamining solutions to various business problems and generating data visualizations using r, python, and tableau.\n",
      "---\n",
      " + experience with machine learning techniques and algorithms (such as k-nn, naive bayes, etc.)\n",
      "---\n",
      " + experience object-oriented programming (oop) concepts using python, c++ and php.\n",
      "---\n",
      " + extensively worked on statistical analysis tools and adept at writing code in advanced excel, r, matlab, python.\n",
      "---\n",
      " + experienced the full software lifecycle in sdlc, agile and scrum methodologies.\n",
      "---\n",
      " + implemented deep learning models and numerical computation with the help of data flow graphs using tensor flow machine learning.\n",
      "---\n",
      " + skilled in using dplyr and pandas in r and python for performing exploratory data analysis.\n",
      "---\n",
      " + i'm a result oriented and self-motivated individual who can adapt to the fast-changing industry.\n",
      "---\n",
      " + experience in discovery of data, cleansing of the data, collecting data and retrieve data using mysql.\n",
      "---\n",
      " + analysis of data from loggers and trend data from building management systems. exploring several types of regression, anova, hypothesis testing and correlation between the data using minitab (statistical software) and r.\n",
      "---\n",
      " + proficient knowledge of statistics, mathematics, machine learning, recommendation algorithms and analytics with an excellent understanding of business operations and analytics tools for effective analysis of data.\n",
      "---\n",
      " + familiar on deep learning projects for image identification cnn, rnn for stock price prediction autoencoders for movie recommender system (pytorch), image captioning (cnn-rnn autoencoder architecture). work experience data scientist davita kidney care - denver, co october 2018 to present analyzed large datasets apply machine learning techniques and developed predictive models, statistical models and developing and enhanced statistical models.\n",
      "---\n",
      " + used pandas, numpy, scipy, matplotlib, scikit-learn, in python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, random forests, lstm, & rnn for data analysis.\n",
      "---\n",
      " + developed rnn models for predicting the current hemoglobin level of the patients for the ninety first day.\n",
      "---\n",
      " + built lstm and ann models for predicting the loss.\n",
      "---\n",
      " + used jenkins pipeline to run and deploy the code.\n",
      "---\n",
      " + used jupyter notebook for deploying locally.\n",
      "---\n",
      " + have used the tools kibana, and bit-bucket as repository.\n",
      "---\n",
      " + have done feature selection and avoiding missing values used supervised learning such as linear regression, random forest.\n",
      "---\n",
      " + conducted studies, rapid plots and using advance data mining and statistical modelling techniques to build a solution that optimize the quality and performance of data. data scientist/ machine learning engineer united health group - minneapolis, mn september 2017 to september 2018 provided the architectural leadership in shaping strategic, business technology projects, with an emphasis on application architecture.\n",
      "---\n",
      " + participated in all phases of data mining, data collection, data cleaning, developing models, validation, and visualization and performed gap analysis.\n",
      "---\n",
      " + used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, nltk in python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " + analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.\n",
      "---\n",
      " + conducted studies, rapid plots and using advance data mining and statistical modelling techniques to build a solution that optimize the quality and performance of data.\n",
      "---\n",
      " + developed mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws. implemented a python-based distributed random forest via python streaming.\n",
      "---\n",
      " + developed mapreduce/spark python modules for machine learning & predictive analytics in hadoop on aws.\n",
      "---\n",
      " + utilized spark, scala, hadoop, hbase, kafka, spark streaming, mllib, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. python developer myntra - bengaluru, karnataka march 2013 to july 2015 worked on the project from gathering requirements to developing the entire application.\n",
      "---\n",
      " + worked on anaconda python environment.\n",
      "---\n",
      " + created, activated and programmed in anaconda environment.\n",
      "---\n",
      " + wrote programs for performance calculations using numpy and sql alchemy.\n",
      "---\n",
      " + wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " + extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " + worked on development of sql and stored procedures on mysql\n",
      "---\n",
      " + design and build a text classification application using different text classification models.\n",
      "---\n",
      " + used jira for defect tracking and project management\n",
      "---\n",
      " + involved in sprint planning sessions and participated in the daily agile scrum meetings\n",
      "---\n",
      " + performed qa testing on the application. python developer cmc - hyderabad, telangana june 2012 to january 2013 \n",
      "---\n",
      " designed and developed the user interface of the project with html, css and javascript \n",
      "---\n",
      " written python scripts to parse the xml documents and load the data in database. \n",
      "---\n",
      " client-side validations and manipulations are done using javascript and jquery\n",
      "---\n",
      " experienced in writing indexes, views, constraints, stored procedures, triggers, cursors and user defined functions or subroutines in mysql.\n",
      "---\n",
      " responsible for debugging and troubleshooting the application. \n",
      "---\n",
      " utilized subversion control tool to coordinate team work. \n",
      "---\n",
      " used selenium libraries to write fully functioning test automation process. \n",
      "---\n",
      " mba in finance and marketing acharya academy of management studies 2013 to 2015 bachelor of technology in electrical and electronics engineering usha rama college of engineering and technology 2008 to 2012 skills python (3 years), matplotlib (1 year), numpy (3 years), machine learning (3 years), sql (2 years), visio, business intelligence, microsoft office, excel, html, testing, deep learning (1 year), nlp (less than 1 year), tensorflow (less than 1 year), pytorch (less than 1 year), regression (1 year), random forest (1 year) additional information skills\n",
      "---\n",
      " languages c, data structures, python, r, sql, matlab\n",
      "---\n",
      " python libraries numpy, pandas, matplotlib, scikitlearn, keras, pytorch, tensorflow\n",
      "---\n",
      " development ide jupyter notebook, spyder, pycharm\n",
      "---\n",
      " machine learning linear regression, logistic regression, decision tree, svm, naive bayes, knn\n",
      "---\n",
      " rnn, deep learning\n",
      "---\n",
      " algorithms k-means, random forest, dimensionality regression algorithms\n",
      "\n",
      "---\n",
      " migrating inhouse projects onto cloud like snowflake, redshift, s3 and databricks.\n",
      "---\n",
      " familiarity and comfort with most aspects of python3 including object-oriented programming and the data science stack, wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " wrote flexible production-ready code (in r or python) for different components of the data science process such as data ingestion, data manipulation, feature selection & engineering, model training, model validation, and model deployment.\n",
      "---\n",
      " good experience working with financial data and creating calculated fields extracted from diverse sources.\n",
      "---\n",
      " deep understanding of modern machine learning techniques and their mathematical underpinnings, such as classification, recommendation systems, and natural language processing.\n",
      "---\n",
      " strong understanding and application of statistical methods and skills like finding central tendency, a measure of dispersion using various distributions, experimental design, variance analysis, a/b testing, and regression.\n",
      "---\n",
      " exposed solving both classification and regression problems from messy data using machine learning algorithms.\n",
      "---\n",
      " experienced building natural language processing models using linear classifiers like fasttext.\n",
      "---\n",
      " delivered business value by translating complex data from bmc remedy tool into meaningful insights using visualization tools (tableau).\n",
      "---\n",
      " proficient with data visualization and translating complex problems into actionable insights.\n",
      "---\n",
      " implemented python to retrieve and manipulate data, visualize the data for key findings and explained to clients.\n",
      "---\n",
      " worked on io operations of different file formats like csv, excel, json, txt using python.\n",
      "---\n",
      " good experience with pyspark mainly for getting data from diverse sources for data analysis.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " worked with business groups and stakeholders to ensure models can be implemented as part of a delivered solution replicable across many clients with no impact on business.\n",
      "---\n",
      " presented key findings to stakeholders to drive improvements and solutions from concept through to delivery.\n",
      "---\n",
      " displayed strong teamwork and interpersonal skills with the ability to communicate to all levels of management.\n",
      "---\n",
      " implemented sql scripts, stored procedures and triggers in mysql server to handle user requests and work with the data in the database.\n",
      "---\n",
      " experienced documenting all modeling steps in a systematic way including modeling process, insights generated, model validation results and checklists built in the project.\n",
      "---\n",
      " develop software to automate operational processes along with coding for the shared engineering deliverables \n",
      "---\n",
      " experience in configuring, deploying the web applications on aws servers.\n",
      "---\n",
      " performed configuration, deployment and support of cloud services including amazon web services (aws).\n",
      "---\n",
      " created a database using mysql, wrote several queries to extract data from the database.\n",
      "---\n",
      " wrote and executed various mysql database queries from python using python -mysql connector and mysql.\n",
      "---\n",
      " involved in the development of web services using soap for sending and getting data from the external interface.\n",
      "---\n",
      " provide technical guidance, recommendations, and resolutions to new team members and built a new team.\n",
      "---\n",
      " good exposure and business knowledge on aws services like s3, ec2, amazon redshift, kinesis and ml azure. work experience data scientist capital one february 2019 to present \n",
      "---\n",
      " migrating in-house databases onto aws cloud, snowflake and running the jobs on databricks.\n",
      "---\n",
      " identify, analyze, and interpret patterns in complex data sets using databricks and tableau.\n",
      "---\n",
      " developing python models to identify high-risk customers and performing comparative analysis over time to highlight high-risk customers variations.\n",
      "---\n",
      " creating a data pipeline on aws cloud which allows flexible data movement between different data sources.\n",
      "---\n",
      " moving sql and oracle compliance data onto aws redshift and performing scaling and data validity.\n",
      "---\n",
      " using python database connectors to connect aws redshift and perform sql operations from cli.\n",
      "---\n",
      " creating dashboards, connecting dynamodb to tableau and analyzing the changes in realtime.\n",
      "---\n",
      " good experience in data blending, data joining, applying filters and proving granularity in tableau.\n",
      "---\n",
      " experience using python logger module to keep track of different warnings, info, debug messages when connected to databases like dynamodb, sql and redshift. this info is basically stored as a log file. data scientist tamuc - commerce, tx may 2017 to january 2019 recruited to work as a data scientist to do requirement gathering and hypothesis testing for new implementations in websites. i was also responsible to perform statistical analysis on the datasets we receive and find the relationship between the features and target variables. i also worked on a project non-toxic text classification to stop cyberbullying over the social media network. we implemented using fasttext linear classifier algorithm and tested its accuracy using other machine learning algorithms. i also worked with dr. isaac gang as a data engineer to maintain \"we teach cs collaborative\" database.\n",
      "---\n",
      " key contributions: \n",
      "---\n",
      " scrapped data from various websites, cleaned and processed unstructured data to structured data and performed exploratory data analysis using python libraries like pandas, numpy, matplotlib, seaborn.\n",
      "---\n",
      " translated complex unstructured data and analysed the results into compelling visualizations and stories that clearly articulated the strategy and follow-up actions to the stakeholders.\n",
      "---\n",
      " downloaded twitter tweets using twitter api, trained manually, segregated positive and negative tweets.\n",
      "---\n",
      " performed hypothesis testing and statistical analysis of various data sets using python libraries and mathematically to find the acceptance and rejection percentage on the null hypothesis and alternate hypothesis.\n",
      "---\n",
      " used supervised learning algorithms and natural language processing to analyze the level of toxicity in the text, particularly in social networks and stop cyberbullying.\n",
      "---\n",
      " worked as data engineer for \"texas a&m university-commerce we teach_cs collaborative\" funded by uta.\n",
      "---\n",
      " created dashboards and reports to regularly communicate results and monitor key metrics.\n",
      "---\n",
      " worked closely with various departments to track the survey results and find the insights.\n",
      "---\n",
      " responsible to monitor computer science department website changes and icel server.\n",
      "---\n",
      " analyzed the clickstream data and reported the findings to all the departments in form of charts.\n",
      "---\n",
      " conducted text analysis on large datasets of unstructured text and performed statistical analysis using a variety of statistical tools like minitab and python. data analyst/ python developer hewlett packard enterprise - bengaluru, karnataka august 2014 to december 2016 key contributions: \n",
      "---\n",
      " worked independently on complex data analysis, business analytics & data mining tasks on big datasets.\n",
      "---\n",
      " analyzed structured and unstructured data from incidents with complex statistical analysis to find the incident origin and customer behavior towards the incidents.\n",
      "---\n",
      " experienced in collecting data from multiple databases and websites, validated data integrity and accuracy.\n",
      "---\n",
      " leveraged data and performed intensive analysis across all areas of our business to drive growth strategies, including product development and rider engagement strategies.\n",
      "---\n",
      " communicated with clients and store managers in a clear and concise manner for actionable changes focused on improving velocity, predictability, and efficiency of each store.\n",
      "---\n",
      " developed and implemented a database monitoring system web page automation using flask and django which reduced 4 hours of amos team manual work.\n",
      "---\n",
      " used legacy mainframes, sql, unix scripting, autosys, bmc remedy as part of project enhancement.\n",
      "---\n",
      " responsible for all aspects of the software development life cycle for the applicable projects, including gathering requirements, design, implementation, and deployment.\n",
      "---\n",
      " automated many amos tasks reducing 40% manual effort and 30% ticket count increasing efficiency.\n",
      "---\n",
      " experienced using databases such as postgres and mysql.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options \n",
      "---\n",
      " built sql queries to perform various crud operations with ddl, dml, dcl and tcl commands.\n",
      "---\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "---\n",
      " honors and awards ? excellence award- awarded by mr. ben wishart, ahold cio, for maintaining a good relationship with clients and stakeholders.\n",
      "---\n",
      " ? star performer- awarded by ravi kumar, ahold project manager, hewlett packard.\n",
      "---\n",
      " ? conduct of appreciation- awarded by dr. isaac gang, founder of \"weteachcs texas a&m collaborative\" for working as a data engineer in this project.\n",
      "---\n",
      " ? texas a&m university computer science scholarship- awarded for 2 years of master's \n",
      "---\n",
      " for showing good academic and research work on campus. \n",
      "---\n",
      " m.s. in computer science and information systems. in commerce texas a&m university 2018 bachelors in electronics and communication engineering in electronics and communication engineering jawaharlal nehru technological university 2014 skills testing, excel, business intelligence, access, sql additional information areas of expertise include:\n",
      "---\n",
      " python\n",
      "---\n",
      " pyspark\n",
      "---\n",
      " sql\n",
      "---\n",
      " tableau\n",
      "---\n",
      " minitab\n",
      "---\n",
      " statistical modeling\n",
      "---\n",
      " natural language processing\n",
      "---\n",
      " snowflake\n",
      "---\n",
      " pandas\n",
      "---\n",
      " numpy\n",
      "---\n",
      " scikitlearn\n",
      "---\n",
      " keras\n",
      "---\n",
      " matplotlib\n",
      "---\n",
      " nltk\n",
      "---\n",
      " postgresql\n",
      "---\n",
      " aws (ec2, redshift, s3 etc.)\n",
      "---\n",
      " machine learning algorithms\n",
      "---\n",
      " decision trees\n",
      "---\n",
      " random forest\n",
      "---\n",
      " rnn\n",
      "---\n",
      " gradient descent\n",
      "---\n",
      " na\\xefve bayes\n",
      "---\n",
      " logistic regression\n",
      "---\n",
      " collaborative filtering\n",
      "\n",
      "---\n",
      " 8+years of experience in datascience and analytics including machinelearning, datamining, datablending&statisticalanalysis.\n",
      "---\n",
      " over 5+experience with machinelearning techniques and algorithms (such as k-nn, naivebayes, etc).\n",
      "---\n",
      " experience in aws (amazonwebservices)ec2, vpc, iam, iam, s3, cloudfront, cloudwatch, cloudformation, glacier, rdsconfig, route53, sns, sqs, elasticcache.\n",
      "---\n",
      " azurecloud extensive full cycle cloudazure experience with full bigdata, elasticsearch and solr, machinelearning and deeplearning development and deployment.\n",
      "---\n",
      " experienced with machinelearningalgorithms such as logisticregression, randomforest, xgboost, knn, svm, neuralnetwork, linearregression, lassoregression,andk-means.\n",
      "---\n",
      " expertise in synthesizing machinelearning, predictiveanalytics and bigdata technologies like hadoop, hive, pig.\n",
      "---\n",
      " strong skills in statistical methodologies such as a/btest, experimentdesign, hypothesistest, anova.\n",
      "---\n",
      " experience in implementing data analysis with various analytic tools, such as anaconda 4.0,r 3.0 (ggplot2, caret, dplyr) and excel.\n",
      "---\n",
      " solid ability to write and optimize diverse sqlqueries, working knowledge of rdbms like sqlserver2008\n",
      "---\n",
      " experience in bigdata technologies like spark1.6, sparksql, pyspark, hadoop2.x, hdfs, hive1.x.\n",
      "---\n",
      " experience in visualization tools liketableau9.x, 10.x, datablendingfor creating dashboards.\n",
      "---\n",
      " proficient in predictivemodeling, dataminingmethods, factoranalysis, anova, hypotheticaltesting, normal distribution and other advanced statistical and econometric techniques.\n",
      "---\n",
      " developed predictive models using decisiontree, randomforest, naivebayes, logisticregression, clusteranalysis, and neuralnetworks.\n",
      "---\n",
      " excellent knowledge of machine learning, mathematical modeling and operations research. comfortable with r, python, sas , matlab, relationaldatabases. deep understanding & exposure of bigdataeco-system.\n",
      "---\n",
      " expert in creating pl/sqlschemaobjects like packages, procedures, functions, subprograms, triggers, views, materializedviews, indexes, constraints, sequences, exceptionhandling, dynamicsql/cursors, nativecompilation, collectiontypes, recordtype, objecttype using sqldeveloper.\n",
      "---\n",
      " hands on experience in implementing modelviewcontrol (mvc) architecture using spring, jdk, corejava (collections, oopsconcepts), jsp, servlets, struts, springs, hibernate, jdbcand provided serveradministrator duties logicalposition.\n",
      "---\n",
      " worked with complex applications such as r, sas, matlab,and spss to developeda neuralnetwork, cluster analysis.\n",
      "---\n",
      " experienced in dataintegrationvalidation and dataquality controls for etl process and datawarehousing using msvisualstudiossis, ssas, ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny datavisualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql and r and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in a development environment like git and vm.\n",
      "---\n",
      " analyzed trading mechanism for real-time transactions and build collateral management tools.\n",
      "---\n",
      " compiled data from various sources to perform complex analysis for actionable results.\n",
      "---\n",
      " utilized machine learning algorithms such as linear regression, multivariate regression, naive bayes, random forests, k-means, & knn for data analysis.\n",
      "---\n",
      " measured efficiency of hadoop/hive environment ensuring sla is met.\n",
      "---\n",
      " developed spark code using scala and spark-sql/streaming for faster processing of data.\n",
      "---\n",
      " prepared spark build from the source code and ran the pig scripts using spark rather using mr jobs for better performance.\n",
      "---\n",
      " analyzing the system for new enhancements/functionalities and perform impact analysis of the application for implementing etl changes.\n",
      "---\n",
      " imported data using sqoop to load data from mysql to hdfs on regular basis.\n",
      "---\n",
      " developed scripts and batch job to schedule various hadoop program. used tensorflow to train the model from insightful data and look at thousands of examples.\n",
      "---\n",
      " designing, developing and optimizing sql code (ddl / dml).\n",
      "---\n",
      " building performant, scalable etl processes to load, cleanse and validate data.\n",
      "---\n",
      " expertise in data archival and data migration, ad-hoc reporting and code utilizing sas on unix and windows environments.\n",
      "---\n",
      " tested and debugged sas programs against the test data.\n",
      "---\n",
      " processed the data in sas for the given requirement using sas programming concepts.\n",
      "---\n",
      " imported and exported data files to and from sas using proc import and proc export from excel and various delimited text-based data files such as .txt (tab delimited) and .csv (comma delimited) files into sas datasets for analysis.\n",
      "---\n",
      " expertise in producing rtf, pdf, html files using sas ods facility.\n",
      "---\n",
      " providing support for data processes. this will involve monitoring data, profiling database usage, trouble shooting, tuning and ensuring data integrity.\n",
      "---\n",
      " participating in the full software development lifecycle with requirements, solution design, development, qa implementation, and product support using scrum and other agile methodologies.\n",
      "---\n",
      " collaborate with team members and stakeholders in design and development of data environment.\n",
      "---\n",
      " learning new tools and skillsets as needs arise.\n",
      "---\n",
      " preparing associated documentation for specifications, requirements and testing.\n",
      "---\n",
      " optimizing the tensorflow model for an efficiency.\n",
      "---\n",
      " used tensorflow for text summarization.\n",
      "---\n",
      " used spark api over cloudera hadoop yarn to perform analytics on data in hive.\n",
      "---\n",
      " wrote hive queries for data analysis to meet the business requirements.\n",
      "---\n",
      " developed kafka producer and consumers for message handling.\n",
      "---\n",
      " responsible for analyzing multi-platform applications using python.\n",
      "---\n",
      " used storm for an automatic mechanism to analyze large amounts of non-unique data points with low latency and high throughput.\n",
      "---\n",
      " developed mapreduce jobs in python for data cleaning and data processing.\n",
      "---\n",
      " provided the architectural leadership in shaping strategic, business technology projects, with an emphasis on application architecture.\n",
      "---\n",
      " utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large, business technology programs.\n",
      "---\n",
      " participated in all phases of datamining, datacollection, datacleaning, developingmodels, validation, and visualization and performed gapanalysis.\n",
      "---\n",
      " developed mapreduce/spark, r modules for machine learning & predictive analytics in hadoop on aws. implemented a r-based distributed randomforest.\n",
      "---\n",
      " utilized spark, scala, hadoop, sparkstreaming, mllib, rabroadvariety of machinelearning methods includingclassifications, regressions, dimensionallyreduction etc. and utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " well versed with: cloudiaas and paas implementations in both private and publicclouds like vmware, openstack, amazonaws and cloudfoundry (pivotal and hpstackato).\n",
      "---\n",
      " used r and h2o.ai libraries for developing various machinelearningalgorithms and utilizedmachinelearningalgorithms such as linearregression, multivariateregression, naivebayes, randomforests, k-means, &knn for dataanalysis.\n",
      "---\n",
      " worked on databasedesign, relationalintegrityconstraints, olap, oltp, cubes, andnormalization (3n0f) and de-normalization of the database.\n",
      "---\n",
      " worked on customersegmentation using an unsupervised learning techniqueclustering.\n",
      "---\n",
      " utilized spark, scala, hadoop, hbase, kafka, sparkstreaming, mllib, python, a broad variety of machinelearningmethods including classifications, regressions, dimensionallyreduction etc.\n",
      "---\n",
      " data analysis, reporting using tableauperform numerous data pulling requests using sqlserver2012.\n",
      "---\n",
      " designed and implemented system architecture for amazonec2 based cloud-hosted solution for the client.\n",
      "---\n",
      " tested complex etlmappings and sessions based on business user requirements and business rules to load data from source flat files and rdbms tables to target tables.\n",
      "---\n",
      " hands on experience in hadoopecosystem with componentshadoopmapreduce, hdfs, oozie, hiveql, sqoop, hbase, mongodb, zookeeper, pig, andflume.\n",
      "---\n",
      " the conducted analysis in assessing customer consuming behaviors and discover the value of customers with rmfanalysis, applied customer segmentation with clustering algorithms such as k-meansclustering and hierarchicalclustering.\n",
      "---\n",
      " collaborated with data engineers to implement the etlprocess, wrote and optimized sqlqueries to perform dataextraction and merging from oracle.\n",
      "---\n",
      " used rand spark to develop a variety of models and algorithms for analytic purposes.\n",
      "---\n",
      " performed dataintegritychecks, datacleaning, exploratory analysis and feature engineer using r.\n",
      "---\n",
      " developed personalized product recommendation with machinelearningalgorithms, including gradientboostingtree and collaborative filtering to better meet the needs of existing customers and acquire new customers.\n",
      "---\n",
      " used r and spark to implement different machinelearningalgorithms, including generalizedlinearmodel, randomforest, svm, boosting and neuralnetwork.\n",
      "---\n",
      " evaluated parameters with k-foldcrossvalidation and optimized performance of models.\n",
      "---\n",
      " a highly immersive data science program involving datamanipulation and visualization, webscraping, machinelearning, git, sql, unixcommands, rprogramming, nosql.\n",
      "---\n",
      " identified risk level and eligibility of new insurance applicants with machinelearning algorithms.\n",
      "---\n",
      " utilized sql and hiveql to query, manipulate data from variety data sources including oracle and hdfs, while maintaining data integrity.\n",
      "---\n",
      " performed datavisualization and designeddashboards with tableau and d3.js and provided complexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders.\n",
      "---\n",
      " participated in jad sessions, gathered information from businessanalysts, end users and other stakeholders to determine the requirements.\n",
      "---\n",
      " hands on experience in cloudcomputing such asawsstorage, compute, databasessql\n",
      "---\n",
      " designed the datawarehouse and mdmhubconceptual, logical and physicaldatamodels.\n",
      "---\n",
      " performed dailymonitoring of oracle instances using oracleenterprisemanager, addm, toad, monitorusers, tablespaces, memorystructures, rollbacksegments, logs, andalerts.\n",
      "---\n",
      " used normalization methods up to 3nf and de-normalization techniques for effective performance in oltp and olap systems.\n",
      "---\n",
      " generated ddl scripts using forwardengineering technique to create objects and deploy them into the databases.\n",
      "---\n",
      " worked on database testing, writing complex sqlqueries to verify the transactions and business logic like identifying the duplicate rows by using sqldeveloper and pl/sql developer.\n",
      "---\n",
      " used teradatasqlassistant, teradataadministrator, pmon and data load/export utilities like bteq, fastload, multiload, fastexport, tpump on unix/windows environments and running the batch process for teradata.\n",
      "---\n",
      " developed sqlqueries to fetch complex data from different tables in remote databases using joins, database links and bulkcollects.\n",
      "---\n",
      " the migrated database from legacy systems, sql server to oracle and netezza.\n",
      "---\n",
      " used ssis to create etl packages to validate, extract, transform and load data to pull data from source servers to staging database\n",
      "---\n",
      " worked on sqlserverconceptsssis (sqlserverintegrationservices), ssas (sqlserveranalysisservices) and ssrs (sqlserverreportingservices). python developer aspect software july 2010 to may 2011 environment: er studio, teradata13.1, sql, pl/sql, bteq, db2, oracle, mdm, netezza, etl, rtf unix, sql server2010, informatica, ssrs, ssis, ssas, sas, aginity.\n",
      "---\n",
      " client: aspect software - india july 2010 - may 2011 - july 2012\n",
      "---\n",
      " role: python developer\n",
      "---\n",
      " worked on the project from gathering requirements to developing the entire application. worked on anaconda python environment. created, activated and programmed in anaconda environment. wrote programs for performance calculations using numpy and sqlalchemy.\n",
      "---\n",
      " wrote python routines to log into the websites and fetch data for selected options.\n",
      "---\n",
      " used python modules of urllib, urllib2, requests for web crawling. experience using all these ml techniques: clustering, regression, classification, graphical models.\n",
      "---\n",
      " extensive experience in text analytics, developing different statistical machine learning, data mining solutions to various business problems and generating data visualizations using r, python and tableau.\n",
      "---\n",
      " used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " involved in development of web services using soap for sending and getting data from the external interface in the xml format. used with other packages such as beautiful soup for data parsing.\n",
      "---\n",
      " worked on development of sql and stored procedures on mysql.\n",
      "---\n",
      " analyzed the code completely and have reduced the code redundancy to the optimal level.\n",
      "---\n",
      " design and build a text classification application using different text classification models.\n",
      "---\n",
      " used jira for defect tracking and project management.\n",
      "---\n",
      " worked on writing and as well as read data from csv and excel file formats.\n",
      "---\n",
      " involved in sprint planning sessions and participated in the daily agile scrum meetings.\n",
      "---\n",
      " conducted every day scrum as part of the scrum master role.\n",
      "---\n",
      " developed the project in linux environment.\n",
      "---\n",
      " worked on resulting reports of the application.\n",
      "---\n",
      " performed qa testing on the application.\n",
      "---\n",
      " held meetings with client and worked for the entire project with limited help from the client.\n",
      "---\n",
      " environment: python, anaconda, sypder (ide), windows 7, teradata, requests, urllib, urllib2, beautiful soup, tableau, python libraries such as numpy, sql alchemy, mysqldb. \n",
      "---\n",
      " bachelor's skills apache hadoop hdfs (3 years), oracle (3 years), python. (3 years), sql (4 years), xml (3 years) additional information skills:\n",
      "---\n",
      " big data/hadoop technologies hadoop, hdfs, yarn, mapreduce, hive, pig, impala, sqoop, flume, spark, kafka, zookeeper, and oozie\n",
      "---\n",
      " languages\n",
      "---\n",
      " c, c++, html5, dhtml, wsdl, css3 xml, r/r studio, sas enterprise guide, sas r, r (caret, weka, ggplot), python (numpy, scipy, pandas), sql, pl/sql, pig latin, hiveql, shell scripting.\n",
      "---\n",
      " cloud computing tools amazon aws, azure.\n",
      "---\n",
      " databases microsoft sql server 2008 [ ] mysql 4.x/5.x, oracle 10g, 11g, 12c, db2, teradata, netezza\n",
      "---\n",
      " no sql databases hbase, cassandra, mongodb, mariadb\n",
      "---\n",
      " build tools maven, ant, toad, sql loader, rtc, rsa, control-m, oozie, hue, soap ui\n",
      "---\n",
      " development tools microsoft sql studio, eclipse, netbeans, intellij\n",
      "---\n",
      " development methodologies agile/scrum, waterfall, uml, design patterns\n",
      "---\n",
      " version control tools and testing api git, svm, github, svn and junit\n",
      "---\n",
      " etl tools informatica power centre, ssis\n",
      "---\n",
      " reporting tools ms office (word/excel/powerpoint/ visio/outlook), crystal reports xi, ssrs, cognos7.0/6.0.\n",
      "---\n",
      " operating systems all versions of unix, windows, linux, macintosh hd, sun solaris\n",
      "\n",
      "---\n",
      " ? overseeing/managing (product owner) all implementations of advanced statistical, machine learning and predictive modeling techniques pushed regularly to production.\n",
      "---\n",
      " ? regularly conducted a/b tests and experiments to examine feasibility and gauge performance of successful product features.\n",
      "---\n",
      " ? made strategic recommendations on data collection, integration and retention requirements incorporating business requirements and knowledge of best practices (etl, data warehousing). cse 2111 lab instructor the ohio state university january 2015 to may 2015 instructed laboratory course for excel spreadsheets, data modeling and database management. front-end developer virtual lab school april 2014 to december 2014 \n",
      "---\n",
      " collaboration between the ohio state university and the department of defense creating a platform for early childhood development professionals who work with military and civilian families.\n",
      "---\n",
      " responsible for content creation, user interface and front-end development. dragon research intern - data processing tool developer virtual lab school may 2014 to august 2014 \n",
      "---\n",
      " acoustic modeling research tools team under nuance mobility division.\n",
      "---\n",
      " deployment of tools for data quality measurement.\n",
      "---\n",
      " working with automatic speech recognition systems to optimize quality of training data through statistical investigation.\n",
      "---\n",
      " achievement - introduced novel scoring methodology with higher granularity to replace existing system. python developer grace foundation - in january 2013 to may 2013 \n",
      "---\n",
      " trend monitoring database via keyword searches.\n",
      "---\n",
      " mining data from various news sites for aggregation and computational story building.\n",
      "---\n",
      " deliverable was to generate summarized news stories to be reported on a web portal that catered to a weekly audience of more than 10,000 users. \n",
      "---\n",
      " ms in computer science & engineering the ohio state university - columbus, oh 2013 to 2015 links http://www.github.com/valiantone additional information skills\n",
      "---\n",
      " mastery\n",
      "---\n",
      " python, natural language processing, machine learning, numpy, sci-kit learn, shell-scripting, unix, spreadsheet and data modeling, graphic design\n",
      "---\n",
      " proficiency\n",
      "---\n",
      " c/c++, matlab, nltk, restful api, soap, css, json, sqllite, sql, scipy, django, matplotlib, selenium, gensim, wxpython, rapid-prototyping, invision\n",
      "---\n",
      " competency\n",
      "---\n",
      " java, javascript, lucene, nosql, html\n",
      "---\n",
      " specializations\n",
      "---\n",
      " natural language processing and information extraction for unstructured data\n",
      "---\n",
      " automatic summarization\n",
      "---\n",
      " sentiment analysis and opinion mining\n",
      "---\n",
      " web mining and scraping\n",
      "---\n",
      " predictive data analytics\n",
      "---\n",
      " applied machine learning\n",
      "---\n",
      " graphical and user interaction design\n",
      "---\n",
      " data preparation and visualization\n",
      "\n",
      "---\n",
      " 5 (five) years of experience as data scientist / python / c++ developer and data analyst with technical prowess.\n",
      "---\n",
      " worked on projects which involved deep learning, machine learning algorithms, and data transformation.\n",
      "---\n",
      " handful experience with performing data analysis with compiling, analyzing, validating, modeling data sets and developing machine learning models including neural network models for solving the industry problems.\n",
      "---\n",
      " utilized machine learning models like knn, ann, bpnn, cnn, and regression to filter the noisy data scanned from robotic vision system.\n",
      "---\n",
      " hands on project experience of neural network framework like tensorflow and caffe.\n",
      "---\n",
      " involved in software development, production, support, troubleshooting, maintenance using c/c++.\n",
      "---\n",
      " hands on working knowledge of linux operating system, windows os for machine learning applications to create and analyze data sets.\n",
      "---\n",
      " strong experience in c/c++ on linux using stl, matlab, multithreading, boost libraries, data structures, tcp/ip socket programming.\n",
      "---\n",
      " strong in object-oriented programming (oop), object oriented analysis and design patterns.\n",
      "---\n",
      " experience in various aspects of system architecture, software design, development, implementation and validation.\n",
      "---\n",
      " good knowledge on iot (internet of thing) end-to-end application development.\n",
      "---\n",
      " provided software configuration management, source control and version control using tools including tfs and git.\n",
      "---\n",
      " expert in debugging application using debugging tools provided by the visual studio.\n",
      "---\n",
      " proficient in translating highly technical concepts into business language so that key stakeholders can make informed decisions.\n",
      "---\n",
      " experience using in python, shell, bash scripts, kernel debugging.\n",
      "---\n",
      " worked on uml on creating class, sequence, activity, deployment diagrams.\n",
      "---\n",
      " good experience with both waterfall and agile/scrum development.\n",
      "---\n",
      " enthusiastic and quick to learn new technologies and tools and willing to take responsibilities.\n",
      "---\n",
      " a good team player with strong communication and time management skills makes efforts to do the work at hand. authorized to work in the us for any employer work experience products and application developer ( python/c++/data scientist) servo-robot, inc - fairfield, ct february 2016 to present project: i-fact (03/18 to present)\n",
      "---\n",
      " description: our team are developing a software for welding inspection real time for robotic visual system, which will be used to determine whether the weld is of suitable quality for its intended application.\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " designed a noisy filter algorithm to remove noisy data from outer layer and smooth the edge for 3d point cloud obtained from robotic visual system by using c++.\n",
      "---\n",
      " visualized 3d point cloud data and did simple cloud data operation like subtraction, computer cloud to cloud distance with cloud compare.\n",
      "---\n",
      " converted 3d point cloud data to rgb images for welding classification using c++ \n",
      "---\n",
      " used tensorflow framework to implement convolutional neural network to do welding bead type classification with python.\n",
      "---\n",
      " analyzed and worked with all aspects of regression models to interpolate the invalid data such as missing value or unreasonable values \n",
      "---\n",
      " prepared neural network training data sets with standardized data methods \n",
      "---\n",
      " classified 3d point cloud data of different types of welding bead with pointnet++ using python.\n",
      "---\n",
      " designed artificial neural network to do classification to filtering noisy welding profile.\n",
      "---\n",
      " fine tuning the neural network such as change the activation functions and learning rate to acquiring a higher neural network training accuracy \n",
      "---\n",
      " experienced on matlab machine learning toolbox for different kinds of deep learning methods.\n",
      "---\n",
      " designed and developed data management system using mysql.\n",
      "---\n",
      " environment: python, c++, tensorflow, ann, cnn, pointnet++, regression, mysql, cloud compare, point cloud, matlab, visual studio 2015 servo-robot, inc september 2017 to march 2018 description: designed a software to convert the cad file to servo robot application readable 3dx file for further utilization, for example trajectory generation\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " used draw.io for design of application diagram such as use cases, block diagram, high-level and low-level diagram.\n",
      "---\n",
      " used c++ stl containers, algorithms in the application.\n",
      "---\n",
      " visualized cad file with freecad.\n",
      "---\n",
      " integrated freecad source code to our software and implement the function from freecad to converted cad file to point cloud file.\n",
      "---\n",
      " implemented pcl api to do point cloud computation and opencv for point cloud visualization in visual studio 2015.\n",
      "---\n",
      " conducted code reviews according to c++ coding standards and conventions.\n",
      "---\n",
      " worked on performance improvement &memory leakage.\n",
      "---\n",
      " used git for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " developed the test cases according to the requirements.\n",
      "---\n",
      " verification and validating the application software\n",
      "---\n",
      " environment: c++, freecad, cad, stl, point cloud, pcl, git, draw.io, visual studio 2015\n",
      "---\n",
      " project: jetson tx1 (04/17-09/17)\n",
      "---\n",
      " description: jetson tx1 is an embedded system development kit, is for ai computing designed to get you up and running fast with cuda cores. it's in linux system and includes the latest technology for deep learning, computer vision, gpu computing, and graphics. this is an individual project, aim to integrate new servo-robot controller for high end applications such as high-speed tracking and inspection system to jetson tx1.\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " set up an environment for embedded applications using jetson tx1 kit and ubuntu server.\n",
      "---\n",
      " experienced on cuda programing for performance optimization \n",
      "---\n",
      " connected servo-robot camera with high speed communication with controller using giga ethernet protocol.\n",
      "---\n",
      " gathered images using 2d giga camera.\n",
      "---\n",
      " used qt for gui development.\n",
      "---\n",
      " implemented opencv functions on 2d image.\n",
      "---\n",
      " 3d visualization with opengl.\n",
      "---\n",
      " cmake build caffe on ubuntu system, installed cuda and cudnn to speed caffe model.\n",
      "---\n",
      " developed convolutional neural network with caffe on jetson for welding bead localization and used gpu to increase the neural network training speed.\n",
      "---\n",
      " utilized graphics debugger tool and system profiler tool on jetson tx1.\n",
      "---\n",
      " developed python scripts for diagnostic purposes.\n",
      "---\n",
      " environment: linux, giga ethernet protocol, jetson tx1, opencv, opengl, qt, caffe, cnn, graphic debugger tool, system profiler tool, cuda\n",
      "---\n",
      " project: welding bead localization (03/17-02/16)\n",
      "---\n",
      " description: research and design an algorithm to locate the welding bead. this algorithm was implemented on robotic vision system for welding seam finding.\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " tested different types of deep learning methods with matlab machine learning toolbox.\n",
      "---\n",
      " developed image processing algorithms to perform image processing on welding images to locate the welding bead with java using itellij.\n",
      "---\n",
      " implemented template -matching algorithm from pcl to detect the welding area for 3d welding profile map by using c++.\n",
      "---\n",
      " converted 3d welding profile map to rgb images for neural network training.\n",
      "---\n",
      " converted data format to hdf5 for convolutional neural network training.\n",
      "---\n",
      " designed and created convolutional neural network using caffe framework in c++ for welding bead localization.\n",
      "---\n",
      " fine tuning neural network with hyper parameters to increase the neural network training accuracy \n",
      "---\n",
      " wrote documentation for neural network fine tuning procedure.\n",
      "---\n",
      " experienced on different kinds of activation functions for neural network.\n",
      "---\n",
      " environment: matlab, caffe, template matching, java, image processing, 3dwelding profile map full time internship china hubei electrical power design institute november 2014 to april 2015 software engineer (c++)\n",
      "---\n",
      " description: our team define, design and deliver a bom information system, which is desktop application to eliminate the manual files of data such as planning requisitions, planning bom, and engineering bom. data from source systems will be pushed into the data tool, then end user can efficiently import, add, manipulate import data, and export data, which accelerates the process of material ordering and planning.\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " participated in the requirement gathering process and determined technical requirements of project to ensure that specifications are met.\n",
      "---\n",
      " supported project planning process with design options, work estimates and requirement analysis.\n",
      "---\n",
      " involved in systems architecture design and applied design patterns, oop, and stl.\n",
      "---\n",
      " used c/c++ interface to retrieve/update info from/to the database.\n",
      "---\n",
      " implemented functionality such as filtering, sorting, searching, and alerting using c++.\n",
      "---\n",
      " displayed progress through a sequence of logical and numbered steps and display a transient feedback message after a step is saved.\n",
      "---\n",
      " provided input to microsoft sql server database architecture design and data migration from internal data sources.\n",
      "---\n",
      " fixed software issues and documented software development cycle phases.\n",
      "---\n",
      " tested and developed c++ applications for windows platform.\n",
      "---\n",
      " environment: c++, oop, stl, sql server 2008, visual studio 2008 software engineer (c++) shenzhen ruanku information technology may 2014 to october 2014 description: the project was to develop a license plate recognition system to detect and recognize the license plate from video stream real time.\n",
      "---\n",
      " responsibilities \n",
      "---\n",
      " analyzed video and extract frames with license plate using opencv.\n",
      "---\n",
      " applied image processing algorithms like grayscale, sobel filter, dilation, and erosion from opencv to get the license plate using c++.\n",
      "---\n",
      " experienced on image segmentation.\n",
      "---\n",
      " utilized svm to do characters recognition to recognize all numbers from license plate.\n",
      "---\n",
      " designed simple ui using microsoft foundation classes (mfc).\n",
      "---\n",
      " developed the test cases according to the requirements and tested the system.\n",
      "---\n",
      " used c++ stl containers, algorithms in the system.\n",
      "---\n",
      " used microsoft team foundation server (tfs) for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " fixed software bugs and documented software development cycle phases.\n",
      "---\n",
      " environment: opencv, mfc, c++, visual studio 2005, svm, image processing, stl, tfs, \n",
      "---\n",
      " master's skills .net (4 years), c+ (4 years), matlab (3 years), python (3 years), visual studio (4 years)\n",
      "\n",
      "---\n",
      " 5 (five) years of experience as data scientist / python / c++ developer and data analyst with technical prowess.\n",
      "---\n",
      " worked on projects which involved deep learning, machine learning algorithms, and data transformation.\n",
      "---\n",
      " handful experience with performing data analysis with compiling, analyzing, validating, modeling data sets and developing machine learning models including neural network models for solving the industry problems.\n",
      "---\n",
      " utilized machine learning models like knn, ann, bpnn, cnn, and regression to filter the noisy data scanned from robotic vision system.\n",
      "---\n",
      " hands on project experience of neural network framework like tensorflow and caffe.\n",
      "---\n",
      " involved in software development, production, support, troubleshooting, maintenance using c/c++.\n",
      "---\n",
      " hands on working knowledge of linux operating system, windows os for machine learning applications to create and analyze data sets.\n",
      "---\n",
      " strong experience in c/c++ on linux using stl, matlab, multithreading, boost libraries, data structures, tcp/ip socket programming.\n",
      "---\n",
      " strong in object-oriented programming (oop), object oriented analysis and design patterns.\n",
      "---\n",
      " experience in various aspects of system architecture, software design, development, implementation and validation.\n",
      "---\n",
      " good knowledge on iot (internet of thing) end-to-end application development.\n",
      "---\n",
      " provided software configuration management, source control and version control using tools including tfs and git.\n",
      "---\n",
      " expert in debugging application using debugging tools provided by the visual studio.\n",
      "---\n",
      " proficient in translating highly technical concepts into business language so that key stakeholders can make informed decisions.\n",
      "---\n",
      " experience using in python, shell, bash scripts, kernel debugging.\n",
      "---\n",
      " worked on uml on creating class, sequence, activity, deployment diagrams.\n",
      "---\n",
      " good experience with both waterfall and agile/scrum development.\n",
      "---\n",
      " enthusiastic and quick to learn new technologies and tools and willing to take responsibilities.\n",
      "---\n",
      " a good team player with strong communication and time management skills makes efforts to do the work at hand. authorized to work in the us for any employer work experience products and application developer ( python/c++/data scientist) servo-robot, inc - fairfield, ct february 2016 to present project: i-fact (03/18 to present)\n",
      "---\n",
      " description: our team are developing a software for welding inspection real time for robotic visual system, which will be used to determine whether the weld is of suitable quality for its intended application.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " designed a noisy filter algorithm to remove noisy data from outer layer and smooth the edge for 3d point cloud obtained from robotic visual system by using c++.\n",
      "---\n",
      " visualized 3d point cloud data and did simple cloud data operation like subtraction, computer cloud to cloud distance with cloud compare.\n",
      "---\n",
      " converted 3d point cloud data to rgb images for welding classification using c++\n",
      "---\n",
      " used tensorflow framework to implement convolutional neural network to do welding bead type classification with python.\n",
      "---\n",
      " analyzed and worked with all aspects of regression models to interpolate the invalid data such as missing value or unreasonable values\n",
      "---\n",
      " prepared neural network training data sets with standardized data methods\n",
      "---\n",
      " classified 3d point cloud data of different types of welding bead with pointnet++ using python.\n",
      "---\n",
      " designed artificial neural network to do classification to filtering noisy welding profile.\n",
      "---\n",
      " fine tuning the neural network such as change the activation functions and learning rate to acquiring a higher neural network training accuracy\n",
      "---\n",
      " experienced on matlab machine learning toolbox for different kinds of deep learning methods.\n",
      "---\n",
      " designed and developed data management system using mysql.\n",
      "---\n",
      " environment: python, c++, tensorflow, ann, cnn, pointnet++, regression, mysql, cloud compare, point cloud, matlab, visual studio 2015 servo-robot, inc september 2017 to march 2018 description: designed a software to convert the cad file to servo robot application readable 3dx file for further utilization, for example trajectory generation\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " used draw.io for design of application diagram such as use cases, block diagram, high-level and low-level diagram.\n",
      "---\n",
      " used c++ stl containers, algorithms in the application.\n",
      "---\n",
      " visualized cad file with freecad.\n",
      "---\n",
      " integrated freecad source code to our software and implement the function from freecad to converted cad file to point cloud file.\n",
      "---\n",
      " implemented pcl api to do point cloud computation and opencv for point cloud visualization in visual studio 2015.\n",
      "---\n",
      " conducted code reviews according to c++ coding standards and conventions.\n",
      "---\n",
      " worked on performance improvement &memory leakage.\n",
      "---\n",
      " used git for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " developed the test cases according to the requirements.\n",
      "---\n",
      " verification and validating the application software\n",
      "---\n",
      " environment: c++, freecad, cad, stl, point cloud, pcl, git, draw.io, visual studio 2015\n",
      "---\n",
      " project: jetson tx1 (04/17-09/17)\n",
      "---\n",
      " description: jetson tx1 is an embedded system development kit, is for ai computing designed to get you up and running fast with cuda cores. it's in linux system and includes the latest technology for deep learning, computer vision, gpu computing, and graphics. this is an individual project, aim to integrate new servo-robot controller for high end applications such as high-speed tracking and inspection system to jetson tx1.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " set up an environment for embedded applications using jetson tx1 kit and ubuntu server.\n",
      "---\n",
      " experienced on cuda programing for performance optimization\n",
      "---\n",
      " connected servo-robot camera with high speed communication with controller using giga ethernet protocol.\n",
      "---\n",
      " gathered images using 2d giga camera.\n",
      "---\n",
      " used qt for gui development.\n",
      "---\n",
      " implemented opencv functions on 2d image.\n",
      "---\n",
      " 3d visualization with opengl.\n",
      "---\n",
      " cmake build caffe on ubuntu system, installed cuda and cudnn to speed caffe model.\n",
      "---\n",
      " developed convolutional neural network with caffe on jetson for welding bead localization and used gpu to increase the neural network training speed.\n",
      "---\n",
      " utilized graphics debugger tool and system profiler tool on jetson tx1.\n",
      "---\n",
      " developed python scripts for diagnostic purposes.\n",
      "---\n",
      " environment: linux, giga ethernet protocol, jetson tx1, opencv, opengl, qt, caffe, cnn, graphic debugger tool, system profiler tool, cuda\n",
      "---\n",
      " project: welding bead localization (03/17-02/16)\n",
      "---\n",
      " description: research and design an algorithm to locate the welding bead. this algorithm was implemented on robotic vision system for welding seam finding.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " tested different types of deep learning methods with matlab machine learning toolbox.\n",
      "---\n",
      " developed image processing algorithms to perform image processing on welding images to locate the welding bead with java using itellij.\n",
      "---\n",
      " implemented template -matching algorithm from pcl to detect the welding area for 3d welding profile map by using c++.\n",
      "---\n",
      " converted 3d welding profile map to rgb images for neural network training.\n",
      "---\n",
      " converted data format to hdf5 for convolutional neural network training.\n",
      "---\n",
      " designed and created convolutional neural network using caffe framework in c++ for welding bead localization.\n",
      "---\n",
      " fine tuning neural network with hyper parameters to increase the neural network training accuracy\n",
      "---\n",
      " wrote documentation for neural network fine tuning procedure.\n",
      "---\n",
      " experienced on different kinds of activation functions for neural network.\n",
      "---\n",
      " environment: matlab, caffe, template matching, java, image processing, 3dwelding profile map full time internship china hubei electrical power design institute november 2014 to april 2015 software engineer (c++)\n",
      "---\n",
      " description: our team define, design and deliver a bom information system, which is desktop application to eliminate the manual files of data such as planning requisitions, planning bom, and engineering bom. data from source systems will be pushed into the data tool, then end user can efficiently import, add, manipulate import data, and export data, which accelerates the process of material ordering and planning.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " participated in the requirement gathering process and determined technical requirements of project to ensure that specifications are met.\n",
      "---\n",
      " supported project planning process with design options, work estimates and requirement analysis.\n",
      "---\n",
      " involved in systems architecture design and applied design patterns, oop, and stl.\n",
      "---\n",
      " used c/c++ interface to retrieve/update info from/to the database.\n",
      "---\n",
      " implemented functionality such as filtering, sorting, searching, and alerting using c++.\n",
      "---\n",
      " displayed progress through a sequence of logical and numbered steps and display a transient feedback message after a step is saved.\n",
      "---\n",
      " provided input to microsoft sql server database architecture design and data migration from internal data sources.\n",
      "---\n",
      " fixed software issues and documented software development cycle phases.\n",
      "---\n",
      " tested and developed c++ applications for windows platform.\n",
      "---\n",
      " environment: c++, oop, stl, sql server 2008, visual studio 2008 software engineer (c++) shenzhen ruanku information technology may 2014 to october 2014 description: the project was to develop a license plate recognition system to detect and recognize the license plate from video stream real time.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " analyzed video and extract frames with license plate using opencv.\n",
      "---\n",
      " applied image processing algorithms like grayscale, sobel filter, dilation, and erosion from opencv to get the license plate using c++.\n",
      "---\n",
      " experienced on image segmentation.\n",
      "---\n",
      " utilized svm to do characters recognition to recognize all numbers from license plate.\n",
      "---\n",
      " designed simple ui using microsoft foundation classes (mfc).\n",
      "---\n",
      " developed the test cases according to the requirements and tested the system.\n",
      "---\n",
      " used c++ stl containers, algorithms in the system.\n",
      "---\n",
      " used microsoft team foundation server (tfs) for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " fixed software bugs and documented software development cycle phases.\n",
      "---\n",
      " environment: opencv, mfc, c++, visual studio 2005, svm, image processing, stl, tfs, \n",
      "---\n",
      " master's skills .net (4 years), auto cad (less than 1 year), c+ (4 years), cad (less than 1 year), database. (less than 1 year), ethernet (less than 1 year), git (less than 1 year), image processing (1 year), java (less than 1 year), linux (less than 1 year), mac (less than 1 year), macos (less than 1 year), matlab (3 years), mysql. (3 years), open gl (less than 1 year), opengl. (less than 1 year), pcl (less than 1 year), perforce (less than 1 year), python (3 years), visual studio (4 years) additional information technical skills\n",
      "---\n",
      " language: c, c++, python, java, matlab\n",
      "---\n",
      " database: ms sql server, mysql\n",
      "---\n",
      " operating system: linux, windows7/8/10, macos\n",
      "---\n",
      " others: visual studio, perforce, hipchat, sourcetree, git, tensorflow, ann, cnn, pointnet++, point cloud,\n",
      "---\n",
      " freecad, cad, st, pcl, draw.io, giga ethernet protocol, jetson tx1, opencv, opengl, qt, caffe, graphic debugger tool, system profiler tool, cuda, 3dwelding profile map, template matching, image processing\n",
      "\n",
      "---\n",
      " 5 (five) years of experience as data scientist / python / c++ developer and data analyst with technical prowess.\n",
      "---\n",
      " worked on projects which involved deep learning, machine learning algorithms, and data transformation.\n",
      "---\n",
      " handful experience with performing data analysis with compiling, analyzing, validating, modeling data sets and developing machine learning models including neural network models for solving the industry problems.\n",
      "---\n",
      " utilized machine learning models like knn, ann, bpnn, cnn, and regression to filter the noisy data scanned from robotic vision system.\n",
      "---\n",
      " hands on project experience of neural network framework like tensorflow and caffe.\n",
      "---\n",
      " involved in software development, production, support, troubleshooting, maintenance using c/c++.\n",
      "---\n",
      " hands on working knowledge of linux operating system, windows os for machine learning applications to create and analyze data sets.\n",
      "---\n",
      " strong experience in c/c++ on linux using stl, matlab, multithreading, boost libraries, data structures, tcp/ip socket programming.\n",
      "---\n",
      " strong in object-oriented programming (oop), object oriented analysis and design patterns.\n",
      "---\n",
      " experience in various aspects of system architecture, software design, development, implementation and validation.\n",
      "---\n",
      " good knowledge on iot (internet of thing) end-to-end application development.\n",
      "---\n",
      " provided software configuration management, source control and version control using tools including tfs and git.\n",
      "---\n",
      " expert in debugging application using debugging tools provided by the visual studio.\n",
      "---\n",
      " proficient in translating highly technical concepts into business language so that key stakeholders can make informed decisions.\n",
      "---\n",
      " experience using in python, shell, bash scripts, kernel debugging.\n",
      "---\n",
      " worked on uml on creating class, sequence, activity, deployment diagrams.\n",
      "---\n",
      " good experience with both waterfall and agile/scrum development.\n",
      "---\n",
      " enthusiastic and quick to learn new technologies and tools and willing to take responsibilities.\n",
      "---\n",
      " a good team player with strong communication and time management skills makes efforts to do the work at hand. sponsorship required to work in the us work experience products and application developer ( python/c++/data scientist) servo-robot, inc - fairfield, ct february 2016 to present project: i-fact (03/18 to present)\n",
      "---\n",
      " description: our team are developing a software for welding inspection real time for robotic visual system, which will be used to determine whether the weld is of suitable quality for its intended application.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " designed a noisy filter algorithm to remove noisy data from outer layer and smooth the edge for 3d point cloud obtained from robotic visual system by using c++.\n",
      "---\n",
      " visualized 3d point cloud data and did simple cloud data operation like subtraction, computer cloud to cloud distance with cloud compare.\n",
      "---\n",
      " converted 3d point cloud data to rgb images for welding classification using c++\n",
      "---\n",
      " used tensorflow framework to implement convolutional neural network to do welding bead type classification with python.\n",
      "---\n",
      " analyzed and worked with all aspects of regression models to interpolate the invalid data such as missing value or unreasonable values\n",
      "---\n",
      " prepared neural network training data sets with standardized data methods\n",
      "---\n",
      " classified 3d point cloud data of different types of welding bead with pointnet++ using python.\n",
      "---\n",
      " designed artificial neural network to do classification to filtering noisy welding profile.\n",
      "---\n",
      " fine tuning the neural network such as change the activation functions and learning rate to acquiring a higher neural network training accuracy\n",
      "---\n",
      " experienced on matlab machine learning toolbox for different kinds of deep learning methods.\n",
      "---\n",
      " designed and developed data management system using mysql.\n",
      "---\n",
      " environment: python, c++, tensorflow, ann, cnn, pointnet++, regression, mysql, cloud compare, point cloud, matlab, visual studio 2015 servo-robot, inc september 2017 to march 2018 description: designed a software to convert the cad file to servo robot application readable 3dx file for further utilization, for example trajectory generation\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " used draw.io for design of application diagram such as use cases, block diagram, high-level and low-level diagram.\n",
      "---\n",
      " used c++ stl containers, algorithms in the application.\n",
      "---\n",
      " visualized cad file with freecad.\n",
      "---\n",
      " integrated freecad source code to our software and implement the function from freecad to converted cad file to point cloud file.\n",
      "---\n",
      " implemented pcl api to do point cloud computation and opencv for point cloud visualization in visual studio 2015.\n",
      "---\n",
      " conducted code reviews according to c++ coding standards and conventions.\n",
      "---\n",
      " worked on performance improvement &memory leakage.\n",
      "---\n",
      " used git for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " developed the test cases according to the requirements.\n",
      "---\n",
      " verification and validating the application software\n",
      "---\n",
      " environment: c++, freecad, cad, stl, point cloud, pcl, git, draw.io, visual studio 2015\n",
      "---\n",
      " project: jetson tx1 (04/17-09/17)\n",
      "---\n",
      " description: jetson tx1 is an embedded system development kit, is for ai computing designed to get you up and running fast with cuda cores. it's in linux system and includes the latest technology for deep learning, computer vision, gpu computing, and graphics. this is an individual project, aim to integrate new servo-robot controller for high end applications such as high-speed tracking and inspection system to jetson tx1.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " set up an environment for embedded applications using jetson tx1 kit and ubuntu server.\n",
      "---\n",
      " experienced on cuda programing for performance optimization\n",
      "---\n",
      " connected servo-robot camera with high speed communication with controller using giga ethernet protocol.\n",
      "---\n",
      " gathered images using 2d giga camera.\n",
      "---\n",
      " used qt for gui development.\n",
      "---\n",
      " implemented opencv functions on 2d image.\n",
      "---\n",
      " 3d visualization with opengl.\n",
      "---\n",
      " cmake build caffe on ubuntu system, installed cuda and cudnn to speed caffe model.\n",
      "---\n",
      " developed convolutional neural network with caffe on jetson for welding bead localization and used gpu to increase the neural network training speed.\n",
      "---\n",
      " utilized graphics debugger tool and system profiler tool on jetson tx1.\n",
      "---\n",
      " developed python scripts for diagnostic purposes.\n",
      "---\n",
      " environment: linux, giga ethernet protocol, jetson tx1, opencv, opengl, qt, caffe, cnn, graphic debugger tool, system profiler tool, cuda\n",
      "---\n",
      " project: welding bead localization (03/17-02/16)\n",
      "---\n",
      " description: research and design an algorithm to locate the welding bead. this algorithm was implemented on robotic vision system for welding seam finding.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " tested different types of deep learning methods with matlab machine learning toolbox.\n",
      "---\n",
      " developed image processing algorithms to perform image processing on welding images to locate the welding bead with java using itellij.\n",
      "---\n",
      " implemented template -matching algorithm from pcl to detect the welding area for 3d welding profile map by using c++.\n",
      "---\n",
      " converted 3d welding profile map to rgb images for neural network training.\n",
      "---\n",
      " converted data format to hdf5 for convolutional neural network training.\n",
      "---\n",
      " designed and created convolutional neural network using caffe framework in c++ for welding bead localization.\n",
      "---\n",
      " fine tuning neural network with hyper parameters to increase the neural network training accuracy\n",
      "---\n",
      " wrote documentation for neural network fine tuning procedure.\n",
      "---\n",
      " experienced on different kinds of activation functions for neural network.\n",
      "---\n",
      " environment: matlab, caffe, template matching, java, image processing, 3dwelding profile map full time internship china hubei electrical power design institute november 2014 to april 2015 software engineer (c++)\n",
      "---\n",
      " description: our team define, design and deliver a bom information system, which is desktop application to eliminate the manual files of data such as planning requisitions, planning bom, and engineering bom. data from source systems will be pushed into the data tool, then end user can efficiently import, add, manipulate import data, and export data, which accelerates the process of material ordering and planning.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " participated in the requirement gathering process and determined technical requirements of project to ensure that specifications are met.\n",
      "---\n",
      " supported project planning process with design options, work estimates and requirement analysis.\n",
      "---\n",
      " involved in systems architecture design and applied design patterns, oop, and stl.\n",
      "---\n",
      " used c/c++ interface to retrieve/update info from/to the database.\n",
      "---\n",
      " implemented functionality such as filtering, sorting, searching, and alerting using c++.\n",
      "---\n",
      " displayed progress through a sequence of logical and numbered steps and display a transient feedback message after a step is saved.\n",
      "---\n",
      " provided input to microsoft sql server database architecture design and data migration from internal data sources.\n",
      "---\n",
      " fixed software issues and documented software development cycle phases.\n",
      "---\n",
      " tested and developed c++ applications for windows platform.\n",
      "---\n",
      " environment: c++, oop, stl, sql server 2008, visual studio 2008 software engineer (c++) shenzhen ruanku information technology may 2014 to october 2014 description: the project was to develop a license plate recognition system to detect and recognize the license plate from video stream real time.\n",
      "---\n",
      " responsibilities\n",
      "---\n",
      " analyzed video and extract frames with license plate using opencv.\n",
      "---\n",
      " applied image processing algorithms like grayscale, sobel filter, dilation, and erosion from opencv to get the license plate using c++.\n",
      "---\n",
      " experienced on image segmentation.\n",
      "---\n",
      " utilized svm to do characters recognition to recognize all numbers from license plate.\n",
      "---\n",
      " designed simple ui using microsoft foundation classes (mfc).\n",
      "---\n",
      " developed the test cases according to the requirements and tested the system.\n",
      "---\n",
      " used c++ stl containers, algorithms in the system.\n",
      "---\n",
      " used microsoft team foundation server (tfs) for source code control, followed agile and scrum methodologies.\n",
      "---\n",
      " fixed software bugs and documented software development cycle phases.\n",
      "---\n",
      " environment: opencv, mfc, c++, visual studio 2005, svm, image processing, stl, tfs, \n",
      "---\n",
      " master's skills .net (4 years), auto cad (less than 1 year), c+ (4 years), cad (less than 1 year), database. (less than 1 year), ethernet (less than 1 year), git (less than 1 year), image processing (1 year), java (less than 1 year), linux (less than 1 year), mac (less than 1 year), macos (less than 1 year), matlab (3 years), mysql. (3 years), open gl (less than 1 year), opengl. (less than 1 year), pcl (less than 1 year), perforce (less than 1 year), python (3 years), visual studio (4 years) additional information technical skills\n",
      "---\n",
      " language: c, c++, python, java, matlab\n",
      "---\n",
      " database: ms sql server, mysql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " operating system: linux, windows7/8/10, macos\n",
      "---\n",
      " others: visual studio, perforce, hipchat, sourcetree, git, tensorflow, ann, cnn, pointnet++, point cloud,\n",
      "---\n",
      " freecad, cad, st, pcl, draw.io, giga ethernet protocol, jetson tx1, opencv, opengl, qt, caffe, graphic debugger tool, system profiler tool, cuda, 3dwelding profile map, template matching, image processing\n",
      "\n",
      "---\n",
      " helped the policy analysts to design, develop policy data products for multiple audiences including state and district leaders.\n",
      "---\n",
      "web scraped fda, epa and usda public comments (flat files, pdf\\x92s) using python.\n",
      "---\n",
      " applied nlp to perform sentiment analysis on the comments to inform agency regulatory reform efforts.\n",
      "---\n",
      " accomplishments \\x96 automated the manual policy review methodologies. data analyst tragedy assistance program for survivors - virginia september 2018 to december 2018 \n",
      "---\n",
      " created dashboards by performing impact analysis on survey data using tableau.\n",
      "---\n",
      " building an architecture to store to store all their data.\n",
      "---\n",
      " accomplishment: enabled taps to perform data driven decisions by using the analysis that i performed on the survey data. data scientist intern akira technologies - washington, dc september 2018 to december 2018 \n",
      "---\n",
      " developed iam poc by interacting with sql graph database for analyzing employee to employee relationship.\n",
      "---\n",
      " implemented machine learning algorithms to predict employee\\x92s performance.\n",
      "---\n",
      " accomplishments - generated a network analysis graph to represent hierarchy of the organization. senior research assistant ( python developer) computational bioinformatics lab - washington, dc march 2018 to august 2018 \n",
      "---\n",
      " built the genomic datasets and data pipeline by web scraping and data wrangling the nih website. \n",
      "---\n",
      " led a consulting project for biopharma client and implemented various bio tools on their aws infra.\n",
      "---\n",
      " developed data models/metadata for the lab.\n",
      "---\n",
      " accomplishments - key player in the successful launch of the lab\\x92s data pipeline portal.\n",
      "---\n",
      " data czar (data analyst) office of innovation and entrepreneurship - washington, dc february 2018 to march 2018 \n",
      "---\n",
      " built the genomic datasets and data pipeline by web scraping and data wrangling the nih website. \n",
      "---\n",
      " led a consulting project for biopharma client and implemented various bio tools on their aws infra.\n",
      "---\n",
      " developed data models/metadata for the lab.\n",
      "---\n",
      " accomplishments - key player in the successful launch of the lab\\x92s data pipeline portal.\n",
      "---\n",
      " software engineer harbinger systems - in december 2013 to june 2017 \n",
      "---\n",
      " key contributor in developing various modules for healthcare and e learning projects using java, sql.\n",
      "---\n",
      " automated the manual deployment process by independently developing a tool in python.\n",
      "---\n",
      " mentored and coached a batch of college graduates and helped them with their pilot project.\n",
      "---\n",
      " accomplishments - the automation reduced the deployment time by 60% and got an achiever\\x92s award from the leadership team.\n",
      "---\n",
      " master's in data science the george washington university - washington, dc august 2017 to may 2019 master's in computer application symbiosis international university - maharashtra, india june 2012 to may 2014 bachelor's in computer science in computer science university of pune - maharashtra, india june 2009 to may 2012 skills javascript (3 years), python (3 years), sql (3 years), java (3 years), jquery (3 years), excel, data entry, data analysis, tableau, etl, r programming (2 years), sas (1 year), html, microsoft office, powerpoint links https://github.com/monicams89 additional information technical skills\n",
      "---\n",
      " languages/tools: python, r, java, sql, tableau, sas, etl, excel, javascript, jquery, html, css, unix, c, c++, php, github\n",
      "---\n",
      " big data: apache spark, pig, hive, map reduce, aws (ec2)\n",
      "---\n",
      " packages: scikit-learn, numpy, pandas, plotly, ggplot2, twitterr, psych, sapply, bio python\n",
      "---\n",
      " machine learning algorithms: linear and multiple regression, logistic regression, decision tree, random forest, svm, knn, k means.\n",
      "\n",
      "---\n",
      " working on contract for cisco cx innovation group to improve the customer experience using the power of data. \n",
      "---\n",
      " designed, implemented and fine-tuned deep bidirectional transformer networks (bert) and xlnet\n",
      "---\n",
      "for the task of classifying sentiments of customer feedback's and net promoter score data.\n",
      "---\n",
      " successfully improved the performance by 3 % on the sentiment classification task by using data augmentation.\n",
      "---\n",
      " experimented with different nlp techniques (word embeddings, lstm seq2seq paraphrase models ) to increase the training dataset in order to improve the\n",
      "---\n",
      "performance of the sentiment classification task on the test data.\n",
      "---\n",
      " helped in designing the guidelines for data labeling for internal cisco data.\n",
      "---\n",
      " set up evaluation environments for speech-to-text libraries (ibm, google, kaldi) for evaluating their performance on customer feedback recordings.\n",
      "---\n",
      " researching on question answering dnn frameworks (seq2seq, transformer attention-based models). python developer trainee integra technologies llc june 2018 to december 2018 \n",
      "---\n",
      " worked as a volunteer to enhance and maintain a complex python framework.\n",
      "---\n",
      " crafted web solutions to resolve technical problems to satisfy customer business needs.\n",
      "---\n",
      " assisted in architecture design and deployment tasks in back-end web development. research assistant rochester institute of technology - rochester, ny january 2017 to december 2017 \n",
      "---\n",
      " assisted in designing and implementing data-driven models at the computational biomedicine lab at rit to solve health domain problems.\n",
      "---\n",
      " suggested and demonstrated state of the art deep learning ideas to solve the problem of classifying unstructured electrocardiogram signals.\n",
      "---\n",
      " designed and improved the performance of a hybrid deep learning architecture comprising of convolutional neural networks and long short-term memory model by 10%. the improved model showed better performance than previous traditional machine learning models.\n",
      "---\n",
      " worked on feature engineering and addressing issues related to highly unbalanced (non-uniformly distributed) datasets.\n",
      "---\n",
      " performed tuning of deep learning models on aws ec2 cloud instances. software developer intern hcl infosystems ltd may 2013 to july 2013 \n",
      "---\n",
      " built an online management system on asp.net framework for arcedu campus.\n",
      "---\n",
      " assisted in integrating the web application with mysql database.\n",
      "---\n",
      " worked in a team of 20 to design and manage web based forms. \n",
      "---\n",
      " masters in computer science rochester institute of technology december 2017 bachelors in computer science and engineering sir padampat singhania university may 2014 skills python (scipy, numpy, keras, pandas, scikit-learn, matplotlib) (3 years), matlab (2 years), java (2 years), apache spark (less than 1 year), keras (1 year), tensorflow (1 year), r (1 year), aws ec2 (less than 1 year), python, hadoop (less than 1 year), aws, mysql, git links https://github.com/niharvanjararit certifications/licenses introduction to big data with apache spark july 2015 to present deep learning specialization july 2018 to present https://www.coursera.org/account/accomplishments/specialization/certificate/byqny5ned2vg. additional information projects\n",
      "---\n",
      " sentiment classification on text \n",
      "---\n",
      " (python jupyter notebook)\n",
      "---\n",
      " feb 2018 - march 2018\n",
      "---\n",
      " implemented a model which takes a sentence as an input and classifies a sentence as an emotion belonging to one of the 5 classes.\n",
      "---\n",
      " used pretrained glove 50-dimensional vector word embeddings to train an lstm. the model classifies emotions with high accuracy (85%).\n",
      "---\n",
      " image detection and classification of mathematical expressions (crohme dataset) \n",
      "---\n",
      " python \n",
      "---\n",
      " jan2017 \\x96 may 2017 \n",
      "---\n",
      " developed a machine learning pipeline for labeling around 100,000 online handwritten mathematical expression images.\n",
      "---\n",
      " performed segmentation, classification and parsing of images in order to label every expression in the image accurately. \n",
      "---\n",
      " yavis assistant \n",
      "---\n",
      " ibm watson bluemix\n",
      "---\n",
      " python \n",
      "---\n",
      " jan 2017 \\x96 may 2017\n",
      "---\n",
      " created an nlp based ai assistant python application using ibm watson and google calendar rest api\\x92s. \n",
      "---\n",
      " the application can add and retrieve calendar entries through natural language (text).\n",
      "---\n",
      " demand prediction for bakery products (python)\n",
      "---\n",
      " classification of color fundus diabetic retinopathy images matlab\n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " designed image processing techniques and ml techniques (pca) to extract features from retinal images.\n",
      "---\n",
      " the processed features were then trained on an svm model to accurately detect different stages of diabetic retinopathy.\n",
      "---\n",
      " analysis of intrusion detection system data \n",
      "---\n",
      " matlab, python, java\n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " developed data-driven models (svm, random forest, na\\xefve bayes and neural network) for creating anomaly and misuse based intrusion detection system based on kdd cup data. \n",
      "---\n",
      " the predictive models can distinguish between bad (intrusions) and good connections.\n",
      "---\n",
      " demand prediction of bakery goods \n",
      "---\n",
      " python \n",
      "---\n",
      " aug 2016 \\x96 dec 2016\n",
      "---\n",
      " designed predictive models (linear regression) on sales data for maximizing sales and minimizing returns for bakery goods. \n",
      "---\n",
      " data integration was performed to join multiple tables.\n",
      "---\n",
      " classification of diabetes data \n",
      "---\n",
      " weka \n",
      "---\n",
      " january 2016 \\x96 may 2016\n",
      "---\n",
      " performed data cleaning and preparation on the uci diabetes dataset having 100,000 instances and 55 attributes. \n",
      "---\n",
      " performed feature extraction and dimensionality reduction and tested various classification models.\n",
      "\n",
      "---\n",
      " design sql, r & python scripts to directly query and analyze internal and external data sources to package together clear, actionable findings\n",
      "---\n",
      " aid in the development of predictive modeling and machine learning solutions that continually improve with the collection of real-time dealer and sales results from product portfolio, using popular r/python libraries, including tensorflow, h2o, scikit-learn, keras, glm, and nnet\n",
      "---\n",
      " test model fit using cross-validation and bootstrapping methods, hypothesis testing, and measuring error and confidence\n",
      "---\n",
      " develop in an scrum environment with team members, actively using sprints to deliver solutions quickly and continuously exploring ways to improve our results\n",
      "---\n",
      " model data using pandas/r dataframes, powerpivot, tera data, and power bi. \n",
      "---\n",
      " work closely with colleagues in product, operations, and sales to structure problems and understand the impact across various departments within the company \n",
      "---\n",
      " respond to project requests using jira, and follow the project through the entire sdlc cycle by creating/updating solutions/engagements\n",
      "---\n",
      " maintain codebase and documents using subversion and confluence deep learning research intern digibiosys - san francisco bay area, ca december 2018 to april 2019 \n",
      "---\n",
      " provided data visualizations for heart rate monitor readings using django, angular and d3.js\n",
      "---\n",
      " implemented rudimental nonparametric modeling to obtain predictions\n",
      "---\n",
      " tuned ann parameters to fit models to lower rss \n",
      "---\n",
      " statistical signal processing including fft and mle\n",
      "---\n",
      " configured data visualizations to read from aws elastic beanstalk for real-time data feed junior software engineer msfw - springfield, il april 2018 to december 2018 \n",
      "---\n",
      " assisting in the development, implementation and management of technology-based business solutions to improve our clients\\x92 delivery approach.\n",
      "---\n",
      " coding, testing and implementing configuration changes and assisting in the design of software applications to meet both functional and technical requirements using c# and the .net framework\n",
      "---\n",
      " full stack development using html5, css3, and multiple javascript libraries including angular, bootstrap & jquery\n",
      "---\n",
      " implementing analyses that will identify requirements related to people, processes & technology.\n",
      "---\n",
      " maintain and analyze client databases using ms/azure sql server, and repositories using git data analyst dcc marketing - decatur, il april 2017 to april 2018 \n",
      "---\n",
      " updated and redesigned user experience for dmh company website.?\n",
      "---\n",
      " worked closely with social media team on wordpress blogs and social media marketing.\n",
      "---\n",
      " used site analytics and metrics to define and monitor success, using complex sql queries and stored procedures\n",
      "---\n",
      " built user interface, data visualizations and designed overall user experience for town country bank\n",
      "---\n",
      " designed optimized and usable interfaces for financial applications. project management intern project management intern - decatur, il march 2017 to april 2018 \n",
      "---\n",
      " led 4 person team, developing cost effective supply chain for 785c model parts not routinely kept in stock.\n",
      "---\n",
      " developed a mysql database application to store tool parts and suppliers. database developer/ systems administrator, intern southside country club - decatur, il - decatur, il august 2016 to october 2016 \n",
      "---\n",
      " set up a server using red hat linux.\n",
      "---\n",
      " configured microsoft sql server on the server, built and managed a database for use in the jonas club management software system that runs all the workstations. \n",
      "---\n",
      " bachelor's in electrical engineering northwestern university - chicago, il may 2019 to present master's in business administration, data management millikin university - decatur, il august 2016 to january 2018 bachelor's in computer science monmouth college - monmouth, il may 2016 skills c++ (3 years), javascript (4 years), python (5 years), data visualization (4 years), sql (5 years), data analysis (3 years), machine learning (2 years), deep learning (1 year), applied statistics (2 years), data mining (2 years), software engineering (2 years), r (2 years), big data, data science, business intelligence, sap, excel, html, c# (2 years), .net (2 years), tensorflow (1 year), neural networks (less than 1 year), keras (less than 1 year), torch (less than 1 year), pyspark (less than 1 year) links http://github.com/erikedmonds http://erikedmonds.github.io/election certifications/licenses data science immersive bootcamp july 2018 to present springboard\\x92s data science program assessments data analysis \\x97 expert august 2019 measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. full results: https://share.indeedassessments.com/share_assignment/4v2gy8hm75w1579r technical support \\x97 proficient august 2019 measures a candidate's ability to apply protocols to identify errors and solutions in order to maintain system function. full results: https://share.indeedassessments.com/share_assignment/rv-teu566d7d7nfs indeed assessments provides skills tests that are not indicative of a license or certification, or continued development in any professional field. publications hospital data mining with r 2017-05 - collected, munged and normalized hospital data from healthdata.gov to manipulate in a mysql database. built a database application with python to be analyzed with r. built a bi dashboard in r using flexdashboard and shiny. ride-sharing database application 2017-05 - built a ride-sharing web application for millikin students, using python and django, run on an apache 2.4 web server hosted at millikin. built the front-end using angularjs.\n",
      "\n",
      "---\n",
      " above 8+ years of experience in large unstructured data, datasets of structured, data visualization, data acquisition, predictive modeling, data validation.\n",
      "---\n",
      " develop, maintain and teach new tools and methodologies related to data science and high - performance computing.\n",
      "---\n",
      " data scientist with proven expertise in data analysis, machine learning, and modeling.\n",
      "---\n",
      " experience in machine learning algorithms such as linear regression, logistic regression, naive bayes, decision trees, k-means clustering and association rules.\n",
      "---\n",
      " experience in applying predictive modeling and machine learning algorithms for analytical reports.\n",
      "---\n",
      " experience using technology to work efficiently with datasets such as scripting, data cleaning tools, statistical software packages.\n",
      "---\n",
      " developed predictive models using decision tree, random forest, na\\xefve bayes, logistic regression, cluster analysis, and neural networks.\n",
      "---\n",
      " very strong in python, statistical analysis, tools, and modeling.\n",
      "---\n",
      " experienced in machine learning and statistical analysis with python scikit-learn.\n",
      "---\n",
      " excellent knowledge in relational data warehouse/olap concepts, database design and methodologies.\n",
      "---\n",
      " strong sql programming skills, with experience in working with functions, packages and triggers.\n",
      "---\n",
      " expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.\n",
      "---\n",
      " skilled in performing data parsing, data manipulation, data architecture, data ingestion and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, remap, subset, reindex, melt and reshape.\n",
      "---\n",
      " worked with nosqldatabase including hbase, cassandra and mongodb.\n",
      "---\n",
      " experienced in big data with hadoop, mapreduce, hdfs and spark.\n",
      "---\n",
      " experienced in data integration validation and data quality controls for etl process and data warehousing using ms visual studio, ssas, ssisand ssrs.\n",
      "---\n",
      " proficient in tableau and r-shiny data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards.\n",
      "---\n",
      " automated recurring reports using sql andpython and visualized them on bi platform like tableau.\n",
      "---\n",
      " worked in development environment like git and vm.\n",
      "---\n",
      " excellent communication skills. successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner.\n",
      "---\n",
      " experience with big data technologies like hadoop and spark would be a plus.\n",
      "---\n",
      " worked and extracted data from various database sources like sql server, oracle and db2.\n",
      "---\n",
      " experience working at pricing and/or revenue management would be a plus.\n",
      "---\n",
      " familiarity with agile principles (e.g. scrum), facilitating workshops and prototyping.\n",
      "---\n",
      " hands on experience with r-studio for doing data pre-processing and building machine learning algorithms on different datasets.\n",
      "---\n",
      " good knowledge in nosql databases like hbase and mongo db. time series analysis -arima, neural networks, sentiment analysis, forecasting and text mining.\n",
      "---\n",
      " cluster analysis, principal component analysis, association rules, recommender systems.\n",
      "---\n",
      " inferential statistics, hypothesis testing, descriptive, and sampling. work experience data scientist palo alto networks dallas tx - dallas, tx august 2018 to present description:palo alto networks, inc. an american multinational cyber security company. its core products are a platform that includes advanced firewalls and cloud-based offerings that extend those firewalls to cover other aspects of security.\n",
      "---\n",
      " extracted data from hdfs and prepared data for exploratory analysis using data munging\n",
      "---\n",
      " built models using statistical techniques like bayesian hmm and machine learning classification models like xgboost, svm, and random forest.\n",
      "---\n",
      " participated in all phases of data mining, data cleaning, data collection, developing models, validation, and visualization and performed gap analysis.\n",
      "---\n",
      " a highly immersive data science program involving data manipulation& visualization, web scraping, machine learning, python programming, sql, git, mongodb, hadoop.\n",
      "---\n",
      " setup storage and data analysis tools in aws cloud computing infrastructure.\n",
      "---\n",
      " installed and used caffe deep learning framework\n",
      "---\n",
      " worked on different data formats such as json, xml and performed machine learning algorithms in python.\n",
      "---\n",
      " worked as data architects and it architects to understand the movement of data and its storage and er studio 9.7\n",
      "---\n",
      " used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, nltk in python for developing various machine learning algorithms.\n",
      "---\n",
      " data manipulation and aggregation from different source using nexus, business objects, toad, power bi and smart view.\n",
      "---\n",
      " implemented agile methodology for building an internal application.\n",
      "---\n",
      " focus on integration overlap and informatica newer commitment to mdm with the acquisition of identity systems.\n",
      "---\n",
      " coded proprietary packages to analyze and visualize spcfile data to identify bad spectra and samples to reduce unnecessary procedures and costs.\n",
      "---\n",
      " programmed a utility in python that used multiple packages (numpy, scipy, pandas)\n",
      "---\n",
      " implemented classification using supervised algorithms like logistic regression, decision trees, naive bayes, knn.\n",
      "---\n",
      " as architect delivered various complex olapdatabases/cubes, scorecards, dashboards and reports.\n",
      "---\n",
      " updated python scripts to match training data with our database stored in aws cloud search, so that we would be able to assign each document a response label for further classification.\n",
      "---\n",
      " used teradata utilities such as fast export, mload for handling various tasks data migration/etl from oltp source systems to olap target systems\n",
      "---\n",
      " data transformation from various resources, data organization, features extraction from raw and stored.\n",
      "---\n",
      " validated the machine learning classifiers using roc curves and lift charts.\n",
      "---\n",
      " environment: unix, python 3.5.2, mllib, sas, regression, logistic regression, hadoop 2.7.4, nosql, teradata, oltp, random forest, olap, hdfs, ods, nltk, svm, json, xml and mapreduce. data scientist cisco - dallas, tx may 2017 to july 2018 description:cisco systems, inc. is an american multinational technology conglomerate that develops, manufactures and sells networking hardware, telecommunications equipment and other high-technology services and products. cisco helps seize the opportunities of tomorrow by proving that amazing things can happen when you connect the unconnected. an integral part of our dna is creating long-lasting customer partnerships, working together to identify our customers' needs and provide solutions that fuel their success.\n",
      "---\n",
      " utilized spark, scala, hadoop, hql, vql, oozie, pyspark, data lake, tensor flow, hbase, cassandra, redshift, mongodb, kafka, kinesis, spark streaming, edward, cuda, mllib, aws, python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.\n",
      "---\n",
      " utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.\n",
      "---\n",
      " application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (nlp), supervised and unsupervised, regression models, social network analysis, neural networks, deep learning, svm, clustering to identify volume using scikit-learn package in python, matlab.\n",
      "---\n",
      " worked onanalyzing data from google analytics, adwords, facebook etc.\n",
      "---\n",
      " evaluated models using cross validation, log loss function, roc curves and used auc for feature selection and elastic technologies like elasticsearch, kibana.\n",
      "---\n",
      " performed multinomial logistic regression, decision tree, random forest, svm to classify package is going to deliver on time for the new route.\n",
      "---\n",
      " performed data analysis by using hive to retrieve the data from hadoop cluster, sql to retrieve data from oracle database and used etl for data transformation.\n",
      "---\n",
      " performed data cleaning, features scaling, features engineering using pandas and numpy packages in python.\n",
      "---\n",
      " performed data cleaning and feature selection using mllib package in pyspark and working with deep learning frameworks such as caffe, neon.\n",
      "---\n",
      " developed spark/scala,r python for regular expression (regex) project in the hadoop/hive environment with linux/windows for big data resources.\n",
      "---\n",
      " used clustering technique k-means to identify outliers and to classify un-labeled data.\n",
      "---\n",
      " tracking operations using sensors until certain criteria is met using air flow technology.\n",
      "---\n",
      " responsible for different data mapping activities from source systems to teradata using utilities like tpump, fexp,bteq, mload, fload etc\n",
      "---\n",
      " addressed over fitting by implementing of the algorithm regularization methods like l1 and l2.\n",
      "---\n",
      " used principal component analysis in feature engineering to analyze high dimensional data.\n",
      "---\n",
      " used mllib, spark's machine learning library to build and evaluate different models.\n",
      "---\n",
      " created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.\n",
      "---\n",
      " developed map reduce pipeline for feature extraction using hive and pig.\n",
      "---\n",
      " created data quality scripts using sql and hive to validate successful data load and quality of the data. created various types of data visualizations using python and tableau.\n",
      "---\n",
      " environment: python 2.x, cdh5, hdfs, hadoop 2.3, hive, impala, aws, linux, spark, tableau desktop, sql server 2014, microsoft excel, matlab, spark sql, pyspark. data scientist cardlytics, inc - atlanta, ga january 2016 to april 2017 description: cardlytics uses purchase-based intelligence to make marketing more relevant and measurable. it help marketers identify, reach and influence likely buyers at scale, as well as measure the true sales impact of marketing campaigns.\n",
      "---\n",
      " used ssis to create etl packages to validate, extract, transform and load data into data warehouse and data mart.\n",
      "---\n",
      " maintained and developed complex sql queries, stored procedures, views, functions and reports that meet customer requirements using microsoft sql server 2008 r2.\n",
      "---\n",
      " created views and table-valued functions, common table expression (cte), joins, complex sub queries to provide the reporting solutions.\n",
      "---\n",
      " optimized the performance of queries with modification in t-sql queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.\n",
      "---\n",
      " created ssis packages using pivot transformation, fuzzy lookup, derived columns, condition split, aggregate, execute sql task, data flow task and execute package task.\n",
      "---\n",
      " migrated data from sas environment to sql server 2008 via sql integration services (ssis).\n",
      "---\n",
      " developed and implemented several types of financial reports (income statement, profit& loss statement, ebit, roic reports) by using ssrs.\n",
      "---\n",
      " developed parameterized dynamic performance reports (gross margin, revenue base on geographic regions, profitability based on web sales and smartphone app sales) and ran the reports every month and distributed them to respective departments through mailing server subscriptions and sharepoint server.\n",
      "---\n",
      " designed and developed new reports and maintained existing reports using microsoft sql reporting services (ssrs) and microsoft excel to support the firm's strategy and management.\n",
      "---\n",
      " created sub-reports, drill down reports, summary reports, parameterized reports, and ad-hoc reports using ssrs.\n",
      "---\n",
      " used sas/sql to pull data out from databases and aggregate to provide detailed reporting based on the user requirements.\n",
      "---\n",
      " used sas for pre-processing data, sql queries, data analysis, generating reports, graphics, and statistical analyses.\n",
      "---\n",
      " provided statistical research analyses and data modeling support for mortgage product.\n",
      "---\n",
      " perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysis using sas programming.\n",
      "---\n",
      " environment: sql server 2008 r2, db2, oracle, sql server management studio, sas/ base, sas/sql, sas/enterprise guide, ms bi suite (ssis/ssrs), t-sql, sharepoint 2010, visual studio 2010, agile/scrum. data analyst beacon healthcare communications - bedminster, nj march 2014 to december 2015 description: at beacon, we're all engagement architects - people with significant industry experience engaging the 3 primary healthcare customers - consumers, providers, and payers.and because we also engage with one another, we're able to provide a more efficient integration of thinking right from the start. one that looks at all the stakeholders, regardless of what you hire us for.\n",
      "---\n",
      " used ssis to create etl packages to validate, extract, transform and load data into data warehouse and data mart.\n",
      "---\n",
      " collaborating with business and technology teams.\n",
      "---\n",
      " data analysis-data collection, data transformation and data loading the data using different etl systems like ssis and informatica.\n",
      "---\n",
      " performed source to target mapping as part of data migration from jd edwards system to agile pdm system.\n",
      "---\n",
      " data migration testing and implementation activities using ssis and ssrs tools of microsoft sql server 2008.\n",
      "---\n",
      " involved in construction of data flow diagrams and documentation of the processes.\n",
      "---\n",
      " interacted with end users for requirements study and analysis by jad (joint application development).\n",
      "---\n",
      " participated in system and use case modeling like activity and use case diagrams.\n",
      "---\n",
      " analyzed user requirements & worked with data modelers to identify entities and relationship for data modeling.\n",
      "---\n",
      " actively participated in the design of data model like conceptual, logical models using erwin. used exception handling application block for checking errors/exceptions across the website.\n",
      "---\n",
      " developed report component, so that it retrieves the data by executing stored procedures throw data access component.\n",
      "---\n",
      " environment: windows, oracle, ms excel, ssis, informatica, gap analysis, erwin data analyst cybermateinfotek limited - hyderabad, telangana december 2012 to february 2014 description: cybermateinfotek ltd. (cil) is a software solutions and it services company, was founded in may 1994 at hyderabad, india and is a. cil is an offshore software development company executing projects on web & web related technologies (both microsoft & open source's).\n",
      "---\n",
      " involved in data mapping specifications to create and execute detailed system test plans. the data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.\n",
      "---\n",
      " worked closely with stakeholders to understand, define, document business questions needed.\n",
      "---\n",
      " review system/application requirements (functional specifications), test results and metrics for quality and completeness.\n",
      "---\n",
      " designed and developed oracle pl/sql procedures and unix shell scripts for data import/export and data conversions.\n",
      "---\n",
      " analyzed the source data coming from different sources (sql server, oracle and also from flat files like access and excel) and working with business users and developers to develop the model.\n",
      "---\n",
      " have used informatica data quality (idq) and informatica power center as etl tools to extract the data from various sources systems and transform them into one common format and load them into target database for the analysis purpose from data warehouse.\n",
      "---\n",
      " accomplished data analysis, statistical reports and graphs based on the business requirement using sas/base, sas/macro and sas/graph, sas/sql, sas/access, sas/ods and sas/connect.\n",
      "---\n",
      " worked on predictive modeling using sas/sql.\n",
      "---\n",
      " executed sql queries to validate actual test results and match expected results as per financial rules.\n",
      "---\n",
      " responsible for maintaining the integrity of the sql database and reporting any issues to the database architect.\n",
      "---\n",
      " design and model the reporting data warehouse considering current and future reporting requirement\n",
      "---\n",
      " involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process.\n",
      "---\n",
      " involved with statistical domain experts to understand the data and worked with data management team on data quality assurance.\n",
      "---\n",
      " environment: sql server, oracle pl/sql, informatics data quality (idq), informatics powercenter (designer, workflow manager workflow monitor), unix, sas/base, sas/macro and sas/graph, sas/sql, sas/access, sas/ods and sas/connect. data analyst rsoft india pvt.ltd - bengaluru, karnataka january 2011 to november 2012 description: rsoft is a leading software product development company in india. we develop mobile & cloud based customer relationship management (crm) software solution for all size of businesses. such as small, medium, large size of businesses, b2b and b2c enterprises to increase leads and sales opportunities.\n",
      "---\n",
      " understanding the requirements and develop various packages in ssis.\n",
      "---\n",
      " gathered requirements from jad/jar sections with developers and business clients.\n",
      "---\n",
      " designed the business requirement collection approach based on the project scope and sdlc methodology.\n",
      "---\n",
      " designs and develops the logical and physical data models to support the data marts and the data warehouse\n",
      "---\n",
      " create sql queries for product components to update facets backend tables and create product prefixes.\n",
      "---\n",
      " involved in formatting data stores and generate uml diagrams of logical and physical data.\n",
      "---\n",
      " developed project plans and manage project scope.\n",
      "---\n",
      " identified/documented data sources and transformation rules required to populate and maintain data warehouse content.\n",
      "---\n",
      " write and execute positive and negative test cases to ensure the data originating from the data warehouse (oracle db) is accurate through to the sql db in the applications.\n",
      "---\n",
      " work and triage facets configuration issues and route work back for correct processing.\n",
      "---\n",
      " document step by step facets configuration steps for the quality assurance team.\n",
      "---\n",
      " assisted in building a business analysis process model using rational rose and visio.\n",
      "---\n",
      " created ad-hoc and custom reports using microsoft access, cognos bi and crystal reports.\n",
      "---\n",
      " performed extensive requirement analysis and developed use cases and workflows.\n",
      "---\n",
      " designed and developed use cases, activity diagrams, sequence diagrams, and ood\n",
      "---\n",
      " played a key role in the planning, user accepted testing, and implementation of system enhancements and conversions.\n",
      "---\n",
      " updated provider tables, diagnosis tables, fee schedules, and service is contained in facetsback-end tables.\n",
      "---\n",
      " environment: ms sql server 2008r2, bids 2008 (ssis), jad/jar, ssis, cognos bi, use cases, activity diagrams, sequence diagrams. \n",
      "---\n",
      " bachelor's skills db2, microsoft sql server, microsoft sql server 2008, sql server, sql server 2008, mysql, oracle, sql, cassandra, hdfs, impala, mapreduce, oozie, sqoop, hbase, kafka, flume, hadoop, mongodb, splunk additional information technical skills\n",
      "---\n",
      " bigdata/hadoop technologies hadoop, hdfs, yarn, mapreduce, hive, pig, impala, sqoop, flume, spark, kafka, storm, drill, zookeeper and oozie\n",
      "---\n",
      " languages\n",
      "---\n",
      " html5,dhtml, wsdl, css3, c, c++, xml,r/r studio, sas enterprise guide, sas, r (caret, weka, ggplot), perl, matlab, mathematica, fortran, dtd, schemas, json, ajax, java, scala, python (numpy, scipy, pandas, gensim, keras), java script, shell scripting\n",
      "---\n",
      " no sql databases cassandra, hbase, mongodb, mariadb\n",
      "---\n",
      " business intelligence tools\n",
      "---\n",
      " tableau server, tableau reader, tableau, splunk, sap business objects, obiee, sap business intelligence, qlikview, amazon redshift, or azure data warehouse\n",
      "---\n",
      " development tools microsoft sql studio, intellij, eclipse, netbeans.\n",
      "---\n",
      " development methodologies agile/scrum, uml, design patterns, waterfall\n",
      "---\n",
      " build tools jenkins, toad, sql loader, maven, ant, rtc, rsa, control-m, oziee, hue, soap ui\n",
      "---\n",
      " reporting tools ms office (word/excel/power point/ visio/outlook), crystal reports xi, ssrs, cognos 7.0/6.0.\n",
      "---\n",
      " databases microsoft sql server 2008,2010/2012, mysql 4.x/5.x, oracle 11g, 12c, db2, teradata, netezza\n",
      "---\n",
      " operating systems all versions of windows, unix, linux, macintosh hd, sun solaris\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "path = '../datasets/resume_samples.txt'\n",
    "df = pd.DataFrame()\n",
    "counter = 0\n",
    "id_list = []\n",
    "title_list = []\n",
    "skill_list = []\n",
    "#exp_list = []\n",
    "with open(path, 'r',encoding=\"utf8\", errors='backslashreplace') as f:\n",
    "    for line in f.readlines():\n",
    "        \n",
    "        splitted = line.split(':::')\n",
    "        title = splitted[1].split(';')[0]\n",
    "        if 'data scientist' in title.lower():\n",
    "            splitted = splitted[2].lower()\n",
    "            splitted = splitted.replace('\\\\xa0',';;;')\n",
    "            splitted = splitted.replace('*',';;;')\n",
    "            splitted = splitted.replace('\\\\x95',';;;')\n",
    "            splitted = splitted.replace('education',';;;')\n",
    "            #splitted = splitted.replace(',',';;;')\n",
    "            splitted = splitted.replace(';',';;;')\n",
    "            splitted = splitted.split(';;;')\n",
    "            \n",
    "            for skill in splitted[1:]:\n",
    "                if (len(skill)>1) & ('responsibilities:' not in skill.lower()):\n",
    "                    print(skill)\n",
    "                    print('---')\n",
    "                    id_list.append(counter)\n",
    "                    \n",
    "                    title_list.append(title)\n",
    "                    skill_list.append(skill)\n",
    "            counter+=1\n",
    "df['id']=id_list\n",
    "df['title']=title_list\n",
    "df['skill']=skill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18593684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>design sql, r &amp; python scripts to directly qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>aid in the development of predictive modeling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>test model fit using cross-validation and boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>develop in an scrum environment with team mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>model data using pandas/r dataframes, powerpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5733</th>\n",
       "      <td>66</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>development methodologies agile/scrum, uml, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>66</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>build tools jenkins, toad, sql loader, maven,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>66</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>reporting tools ms office (word/excel/power p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>66</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>databases microsoft sql server 2008,2010/2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>66</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>operating systems all versions of windows, un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5738 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                title  \\\n",
       "0      0  Data Scientist/Analytics Consultant   \n",
       "1      0  Data Scientist/Analytics Consultant   \n",
       "2      0  Data Scientist/Analytics Consultant   \n",
       "3      0  Data Scientist/Analytics Consultant   \n",
       "4      0  Data Scientist/Analytics Consultant   \n",
       "...   ..                                  ...   \n",
       "5733  66                       Data Scientist   \n",
       "5734  66                       Data Scientist   \n",
       "5735  66                       Data Scientist   \n",
       "5736  66                       Data Scientist   \n",
       "5737  66                       Data Scientist   \n",
       "\n",
       "                                                  skill  \n",
       "0      design sql, r & python scripts to directly qu...  \n",
       "1      aid in the development of predictive modeling...  \n",
       "2      test model fit using cross-validation and boo...  \n",
       "3      develop in an scrum environment with team mem...  \n",
       "4      model data using pandas/r dataframes, powerpi...  \n",
       "...                                                 ...  \n",
       "5733   development methodologies agile/scrum, uml, d...  \n",
       "5734   build tools jenkins, toad, sql loader, maven,...  \n",
       "5735   reporting tools ms office (word/excel/power p...  \n",
       "5736   databases microsoft sql server 2008,2010/2012...  \n",
       "5737   operating systems all versions of windows, un...  \n",
       "\n",
       "[5738 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbfd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_bert(string):\n",
    "    string = string.strip(\"\\n.:\")\n",
    "    string = string.strip(\"\")\n",
    "    string = string.strip(\"\\\\n\")\n",
    "    string = string.replace(\"/\",\" \")\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea88b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator KMeans from version 0.23.1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k31_full = pickle.load(open('../Model_Selection/k_31_full', 'rb'))\n",
    "objectives = []\n",
    "model = 'all-distilroberta-v1'\n",
    "model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635fea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = []\n",
    "for s in df['skill'].to_list():\n",
    "    skill_list.append(preprocess_for_bert(s))\n",
    "embeddings = model.encode(skill_list)\n",
    "clusters =  k31_full.predict(np.array(embeddings.tolist()))\n",
    "df['cluster']=clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7770d1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>skill</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>design sql, r &amp; python scripts to directly qu...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>aid in the development of predictive modeling...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>test model fit using cross-validation and boo...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>develop in an scrum environment with team mem...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>model data using pandas/r dataframes, powerpi...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                title  \\\n",
       "0   0  Data Scientist/Analytics Consultant   \n",
       "1   0  Data Scientist/Analytics Consultant   \n",
       "2   0  Data Scientist/Analytics Consultant   \n",
       "3   0  Data Scientist/Analytics Consultant   \n",
       "4   0  Data Scientist/Analytics Consultant   \n",
       "\n",
       "                                               skill  cluster  \n",
       "0   design sql, r & python scripts to directly qu...       11  \n",
       "1   aid in the development of predictive modeling...        6  \n",
       "2   test model fit using cross-validation and boo...       19  \n",
       "3   develop in an scrum environment with team mem...       16  \n",
       "4   model data using pandas/r dataframes, powerpi...       19  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c4dea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "embeddings_f = embeddings.astype(float)\n",
    "for i, row in df.iterrows():\n",
    "    cluster = row['cluster']\n",
    "    scores.append(util.pytorch_cos_sim(k31_full.cluster_centers_[cluster], embeddings_f[i]).item())\n",
    "df['score']=scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd92543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding']=embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cff7df14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>skill</th>\n",
       "      <th>cluster</th>\n",
       "      <th>score</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>design sql, r &amp; python scripts to directly qu...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.675510</td>\n",
       "      <td>[0.020906170830130577, -0.02053937129676342, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>aid in the development of predictive modeling...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.641226</td>\n",
       "      <td>[0.045750461518764496, -0.03818187490105629, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>test model fit using cross-validation and boo...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.368120</td>\n",
       "      <td>[-0.011868440546095371, -0.0615849532186985, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>develop in an scrum environment with team mem...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.603168</td>\n",
       "      <td>[0.04099218547344208, -0.02379242517054081, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Analytics Consultant</td>\n",
       "      <td>model data using pandas/r dataframes, powerpi...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.494827</td>\n",
       "      <td>[-0.0024511003866791725, -0.02079622820019722,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                title  \\\n",
       "0   0  Data Scientist/Analytics Consultant   \n",
       "1   0  Data Scientist/Analytics Consultant   \n",
       "2   0  Data Scientist/Analytics Consultant   \n",
       "3   0  Data Scientist/Analytics Consultant   \n",
       "4   0  Data Scientist/Analytics Consultant   \n",
       "\n",
       "                                               skill  cluster     score  \\\n",
       "0   design sql, r & python scripts to directly qu...       11  0.675510   \n",
       "1   aid in the development of predictive modeling...        6  0.641226   \n",
       "2   test model fit using cross-validation and boo...       19  0.368120   \n",
       "3   develop in an scrum environment with team mem...       16  0.603168   \n",
       "4   model data using pandas/r dataframes, powerpi...       19  0.494827   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.020906170830130577, -0.02053937129676342, -...  \n",
       "1  [0.045750461518764496, -0.03818187490105629, -...  \n",
       "2  [-0.011868440546095371, -0.0615849532186985, -...  \n",
       "3  [0.04099218547344208, -0.02379242517054081, 0....  \n",
       "4  [-0.0024511003866791725, -0.02079622820019722,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa0934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/df_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbf08312",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "top10s =[]\n",
    "top25s=[]\n",
    "top50s=[]\n",
    "for cluster in range(len(k31_full.cluster_centers_)):\n",
    "    df_ = df[df['cluster']==cluster]\n",
    "    mean = df_['score'].mean()\n",
    "    means.append(mean)\n",
    "    df_ = df_.sort_values(by=['score'],ascending=False)\n",
    "    top10 = df_['score'][:int(len(df_['score'])*0.10)].mean()\n",
    "    top10s.append(top10)\n",
    "    top25 = df_['score'][:int(len(df_['score'])*0.25)].mean()\n",
    "    top25s.append(top25)\n",
    "    top50 = df_['score'][:int(len(df_['score'])*0.50)].mean()\n",
    "    top50s.append(top50)\n",
    "\n",
    "df_cv_summary = pd.DataFrame()\n",
    "df_cv_summary['cluster']=range(len(k31_full.cluster_centers_))\n",
    "\n",
    "df_cv_summary['mean']=means\n",
    "df_cv_summary['top10']=top10s\n",
    "df_cv_summary['top25']=top25s\n",
    "df_cv_summary['top50']=top50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a083c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score is 0.4863529743269017\n",
      "top10% of scores is 0.7222289888115668\n",
      "top25% of scores is 0.6733023379227258\n",
      "top50% of scores is 0.6137796974992537\n"
     ]
    }
   ],
   "source": [
    "mean = df['score'].mean()\n",
    "df_=df.sort_values(by=['score'],ascending=False)\n",
    "top10 = df_['score'][:int(len(df_['score'])*0.10)].mean()\n",
    "top25 = df_['score'][:int(len(df_['score'])*0.25)].mean()\n",
    "top50 = df_['score'][:int(len(df_['score'])*0.50)].mean()\n",
    "\n",
    "print('mean score is '+str(mean))\n",
    "print('top10% of scores is '+str(top10))\n",
    "print('top25% of scores is '+str(top25))\n",
    "print('top50% of scores is '+str(top50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "efed3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_summary.to_csv('df_cv_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f4dfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    result = 1 - spatial.distance.cosine(k21.cluster_centers_[clusters[index]], embeddings[index])\n",
    "    results.append(result)\n",
    "df['similarity_to_cluster']=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a2a34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k20_bigrams = pickle.load(open('../Experiments/k20_bigrams', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5590d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "for cluster in clusters:\n",
    "    bigrams.append(k20_bigrams[cluster])\n",
    "df['bigrams_k20']=bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10eef27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:34:36,442 - INFO     - NumExpr defaulting to 2 threads.\n"
     ]
    }
   ],
   "source": [
    "d=dtale.show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57304b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To gain access to an instance object simply pass the value from 'ID' to dtale.get_instance(ID)\n",
      "\n",
      "ID Name                                    URL\n",
      " 1      http://f468fa972f9e:40000/dtale/main/1\n",
      "        http://f468fa972f9e:40000/dtale/main/1\n"
     ]
    }
   ],
   "source": [
    "dtale.instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ee55b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1427 data visualization, large datasets, experience data, knowledge data, data visualisation, large data, big data, understanding data, data engineering, data pipelines, \n",
      "-----------------------\n",
      "3 1061 software development, software engineering, best practices, data science, experience working, processing frameworks, frameworks spark, tools (e.g., standard software, ci cd,, \n",
      "-----------------------\n",
      "11 847 develop processes, processes tools, tools monitor, model performance, performance data, data accuracy, analyze data, data analysis, ability develop, develop experimental, \n",
      "-----------------------\n",
      "16 492 machine learning, learning techniques, learning algorithms, understanding machine, experience machine, deep learning, learning experience, strong knowledge, knowledge machine, machine learning,, \n",
      "-----------------------\n",
      "7 383 relational databases, experience sql, knowledge sql, sql queries, experience databases, advanced sql, sql skills, experience relational, sql experience, python sql, \n",
      "-----------------------\n",
      "19 338 programming languages, languages python,, programming skills, python data, programming experience, python, r,, data analysis, proficient python, data science, languages (python,, \n",
      "-----------------------\n",
      "8 302 product teams, support identifying, identifying opportunities, opportunities creating, creating customer, customer value, identify opportunities, closely business, business outcomes, better understand, \n",
      "-----------------------\n",
      "17 140 multiple projects, r&d projects, project team, research partners, specific development, development projects, product engineering, engineering teams, identify trends, experience building, \n",
      "-----------------------\n",
      "13 136 data science, data engineering, experience data, data scientists, years experience, 5+ years, years data, science experience, data scientist, 3+ years, \n",
      "-----------------------\n",
      "1 126 experience demonstrated, scientific journals,, scientific cv, cv contact, contact details;, scientific results, present scientific, scientific technical, drug discovery, discovery early, \n",
      "-----------------------\n",
      "20 89 experience cloud, google cloud, cloud platform, experience working, ms azure, platform technology, technology (aws, (aws ms, ms azure), cloud services, \n",
      "-----------------------\n",
      "6 57 career development, willingness learn, new technologies, learn new, genuine willingness, learn drive, dedicated house, house l&d, l&d department,, department, access, \n",
      "-----------------------\n",
      "0 57 teams across, key stakeholders, strong communication, communication skills, internal stakeholders, across business, ability communicate, technical non-technical, interest working, informal, dynamic, \n",
      "-----------------------\n",
      "14 43 international environment, work environment, relocation packages, city center, great job,, job, international, willing travel, relocation support, flexible enthusiastic, enthusiastic work, \n",
      "-----------------------\n",
      "10 36 flexible working, work environment, working hours, environment flexible, working environment, flexible work, dynamic work, work home, hours flexible, large degree, \n",
      "-----------------------\n",
      "2 29 team player, ability work, communication skills, team environment, excellent communication, collaboration skills, skills ability, within team, communication teamwork, good communication, \n",
      "-----------------------\n",
      "12 27 competitive salary, competitive compensation, salary competitive, compensation package, package competitive, benefits package, including performance, social benefits, benefits competitive, competitive remuneration, \n",
      "-----------------------\n",
      "18 23 working people, problem solving, lifestyle curiosity,, curiosity, learning,, learning, sharing,, sharing, working, people share, share hunger.\"], think creatively, solve problems, \n",
      "-----------------------\n",
      "9 22 computer science,, degree computer, science, mathematics,, masters degree, phd computer, related field, computer science, quantitative field, statistics, engineering,, engineering, computer, \n",
      "-----------------------\n",
      "4 16 communication skills, skills excellent, excellent communication, written communication, verbal written, skills good, good communication, communication skills,, strong communication, verbal communication, \n",
      "-----------------------\n",
      "5 3 fluent english, english fluent, written spoken, spoken english, english fluency, fluent written, english skills, language skills, excellent written, written verbal, \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#df['cluster'].value_counts()\n",
    "for c, count in df['cluster'].value_counts().iteritems():\n",
    "    print(str(c)+\" \"+str(count)+\" \"+k20_bigrams[c])\n",
    "    print('-----------------------')\n",
    "    \n",
    "for i in range(0,20):\n",
    "    if i not in df['cluster'].to_list():\n",
    "         print('missing '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8215000b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_matrix = []\n",
    "\n",
    "for id in np.unique(df[['id']].values):\n",
    "    cluster_counter = []\n",
    "    filtered = df[df['id']==id]\n",
    "    for cluster in range(0,20):\n",
    "        c=0\n",
    "        for index, row in filtered.iterrows():\n",
    "            if row['cluster']==cluster:\n",
    "                c+=1/len(filtered)\n",
    "        cluster_counter.append(c)\n",
    "    cluster_matrix.append(cluster_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ecd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume  = pd.DataFrame()\n",
    "df_resume['id']=np.unique(df[['id']].values)\n",
    "df_resume['matrix']=cluster_matrix\n",
    "#df_resume = df_resume.append(pd.DataFrame(df_resume['matrix'].tolist()))\n",
    "foo=pd.DataFrame(df_resume['matrix'].tolist())\n",
    "df_resume = pd.concat([df_resume,foo], axis=1)\n",
    "counted = df_resume.iloc[:,2:].astype(bool).sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa857b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored = df_resume.iloc[:,2:].sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305a0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 15 scored 14.830993416249223 Most common bigrams of this cluster are data visualization, large datasets, experience data, knowledge data, data visualisation, large data, big data, understanding data, data engineering, data pipelines, \n",
      "---------------------\n",
      "cluster 3 scored 13.658230137730028 Most common bigrams of this cluster are software development, software engineering, best practices, data science, experience working, processing frameworks, frameworks spark, tools (e.g., standard software, ci cd,, \n",
      "---------------------\n",
      "cluster 11 scored 9.86464014150455 Most common bigrams of this cluster are develop processes, processes tools, tools monitor, model performance, performance data, data accuracy, analyze data, data analysis, ability develop, develop experimental, \n",
      "---------------------\n",
      "cluster 16 scored 5.301561245213319 Most common bigrams of this cluster are machine learning, learning techniques, learning algorithms, understanding machine, experience machine, deep learning, learning experience, strong knowledge, knowledge machine, machine learning,, \n",
      "---------------------\n",
      "cluster 8 scored 4.035537649346849 Most common bigrams of this cluster are product teams, support identifying, identifying opportunities, opportunities creating, creating customer, customer value, identify opportunities, closely business, business outcomes, better understand, \n",
      "---------------------\n",
      "cluster 19 scored 3.9439604040416816 Most common bigrams of this cluster are programming languages, languages python,, programming skills, python data, programming experience, python, r,, data analysis, proficient python, data science, languages (python,, \n",
      "---------------------\n",
      "cluster 7 scored 3.76658841970619 Most common bigrams of this cluster are relational databases, experience sql, knowledge sql, sql queries, experience databases, advanced sql, sql skills, experience relational, sql experience, python sql, \n",
      "---------------------\n",
      "cluster 1 scored 2.0366765115125465 Most common bigrams of this cluster are experience demonstrated, scientific journals,, scientific cv, cv contact, contact details;, scientific results, present scientific, scientific technical, drug discovery, discovery early, \n",
      "---------------------\n",
      "cluster 17 scored 1.7857924455712717 Most common bigrams of this cluster are multiple projects, r&d projects, project team, research partners, specific development, development projects, product engineering, engineering teams, identify trends, experience building, \n",
      "---------------------\n",
      "cluster 13 scored 1.531772040910981 Most common bigrams of this cluster are data science, data engineering, experience data, data scientists, years experience, 5+ years, years data, science experience, data scientist, 3+ years, \n",
      "---------------------\n",
      "cluster 6 scored 1.1628802955053417 Most common bigrams of this cluster are career development, willingness learn, new technologies, learn new, genuine willingness, learn drive, dedicated house, house l&d, l&d department,, department, access, \n",
      "---------------------\n",
      "cluster 14 scored 0.8986972321999951 Most common bigrams of this cluster are international environment, work environment, relocation packages, city center, great job,, job, international, willing travel, relocation support, flexible enthusiastic, enthusiastic work, \n",
      "---------------------\n",
      "cluster 0 scored 0.826620564653588 Most common bigrams of this cluster are teams across, key stakeholders, strong communication, communication skills, internal stakeholders, across business, ability communicate, technical non-technical, interest working, informal, dynamic, \n",
      "---------------------\n",
      "cluster 9 scored 0.48893708941207087 Most common bigrams of this cluster are computer science,, degree computer, science, mathematics,, masters degree, phd computer, related field, computer science, quantitative field, statistics, engineering,, engineering, computer, \n",
      "---------------------\n",
      "cluster 18 scored 0.47463696708729014 Most common bigrams of this cluster are working people, problem solving, lifestyle curiosity,, curiosity, learning,, learning, sharing,, sharing, working, people share, share hunger.\"], think creatively, solve problems, \n",
      "---------------------\n",
      "cluster 10 scored 0.42890484094086523 Most common bigrams of this cluster are flexible working, work environment, working hours, environment flexible, working environment, flexible work, dynamic work, work home, hours flexible, large degree, \n",
      "---------------------\n",
      "cluster 12 scored 0.42876370126704144 Most common bigrams of this cluster are competitive salary, competitive compensation, salary competitive, compensation package, package competitive, benefits package, including performance, social benefits, benefits competitive, competitive remuneration, \n",
      "---------------------\n",
      "cluster 2 scored 0.3089345253672636 Most common bigrams of this cluster are team player, ability work, communication skills, team environment, excellent communication, collaboration skills, skills ability, within team, communication teamwork, good communication, \n",
      "---------------------\n",
      "cluster 4 scored 0.3071563433451635 Most common bigrams of this cluster are communication skills, skills excellent, excellent communication, written communication, verbal written, skills good, good communication, communication skills,, strong communication, verbal communication, \n",
      "---------------------\n",
      "cluster 5 scored 0.0538181998530161 Most common bigrams of this cluster are fluent english, english fluent, written spoken, spoken english, english fluency, fluent written, english skills, language skills, excellent written, written verbal, \n",
      "---------------------\n",
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:40:42,921 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:40:45,427 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:40:45,959 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:40:47,044 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:40:56,802 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:41:29,002 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:42:21,990 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 20:44:00,929 - ERROR    - Exception occurred while processing request: min() arg is an empty sequence\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 105, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/dtale/views.py\", line 2381, in load_filtered_ranges\n",
      "    overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n",
      "ValueError: min() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "for cluster, score in scored.iteritems():\n",
    "    print(\"cluster \"+str(cluster)+\" scored \"+str(score)+\" Most common bigrams of this cluster are \"+k20_bigrams[cluster])\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca82108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
