{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 010 Retrieving Skill Sets in Job Postings\n",
    "This notebook shows how we gathered a dataset of job postings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from langdetect import detect\n",
    "import os\n",
    "import pickle\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browser header makes web scraping work easier\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function scrapes indeed for a given input term for a given country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(search_term, page_count, country): #returns a list of search links given an input search term\n",
    "    pages = []\n",
    "    for page_no in range(page_count):\n",
    "        page = 'https://'+country+'.indeed.com/jobs?q='+search_term+'&start='+str(page_no*10)\n",
    "        pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets a list of all job postings on one of Indeed search pages. It is combined with function get_pages()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(search_link, country): #Takes in one search page and returns all job links on the page\n",
    "    links = []\n",
    "    page = requests.get(search_link, headers = headers)\n",
    "    search_soup = BeautifulSoup(page.text)\n",
    "    #print(search_soup)\n",
    "    mydivs = search_soup.find_all(\"div\", {\"class\": \"mosaic mosaic-provider-jobcards\"})\n",
    "    for div in mydivs:\n",
    "        classes = div.find_all(\"a\")\n",
    "        for c in classes:\n",
    "            sub = c.get('data-jk')\n",
    "            if isinstance(sub, str):\n",
    "                link = 'https://'+country+'.indeed.com/viewjob?jk='+sub\n",
    "            if link not in links:\n",
    "                links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of urls, created by get_links(). It visits each link and saves the response in a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requests(urls): #Takes in a job link and saves request.text to file\n",
    "    for url in urls:\n",
    "        captcha = True\n",
    "        while captcha:\n",
    "            response = requests.get(url, headers = headers)\n",
    "        \n",
    "            jk = url[33:]\n",
    "            path = \"..\\datasets\\requests\\\\\"+jk\n",
    "        \n",
    "            soup = BeautifulSoup(response.text)\n",
    "            title = soup.find(\"title\")\n",
    "        \n",
    "            if \"hCaptcha solve page\" not in title: # not a captcha\n",
    "                captcha = False\n",
    "            else:\n",
    "                time.sleep(1200)\n",
    "            \n",
    "        with open(path, \"w\") as file:\n",
    "            file.write(response.text)\n",
    "            time.sleep(10+random.uniform(-1,1)) #behave a bit like human\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \n",
    "In this section a set of job postings is scraped from the web, it is stored in a set of response text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['se','no','nl','de','be','es','it','ch','fr','uk','pt','fi','lu','at']\n",
    "link_list = []\n",
    "for country in countries:\n",
    "    pages = get_pages(\"Data Scientist\", 10, country)\n",
    "    for page in pages:\n",
    "        links = get_links(page, country)\n",
    "        link_list.append(links)\n",
    "        time.sleep(10+random.uniform(-1,1)) #behave a bit like human\n",
    "with open(\"link_list.txt\", \"wb\") as fp:\n",
    "    pickle.dump(link_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://nl.indeed.com/viewjob?jk=16f151177a6de429', 'https://nl.indeed.com/viewjob?jk=64bc5ad1e3b61059', 'https://nl.indeed.com/viewjob?jk=a5f719a1728e7074', 'https://nl.indeed.com/viewjob?jk=a7712338f646bd55', 'https://nl.indeed.com/viewjob?jk=984b6301bd1ffb16', 'https://nl.indeed.com/viewjob?jk=23a7e7cfb60d054e', 'https://nl.indeed.com/viewjob?jk=6b5f267033d3b2c3', 'https://nl.indeed.com/viewjob?jk=33213b318cb79506', 'https://nl.indeed.com/viewjob?jk=33ca3ecf5e219745', 'https://nl.indeed.com/viewjob?jk=9060ff5fee52893c', 'https://nl.indeed.com/viewjob?jk=851446284e9a6f45', 'https://nl.indeed.com/viewjob?jk=066501a6c6c3589f', 'https://nl.indeed.com/viewjob?jk=faf312d38dd2cc41', 'https://nl.indeed.com/viewjob?jk=a7dab3a06bddd0aa', 'https://nl.indeed.com/viewjob?jk=78935aa3094b1501']]\n"
     ]
    }
   ],
   "source": [
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list in link_list:\n",
    "    get_requests(list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a dataset from the acquired http responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_row(request):\n",
    "    with open(\"../datasets/requests/\"+ request) as file:\n",
    "        text = file.read()\n",
    "        soup = BeautifulSoup(text)  \n",
    "        full_text = soup.find(\"div\",{\"id\": \"jobDescriptionText\"}).text\n",
    "        if(detect(full_text) != 'en'): #Only parse English job postings\n",
    "            return \n",
    "        \n",
    "        timestamp = None\n",
    "        content = []\n",
    "        title_location = soup.find(\"title\").text\n",
    "        title = title_location.split(\"-\")[0]\n",
    "        location = title_location.split(\"-\")[1]\n",
    "        url = soup.find_all(\"meta\", {\"id\": \"indeed-share-url\"})\n",
    "        url = str(url[0])[15:-25]\n",
    "        \n",
    "        country = url[8:10]\n",
    "\n",
    "        list_elements = []\n",
    "    \n",
    "        divs = soup.find_all(\"div\",{\"id\": \"jobDescriptionText\"})\n",
    "        for div in divs:\n",
    "            uls = div.find_all(\"ul\")\n",
    "            for ul in uls:\n",
    "                for li in ul.find_all('li'):\n",
    "                    list_elements.append(li.text)\n",
    "    return [timestamp,url,title,location,country,full_text,list_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../datasets/requests'\n",
    "df = pd.DataFrame(columns = [\"dt\",\"url\",\"title\",\"location\",\"country\",\"full_text\",\"list_elements\"])\n",
    "for filename in os.listdir(dir):\n",
    "    row_list = request_to_row(filename)\n",
    "    df.loc[len(df)] = row_list \n",
    "df_.to_csv(\"../datasets/requests/df_europe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining with dataset downloaded from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dl=pd.read_csv('../datasets/data_scientist_united_states_job_postings_jobspikr.csv')\n",
    "df_dl = df_dl.dropna(subset=['html_job_description']) # Without the full html the job postings are not useful to us\n",
    "\n",
    "df_dl=df_dl[[\"crawl_timestamp\",\"url\",\"job_title\",\"inferred_city\",\"job_description\",\"html_job_description\"]]# selecting columns\n",
    "countries = []\n",
    "list_of_list_elements = []\n",
    "for index, row in df_dl.iterrows():\n",
    "    countries.append(\"us\")\n",
    "    soup = BeautifulSoup(row[\"html_job_description\"])\n",
    "    list_elements = []\n",
    "    \n",
    "    divs = soup.find_all(\"div\",{\"id\": \"jobDescriptionText\"})\n",
    "    for div in divs:\n",
    "        uls = div.find_all(\"ul\")\n",
    "        for ul in uls:\n",
    "            for li in ul.find_all('li'):\n",
    "                list_elements.append(li.text)\n",
    "    list_of_list_elements.append(list_elements)\n",
    "    \n",
    "df_dl[\"country\"]=countries\n",
    "df_dl[\"list_elements\"] = list_of_list_elements\n",
    "df_dl = df_dl.drop([\"html_job_description\"], axis = 1)\n",
    "df_dl.columns = [\"dt\",\"url\",\"title\",\"location\",\"full_text\",\"country\",\"list_elements\"]\n",
    "df_dl = df_dl.reindex(columns=[\"dt\",\"url\",\"title\",\"location\",\"country\",\"full_text\",\"list_elements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2633"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appending the european dataset with the USA downloaded dataset\n",
    "df_full = df.append(df_dl)\n",
    "df_full = df_full.drop_duplicates(subset=['location','full_text'])\n",
    "df_full.to_csv('../datasets/df_full.csv')\n",
    "len(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Skills in a Dataset of Job Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../Datasets/df_full.csv')\n",
    "row_id = range(0,len(df),1)\n",
    "df['row_id'] = row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_sbert(string):\n",
    "    string = string.strip(\"\\n.’:\")\n",
    "    string = string.strip(\"’\")\n",
    "    string = string.strip(\"\\\\n\")\n",
    "    string = string.replace(\"/\",\" \")\n",
    "    return(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivots df of vacancy texts into a df of job requirements retrieved from 'list_elements' in vacancy df\n",
    "columns = df.columns.values.tolist()\n",
    "df_r = pd.DataFrame(columns = ['requirement_raw','requirement_tokenized']+columns)\n",
    "for index, row in df.iterrows():\n",
    "    string = str(row['list_elements'])\n",
    "    remove = ['\\,','[',']','\\\\n']\n",
    "    for char in remove:\n",
    "        string.replace(char, '')\n",
    "    string = string.strip('\\n')\n",
    "    splitted = string.split('\\'') #list of requirements is stored as string so split on '\n",
    "    list = []\n",
    "    for item in splitted:\n",
    "        if len(item)>2:\n",
    "            requirement_raw = item\n",
    "            requirement_tokenized = preprocess_for_sbert(item)\n",
    "            requirement_tokenized = gensim.utils.simple_preprocess(requirement_tokenized)\n",
    "            row_list = [requirement_raw,requirement_tokenized]\n",
    "            for col in columns:\n",
    "                row_list = row_list+[df.loc[index,col]]          \n",
    "            df_r.loc[len(df_r)] = row_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requirement_raw</th>\n",
       "      <th>requirement_tokenized</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dt</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>full_text</th>\n",
       "      <th>list_elements</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Collaborate with stakeholders and other engine...</td>\n",
       "      <td>[collaborate, with, stakeholders, and, other, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-08 21:01:03.303868</td>\n",
       "      <td>https://se.indeed.com/viewjob?jk=b0669075c820856d</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Jobba hemifrån</td>\n",
       "      <td>se</td>\n",
       "      <td>About us\\n\\nHere at Mavenoid, we are building ...</td>\n",
       "      <td>['Collaborate with stakeholders and other engi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nApply the right tools for the job and solve ...</td>\n",
       "      <td>[apply, the, right, tools, for, the, job, and,...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-08 21:01:03.303868</td>\n",
       "      <td>https://se.indeed.com/viewjob?jk=b0669075c820856d</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Jobba hemifrån</td>\n",
       "      <td>se</td>\n",
       "      <td>About us\\n\\nHere at Mavenoid, we are building ...</td>\n",
       "      <td>['Collaborate with stakeholders and other engi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nDevelop knowledge representations and virtua...</td>\n",
       "      <td>[develop, knowledge, representations, and, vir...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-08 21:01:03.303868</td>\n",
       "      <td>https://se.indeed.com/viewjob?jk=b0669075c820856d</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Jobba hemifrån</td>\n",
       "      <td>se</td>\n",
       "      <td>About us\\n\\nHere at Mavenoid, we are building ...</td>\n",
       "      <td>['Collaborate with stakeholders and other engi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nProvide model explanations and apply structu...</td>\n",
       "      <td>[provide, model, explanations, and, apply, str...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-08 21:01:03.303868</td>\n",
       "      <td>https://se.indeed.com/viewjob?jk=b0669075c820856d</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Jobba hemifrån</td>\n",
       "      <td>se</td>\n",
       "      <td>About us\\n\\nHere at Mavenoid, we are building ...</td>\n",
       "      <td>['Collaborate with stakeholders and other engi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nLearn desired behavior from examples using c...</td>\n",
       "      <td>[learn, desired, behavior, from, examples, usi...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-08 21:01:03.303868</td>\n",
       "      <td>https://se.indeed.com/viewjob?jk=b0669075c820856d</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Jobba hemifrån</td>\n",
       "      <td>se</td>\n",
       "      <td>About us\\n\\nHere at Mavenoid, we are building ...</td>\n",
       "      <td>['Collaborate with stakeholders and other engi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     requirement_raw  \\\n",
       "0  Collaborate with stakeholders and other engine...   \n",
       "1  \\nApply the right tools for the job and solve ...   \n",
       "2  \\nDevelop knowledge representations and virtua...   \n",
       "3  \\nProvide model explanations and apply structu...   \n",
       "4  \\nLearn desired behavior from examples using c...   \n",
       "\n",
       "                               requirement_tokenized Unnamed: 0  \\\n",
       "0  [collaborate, with, stakeholders, and, other, ...          0   \n",
       "1  [apply, the, right, tools, for, the, job, and,...          0   \n",
       "2  [develop, knowledge, representations, and, vir...          0   \n",
       "3  [provide, model, explanations, and, apply, str...          0   \n",
       "4  [learn, desired, behavior, from, examples, usi...          0   \n",
       "\n",
       "                           dt  \\\n",
       "0  2021-08-08 21:01:03.303868   \n",
       "1  2021-08-08 21:01:03.303868   \n",
       "2  2021-08-08 21:01:03.303868   \n",
       "3  2021-08-08 21:01:03.303868   \n",
       "4  2021-08-08 21:01:03.303868   \n",
       "\n",
       "                                                 url            title  \\\n",
       "0  https://se.indeed.com/viewjob?jk=b0669075c820856d  Data Scientist    \n",
       "1  https://se.indeed.com/viewjob?jk=b0669075c820856d  Data Scientist    \n",
       "2  https://se.indeed.com/viewjob?jk=b0669075c820856d  Data Scientist    \n",
       "3  https://se.indeed.com/viewjob?jk=b0669075c820856d  Data Scientist    \n",
       "4  https://se.indeed.com/viewjob?jk=b0669075c820856d  Data Scientist    \n",
       "\n",
       "           location country  \\\n",
       "0   Jobba hemifrån       se   \n",
       "1   Jobba hemifrån       se   \n",
       "2   Jobba hemifrån       se   \n",
       "3   Jobba hemifrån       se   \n",
       "4   Jobba hemifrån       se   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  About us\\n\\nHere at Mavenoid, we are building ...   \n",
       "1  About us\\n\\nHere at Mavenoid, we are building ...   \n",
       "2  About us\\n\\nHere at Mavenoid, we are building ...   \n",
       "3  About us\\n\\nHere at Mavenoid, we are building ...   \n",
       "4  About us\\n\\nHere at Mavenoid, we are building ...   \n",
       "\n",
       "                                       list_elements row_id  \n",
       "0  ['Collaborate with stakeholders and other engi...      0  \n",
       "1  ['Collaborate with stakeholders and other engi...      0  \n",
       "2  ['Collaborate with stakeholders and other engi...      0  \n",
       "3  ['Collaborate with stakeholders and other engi...      0  \n",
       "4  ['Collaborate with stakeholders and other engi...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.to_csv(\"../datasets/df_r.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
