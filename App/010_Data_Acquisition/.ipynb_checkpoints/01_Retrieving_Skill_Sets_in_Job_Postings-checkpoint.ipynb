{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 010 Retrieving Skill Sets in Job Postings\n",
    "This notebook shows how we gathered a dataset of job postings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from langdetect import detect\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browser header makes web scraping work easier\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function scrapes indeed for a given input term for a given country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(search_term, page_count, country): #returns a list of search links given an input search term\n",
    "    pages = []\n",
    "    for page_no in range(page_count):\n",
    "        page = 'https://'+country+'.indeed.com/jobs?q='+search_term+'&start='+str(page_no*10)\n",
    "        pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gets a list of all job postings on one of Indeed search pages. It is combined with function get_pages()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(search_link, country): #Takes in one search page and returns all job links on the page\n",
    "    links = []\n",
    "    page = requests.get(search_link, headers = headers)\n",
    "    search_soup = BeautifulSoup(page.text)\n",
    "    #print(search_soup)\n",
    "    mydivs = search_soup.find_all(\"div\", {\"class\": \"mosaic mosaic-provider-jobcards\"})\n",
    "    for div in mydivs:\n",
    "        classes = div.find_all(\"a\")\n",
    "        for c in classes:\n",
    "            sub = c.get('data-jk')\n",
    "            if isinstance(sub, str):\n",
    "                link = 'https://'+country+'.indeed.com/viewjob?jk='+sub\n",
    "            if link not in links:\n",
    "                links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of urls, created by get_links(). It visits each link and saves the response in a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requests(urls): #Takes in a job link and saves request.text to file\n",
    "    for url in urls:\n",
    "        captcha = True\n",
    "        while captcha:\n",
    "            response = requests.get(url, headers = headers)\n",
    "        \n",
    "            jk = url[33:]\n",
    "            path = \"..\\datasets\\requests\\\\\"+jk\n",
    "        \n",
    "            soup = BeautifulSoup(response.text)\n",
    "            title = soup.find(\"title\")\n",
    "        \n",
    "            if \"hCaptcha solve page\" not in title: # not a captcha\n",
    "                captcha = False\n",
    "            else:\n",
    "                time.sleep(1200)\n",
    "            \n",
    "        with open(path, \"w\") as file:\n",
    "            file.write(response.text)\n",
    "            time.sleep(10+random.uniform(-1,1)) #behave a bit like human\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \n",
    "In this section a set of job postings is scraped from the web, it is stored in a set of response text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['se','no','nl','de','be','es','it','ch','fr','uk','pt','fi','lu','at']\n",
    "link_list = []\n",
    "for country in countries:\n",
    "    pages = get_pages(\"Data Scientist\", 10, country)\n",
    "    for page in pages:\n",
    "        links = get_links(page, country)\n",
    "        link_list.append(links)\n",
    "        time.sleep(10+random.uniform(-1,1)) #behave a bit like human\n",
    "with open(\"link_list.txt\", \"wb\") as fp:\n",
    "    pickle.dump(link_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://nl.indeed.com/viewjob?jk=16f151177a6de429', 'https://nl.indeed.com/viewjob?jk=64bc5ad1e3b61059', 'https://nl.indeed.com/viewjob?jk=a5f719a1728e7074', 'https://nl.indeed.com/viewjob?jk=a7712338f646bd55', 'https://nl.indeed.com/viewjob?jk=984b6301bd1ffb16', 'https://nl.indeed.com/viewjob?jk=23a7e7cfb60d054e', 'https://nl.indeed.com/viewjob?jk=6b5f267033d3b2c3', 'https://nl.indeed.com/viewjob?jk=33213b318cb79506', 'https://nl.indeed.com/viewjob?jk=33ca3ecf5e219745', 'https://nl.indeed.com/viewjob?jk=9060ff5fee52893c', 'https://nl.indeed.com/viewjob?jk=851446284e9a6f45', 'https://nl.indeed.com/viewjob?jk=066501a6c6c3589f', 'https://nl.indeed.com/viewjob?jk=faf312d38dd2cc41', 'https://nl.indeed.com/viewjob?jk=a7dab3a06bddd0aa', 'https://nl.indeed.com/viewjob?jk=78935aa3094b1501']]\n"
     ]
    }
   ],
   "source": [
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for list in link_list:\n",
    "    get_requests(list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving a dataset from the acquired http responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_row(request):\n",
    "    with open(\"../datasets/requests/\"+ request) as file:\n",
    "        text = file.read()\n",
    "        soup = BeautifulSoup(text)  \n",
    "        full_text = soup.find(\"div\",{\"id\": \"jobDescriptionText\"}).text\n",
    "        if(detect(full_text) != 'en'): #Only parse English job postings\n",
    "            return \n",
    "        \n",
    "        timestamp = None\n",
    "        content = []\n",
    "        title_location = soup.find(\"title\").text\n",
    "        title = title_location.split(\"-\")[0]\n",
    "        location = title_location.split(\"-\")[1]\n",
    "        url = soup.find_all(\"meta\", {\"id\": \"indeed-share-url\"})\n",
    "        url = str(url[0])[15:-25]\n",
    "        \n",
    "        country = url[8:10]\n",
    "\n",
    "        list_elements = []\n",
    "    \n",
    "        divs = soup.find_all(\"div\",{\"id\": \"jobDescriptionText\"})\n",
    "        for div in divs:\n",
    "            uls = div.find_all(\"ul\")\n",
    "            for ul in uls:\n",
    "                for li in ul.find_all('li'):\n",
    "                    list_elements.append(li.text)\n",
    "    return [timestamp,url,title,location,country,full_text,list_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../datasets/requests'\n",
    "df = pd.DataFrame(columns = [\"dt\",\"url\",\"title\",\"location\",\"country\",\"full_text\",\"list_elements\"])\n",
    "for filename in os.listdir(dir):\n",
    "    row_list = request_to_row(filename)\n",
    "    df.loc[len(df)] = row_list \n",
    "df_.to_csv(\"../datasets/requests/df_europe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining with dataset downloaded from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dl=pd.read_csv('../datasets/data_scientist_united_states_job_postings_jobspikr.csv')\n",
    "df_dl = df_dl.dropna(subset=['html_job_description']) # Without the full html the job postings are not useful to us\n",
    "\n",
    "df_dl=df_dl[[\"crawl_timestamp\",\"url\",\"job_title\",\"inferred_city\",\"job_description\",\"html_job_description\"]]# selecting columns\n",
    "countries = []\n",
    "list_of_list_elements = []\n",
    "for index, row in df_dl.iterrows():\n",
    "    countries.append(\"us\")\n",
    "    soup = BeautifulSoup(row[\"html_job_description\"])\n",
    "    list_elements = []\n",
    "    \n",
    "    divs = soup.find_all(\"div\",{\"id\": \"jobDescriptionText\"})\n",
    "    for div in divs:\n",
    "        uls = div.find_all(\"ul\")\n",
    "        for ul in uls:\n",
    "            for li in ul.find_all('li'):\n",
    "                list_elements.append(li.text)\n",
    "    list_of_list_elements.append(list_elements)\n",
    "    \n",
    "df_dl[\"country\"]=countries\n",
    "df_dl[\"list_elements\"] = list_of_list_elements\n",
    "df_dl = df_dl.drop([\"html_job_description\"], axis = 1)\n",
    "df_dl.columns = [\"dt\",\"url\",\"title\",\"location\",\"full_text\",\"country\",\"list_elements\"]\n",
    "df_dl = df_dl.reindex(columns=[\"dt\",\"url\",\"title\",\"location\",\"country\",\"full_text\",\"list_elements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2633"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appending the european dataset with the USA downloaded dataset\n",
    "df_full = df.append(df_dl)\n",
    "df_full = df_full.drop_duplicates(subset=['location','full_text'])\n",
    "df_full.to_csv('../datasets/df_full.csv')\n",
    "len(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
